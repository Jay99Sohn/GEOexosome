{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jay99Sohn/GEOexosome/blob/main/GEOexosome.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73GjvE8uJRGI",
        "outputId": "a87d3eee-93bc-4fbd-9d9d-12aeeff19338"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting GEOparse\n",
            "  Downloading GEOparse-2.0.4-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.12/dist-packages (0.14.0)\n",
            "Requirement already satisfied: shap in /usr/local/lib/python3.12/dist-packages (0.50.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.12/dist-packages (from GEOparse) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.17 in /usr/local/lib/python3.12/dist-packages (from GEOparse) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from GEOparse) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.12/dist-packages (from GEOparse) (4.67.1)\n",
            "Requirement already satisfied: scipy<2,>=1.11.4 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn) (1.16.3)\n",
            "Requirement already satisfied: scikit-learn<2,>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn) (1.6.1)\n",
            "Requirement already satisfied: joblib<2,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn) (3.6.0)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.12/dist-packages (from shap) (25.0)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.12/dist-packages (from shap) (0.0.8)\n",
            "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.12/dist-packages (from shap) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from shap) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from shap) (4.15.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.54->shap) (0.43.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.17->GEOparse) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.17->GEOparse) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.21.0->GEOparse) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.21.0->GEOparse) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.21.0->GEOparse) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.21.0->GEOparse) (2025.11.12)\n",
            "Downloading GEOparse-2.0.4-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: GEOparse\n",
            "Successfully installed GEOparse-2.0.4\n",
            "\n",
            "============================================================\n",
            "ENVIRONMENT & LIBRARY SETUP\n",
            "============================================================\n",
            "[INFO] Google Colab detected. Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "[INFO] Drive mounted. Saving results to: /content/drive/MyDrive/geoexosome_results\n",
            "\n",
            "============================================================\n",
            "✓ Setup Complete\n",
            "  - Random seed: 42\n",
            "  - Output path: /content/drive/MyDrive/geoexosome_results\n",
            "============================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Cell 0: Environment & Library Setup\n",
        "\n",
        "# Optional: install required packages when running on Colab\n",
        "# Uncomment the line below if running on Google Colab for the first time:\n",
        "!pip install GEOparse imbalanced-learn shap seaborn matplotlib\n",
        "\n",
        "# ============================================================\n",
        "# Standard Library & Third-party Imports\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import GEOparse\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    roc_auc_score,\n",
        "    roc_curve,\n",
        "    accuracy_score\n",
        ")\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import shap\n",
        "\n",
        "# ============================================================\n",
        "# Configuration & Reproducibility\n",
        "# ============================================================\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "\n",
        "# Set visualization style for reproducible plots\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams[\"figure.dpi\"] = 100\n",
        "plt.rcParams[\"savefig.dpi\"] = 300\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ENVIRONMENT & LIBRARY SETUP\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ============================================================\n",
        "# Environment Detection & Path Configuration\n",
        "# ============================================================\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import drive\n",
        "    print(\"[INFO] Google Colab detected. Mounting Google Drive...\")\n",
        "    drive.mount(\"/content/drive\")\n",
        "    base_save_path = \"/content/drive/MyDrive/geoexosome_results\"\n",
        "    print(f\"[INFO] Drive mounted. Saving results to: {base_save_path}\")\n",
        "else:\n",
        "    base_save_path = \"./geoexosome_results\"\n",
        "    print(f\"[INFO] Local environment detected. Saving results to: {base_save_path}\")\n",
        "\n",
        "os.makedirs(base_save_path, exist_ok=True)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"✓ Setup Complete\")\n",
        "print(f\"  - Random seed: {SEED}\")\n",
        "print(f\"  - Output path: {base_save_path}\")\n",
        "print(\"=\" * 60 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFGnFGcHLgNz",
        "outputId": "731a3222-3d31-4e43-f8db-ba22c7a142fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "20-Dec-2025 15:56:06 INFO GEOparse - Downloading ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE39nnn/GSE39833/soft/GSE39833_family.soft.gz to ./data/GSE39833_family.soft.gz\n",
            "INFO:GEOparse:Downloading ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE39nnn/GSE39833/soft/GSE39833_family.soft.gz to ./data/GSE39833_family.soft.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 1: LOAD GEO DATASET (GSE39833)\n",
            "================================================================================\n",
            "[INFO] Downloading GEO dataset: GSE39833\n",
            "[INFO] This may take 1-2 minutes depending on network speed...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11.4M/11.4M [00:01<00:00, 9.38MB/s]\n",
            "20-Dec-2025 15:56:08 DEBUG downloader - Size validation passed\n",
            "DEBUG:GEOparse:Size validation passed\n",
            "20-Dec-2025 15:56:08 DEBUG downloader - Moving /tmp/tmpypk3_k8a to /content/data/GSE39833_family.soft.gz\n",
            "DEBUG:GEOparse:Moving /tmp/tmpypk3_k8a to /content/data/GSE39833_family.soft.gz\n",
            "20-Dec-2025 15:56:08 DEBUG downloader - Successfully downloaded ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE39nnn/GSE39833/soft/GSE39833_family.soft.gz\n",
            "DEBUG:GEOparse:Successfully downloaded ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE39nnn/GSE39833/soft/GSE39833_family.soft.gz\n",
            "20-Dec-2025 15:56:08 INFO GEOparse - Parsing ./data/GSE39833_family.soft.gz: \n",
            "INFO:GEOparse:Parsing ./data/GSE39833_family.soft.gz: \n",
            "20-Dec-2025 15:56:08 DEBUG GEOparse - DATABASE: GeoMiame\n",
            "DEBUG:GEOparse:DATABASE: GeoMiame\n",
            "20-Dec-2025 15:56:08 DEBUG GEOparse - SERIES: GSE39833\n",
            "DEBUG:GEOparse:SERIES: GSE39833\n",
            "20-Dec-2025 15:56:08 DEBUG GEOparse - PLATFORM: GPL14767\n",
            "DEBUG:GEOparse:PLATFORM: GPL14767\n",
            "20-Dec-2025 15:56:08 DEBUG GEOparse - SAMPLE: GSM980024\n",
            "DEBUG:GEOparse:SAMPLE: GSM980024\n",
            "20-Dec-2025 15:56:08 DEBUG GEOparse - SAMPLE: GSM980025\n",
            "DEBUG:GEOparse:SAMPLE: GSM980025\n",
            "20-Dec-2025 15:56:08 DEBUG GEOparse - SAMPLE: GSM980026\n",
            "DEBUG:GEOparse:SAMPLE: GSM980026\n",
            "20-Dec-2025 15:56:08 DEBUG GEOparse - SAMPLE: GSM980027\n",
            "DEBUG:GEOparse:SAMPLE: GSM980027\n",
            "20-Dec-2025 15:56:08 DEBUG GEOparse - SAMPLE: GSM980028\n",
            "DEBUG:GEOparse:SAMPLE: GSM980028\n",
            "20-Dec-2025 15:56:08 DEBUG GEOparse - SAMPLE: GSM980029\n",
            "DEBUG:GEOparse:SAMPLE: GSM980029\n",
            "20-Dec-2025 15:56:08 DEBUG GEOparse - SAMPLE: GSM980030\n",
            "DEBUG:GEOparse:SAMPLE: GSM980030\n",
            "20-Dec-2025 15:56:08 DEBUG GEOparse - SAMPLE: GSM980031\n",
            "DEBUG:GEOparse:SAMPLE: GSM980031\n",
            "20-Dec-2025 15:56:08 DEBUG GEOparse - SAMPLE: GSM980032\n",
            "DEBUG:GEOparse:SAMPLE: GSM980032\n",
            "20-Dec-2025 15:56:08 DEBUG GEOparse - SAMPLE: GSM980033\n",
            "DEBUG:GEOparse:SAMPLE: GSM980033\n",
            "20-Dec-2025 15:56:08 DEBUG GEOparse - SAMPLE: GSM980034\n",
            "DEBUG:GEOparse:SAMPLE: GSM980034\n",
            "20-Dec-2025 15:56:08 DEBUG GEOparse - SAMPLE: GSM980035\n",
            "DEBUG:GEOparse:SAMPLE: GSM980035\n",
            "20-Dec-2025 15:56:08 DEBUG GEOparse - SAMPLE: GSM980036\n",
            "DEBUG:GEOparse:SAMPLE: GSM980036\n",
            "20-Dec-2025 15:56:08 DEBUG GEOparse - SAMPLE: GSM980037\n",
            "DEBUG:GEOparse:SAMPLE: GSM980037\n",
            "20-Dec-2025 15:56:08 DEBUG GEOparse - SAMPLE: GSM980038\n",
            "DEBUG:GEOparse:SAMPLE: GSM980038\n",
            "20-Dec-2025 15:56:08 DEBUG GEOparse - SAMPLE: GSM980039\n",
            "DEBUG:GEOparse:SAMPLE: GSM980039\n",
            "20-Dec-2025 15:56:08 DEBUG GEOparse - SAMPLE: GSM980040\n",
            "DEBUG:GEOparse:SAMPLE: GSM980040\n",
            "20-Dec-2025 15:56:08 DEBUG GEOparse - SAMPLE: GSM980041\n",
            "DEBUG:GEOparse:SAMPLE: GSM980041\n",
            "20-Dec-2025 15:56:08 DEBUG GEOparse - SAMPLE: GSM980042\n",
            "DEBUG:GEOparse:SAMPLE: GSM980042\n",
            "20-Dec-2025 15:56:09 DEBUG GEOparse - SAMPLE: GSM980043\n",
            "DEBUG:GEOparse:SAMPLE: GSM980043\n",
            "20-Dec-2025 15:56:09 DEBUG GEOparse - SAMPLE: GSM980044\n",
            "DEBUG:GEOparse:SAMPLE: GSM980044\n",
            "20-Dec-2025 15:56:09 DEBUG GEOparse - SAMPLE: GSM980045\n",
            "DEBUG:GEOparse:SAMPLE: GSM980045\n",
            "20-Dec-2025 15:56:09 DEBUG GEOparse - SAMPLE: GSM980046\n",
            "DEBUG:GEOparse:SAMPLE: GSM980046\n",
            "20-Dec-2025 15:56:09 DEBUG GEOparse - SAMPLE: GSM980047\n",
            "DEBUG:GEOparse:SAMPLE: GSM980047\n",
            "20-Dec-2025 15:56:09 DEBUG GEOparse - SAMPLE: GSM980048\n",
            "DEBUG:GEOparse:SAMPLE: GSM980048\n",
            "20-Dec-2025 15:56:09 DEBUG GEOparse - SAMPLE: GSM980049\n",
            "DEBUG:GEOparse:SAMPLE: GSM980049\n",
            "20-Dec-2025 15:56:09 DEBUG GEOparse - SAMPLE: GSM980050\n",
            "DEBUG:GEOparse:SAMPLE: GSM980050\n",
            "20-Dec-2025 15:56:09 DEBUG GEOparse - SAMPLE: GSM980051\n",
            "DEBUG:GEOparse:SAMPLE: GSM980051\n",
            "20-Dec-2025 15:56:09 DEBUG GEOparse - SAMPLE: GSM980052\n",
            "DEBUG:GEOparse:SAMPLE: GSM980052\n",
            "20-Dec-2025 15:56:09 DEBUG GEOparse - SAMPLE: GSM980053\n",
            "DEBUG:GEOparse:SAMPLE: GSM980053\n",
            "20-Dec-2025 15:56:09 DEBUG GEOparse - SAMPLE: GSM980054\n",
            "DEBUG:GEOparse:SAMPLE: GSM980054\n",
            "20-Dec-2025 15:56:09 DEBUG GEOparse - SAMPLE: GSM980055\n",
            "DEBUG:GEOparse:SAMPLE: GSM980055\n",
            "20-Dec-2025 15:56:09 DEBUG GEOparse - SAMPLE: GSM980056\n",
            "DEBUG:GEOparse:SAMPLE: GSM980056\n",
            "20-Dec-2025 15:56:09 DEBUG GEOparse - SAMPLE: GSM980057\n",
            "DEBUG:GEOparse:SAMPLE: GSM980057\n",
            "20-Dec-2025 15:56:09 DEBUG GEOparse - SAMPLE: GSM980058\n",
            "DEBUG:GEOparse:SAMPLE: GSM980058\n",
            "20-Dec-2025 15:56:09 DEBUG GEOparse - SAMPLE: GSM980059\n",
            "DEBUG:GEOparse:SAMPLE: GSM980059\n",
            "20-Dec-2025 15:56:09 DEBUG GEOparse - SAMPLE: GSM980060\n",
            "DEBUG:GEOparse:SAMPLE: GSM980060\n",
            "20-Dec-2025 15:56:09 DEBUG GEOparse - SAMPLE: GSM980061\n",
            "DEBUG:GEOparse:SAMPLE: GSM980061\n",
            "20-Dec-2025 15:56:10 DEBUG GEOparse - SAMPLE: GSM980062\n",
            "DEBUG:GEOparse:SAMPLE: GSM980062\n",
            "20-Dec-2025 15:56:10 DEBUG GEOparse - SAMPLE: GSM980063\n",
            "DEBUG:GEOparse:SAMPLE: GSM980063\n",
            "20-Dec-2025 15:56:10 DEBUG GEOparse - SAMPLE: GSM980064\n",
            "DEBUG:GEOparse:SAMPLE: GSM980064\n",
            "20-Dec-2025 15:56:10 DEBUG GEOparse - SAMPLE: GSM980065\n",
            "DEBUG:GEOparse:SAMPLE: GSM980065\n",
            "20-Dec-2025 15:56:10 DEBUG GEOparse - SAMPLE: GSM980066\n",
            "DEBUG:GEOparse:SAMPLE: GSM980066\n",
            "20-Dec-2025 15:56:10 DEBUG GEOparse - SAMPLE: GSM980067\n",
            "DEBUG:GEOparse:SAMPLE: GSM980067\n",
            "20-Dec-2025 15:56:10 DEBUG GEOparse - SAMPLE: GSM980068\n",
            "DEBUG:GEOparse:SAMPLE: GSM980068\n",
            "20-Dec-2025 15:56:10 DEBUG GEOparse - SAMPLE: GSM980069\n",
            "DEBUG:GEOparse:SAMPLE: GSM980069\n",
            "20-Dec-2025 15:56:10 DEBUG GEOparse - SAMPLE: GSM980070\n",
            "DEBUG:GEOparse:SAMPLE: GSM980070\n",
            "20-Dec-2025 15:56:10 DEBUG GEOparse - SAMPLE: GSM980071\n",
            "DEBUG:GEOparse:SAMPLE: GSM980071\n",
            "20-Dec-2025 15:56:10 DEBUG GEOparse - SAMPLE: GSM980072\n",
            "DEBUG:GEOparse:SAMPLE: GSM980072\n",
            "20-Dec-2025 15:56:10 DEBUG GEOparse - SAMPLE: GSM980073\n",
            "DEBUG:GEOparse:SAMPLE: GSM980073\n",
            "20-Dec-2025 15:56:10 DEBUG GEOparse - SAMPLE: GSM980074\n",
            "DEBUG:GEOparse:SAMPLE: GSM980074\n",
            "20-Dec-2025 15:56:10 DEBUG GEOparse - SAMPLE: GSM980075\n",
            "DEBUG:GEOparse:SAMPLE: GSM980075\n",
            "20-Dec-2025 15:56:10 DEBUG GEOparse - SAMPLE: GSM980076\n",
            "DEBUG:GEOparse:SAMPLE: GSM980076\n",
            "20-Dec-2025 15:56:10 DEBUG GEOparse - SAMPLE: GSM980077\n",
            "DEBUG:GEOparse:SAMPLE: GSM980077\n",
            "20-Dec-2025 15:56:11 DEBUG GEOparse - SAMPLE: GSM980078\n",
            "DEBUG:GEOparse:SAMPLE: GSM980078\n",
            "20-Dec-2025 15:56:11 DEBUG GEOparse - SAMPLE: GSM980079\n",
            "DEBUG:GEOparse:SAMPLE: GSM980079\n",
            "20-Dec-2025 15:56:11 DEBUG GEOparse - SAMPLE: GSM980080\n",
            "DEBUG:GEOparse:SAMPLE: GSM980080\n",
            "20-Dec-2025 15:56:11 DEBUG GEOparse - SAMPLE: GSM980081\n",
            "DEBUG:GEOparse:SAMPLE: GSM980081\n",
            "20-Dec-2025 15:56:11 DEBUG GEOparse - SAMPLE: GSM980082\n",
            "DEBUG:GEOparse:SAMPLE: GSM980082\n",
            "20-Dec-2025 15:56:11 DEBUG GEOparse - SAMPLE: GSM980083\n",
            "DEBUG:GEOparse:SAMPLE: GSM980083\n",
            "20-Dec-2025 15:56:11 DEBUG GEOparse - SAMPLE: GSM980084\n",
            "DEBUG:GEOparse:SAMPLE: GSM980084\n",
            "20-Dec-2025 15:56:11 DEBUG GEOparse - SAMPLE: GSM980085\n",
            "DEBUG:GEOparse:SAMPLE: GSM980085\n",
            "20-Dec-2025 15:56:11 DEBUG GEOparse - SAMPLE: GSM980086\n",
            "DEBUG:GEOparse:SAMPLE: GSM980086\n",
            "20-Dec-2025 15:56:11 DEBUG GEOparse - SAMPLE: GSM980087\n",
            "DEBUG:GEOparse:SAMPLE: GSM980087\n",
            "20-Dec-2025 15:56:11 DEBUG GEOparse - SAMPLE: GSM980088\n",
            "DEBUG:GEOparse:SAMPLE: GSM980088\n",
            "20-Dec-2025 15:56:12 DEBUG GEOparse - SAMPLE: GSM980089\n",
            "DEBUG:GEOparse:SAMPLE: GSM980089\n",
            "20-Dec-2025 15:56:12 DEBUG GEOparse - SAMPLE: GSM980090\n",
            "DEBUG:GEOparse:SAMPLE: GSM980090\n",
            "20-Dec-2025 15:56:12 DEBUG GEOparse - SAMPLE: GSM980091\n",
            "DEBUG:GEOparse:SAMPLE: GSM980091\n",
            "20-Dec-2025 15:56:12 DEBUG GEOparse - SAMPLE: GSM980092\n",
            "DEBUG:GEOparse:SAMPLE: GSM980092\n",
            "20-Dec-2025 15:56:12 DEBUG GEOparse - SAMPLE: GSM980093\n",
            "DEBUG:GEOparse:SAMPLE: GSM980093\n",
            "20-Dec-2025 15:56:12 DEBUG GEOparse - SAMPLE: GSM980094\n",
            "DEBUG:GEOparse:SAMPLE: GSM980094\n",
            "20-Dec-2025 15:56:12 DEBUG GEOparse - SAMPLE: GSM980095\n",
            "DEBUG:GEOparse:SAMPLE: GSM980095\n",
            "20-Dec-2025 15:56:12 DEBUG GEOparse - SAMPLE: GSM980096\n",
            "DEBUG:GEOparse:SAMPLE: GSM980096\n",
            "20-Dec-2025 15:56:12 DEBUG GEOparse - SAMPLE: GSM980097\n",
            "DEBUG:GEOparse:SAMPLE: GSM980097\n",
            "20-Dec-2025 15:56:12 DEBUG GEOparse - SAMPLE: GSM980098\n",
            "DEBUG:GEOparse:SAMPLE: GSM980098\n",
            "20-Dec-2025 15:56:13 DEBUG GEOparse - SAMPLE: GSM980099\n",
            "DEBUG:GEOparse:SAMPLE: GSM980099\n",
            "20-Dec-2025 15:56:13 DEBUG GEOparse - SAMPLE: GSM980100\n",
            "DEBUG:GEOparse:SAMPLE: GSM980100\n",
            "20-Dec-2025 15:56:13 DEBUG GEOparse - SAMPLE: GSM980101\n",
            "DEBUG:GEOparse:SAMPLE: GSM980101\n",
            "20-Dec-2025 15:56:13 DEBUG GEOparse - SAMPLE: GSM980102\n",
            "DEBUG:GEOparse:SAMPLE: GSM980102\n",
            "20-Dec-2025 15:56:13 DEBUG GEOparse - SAMPLE: GSM980103\n",
            "DEBUG:GEOparse:SAMPLE: GSM980103\n",
            "20-Dec-2025 15:56:13 DEBUG GEOparse - SAMPLE: GSM980104\n",
            "DEBUG:GEOparse:SAMPLE: GSM980104\n",
            "20-Dec-2025 15:56:13 DEBUG GEOparse - SAMPLE: GSM980105\n",
            "DEBUG:GEOparse:SAMPLE: GSM980105\n",
            "20-Dec-2025 15:56:13 DEBUG GEOparse - SAMPLE: GSM980106\n",
            "DEBUG:GEOparse:SAMPLE: GSM980106\n",
            "20-Dec-2025 15:56:13 DEBUG GEOparse - SAMPLE: GSM980107\n",
            "DEBUG:GEOparse:SAMPLE: GSM980107\n",
            "20-Dec-2025 15:56:14 DEBUG GEOparse - SAMPLE: GSM980108\n",
            "DEBUG:GEOparse:SAMPLE: GSM980108\n",
            "20-Dec-2025 15:56:14 DEBUG GEOparse - SAMPLE: GSM980109\n",
            "DEBUG:GEOparse:SAMPLE: GSM980109\n",
            "20-Dec-2025 15:56:14 DEBUG GEOparse - SAMPLE: GSM980110\n",
            "DEBUG:GEOparse:SAMPLE: GSM980110\n",
            "20-Dec-2025 15:56:14 DEBUG GEOparse - SAMPLE: GSM980111\n",
            "DEBUG:GEOparse:SAMPLE: GSM980111\n",
            "20-Dec-2025 15:56:14 DEBUG GEOparse - SAMPLE: GSM980112\n",
            "DEBUG:GEOparse:SAMPLE: GSM980112\n",
            "20-Dec-2025 15:56:14 DEBUG GEOparse - SAMPLE: GSM980113\n",
            "DEBUG:GEOparse:SAMPLE: GSM980113\n",
            "20-Dec-2025 15:56:14 DEBUG GEOparse - SAMPLE: GSM980114\n",
            "DEBUG:GEOparse:SAMPLE: GSM980114\n",
            "20-Dec-2025 15:56:14 DEBUG GEOparse - SAMPLE: GSM980115\n",
            "DEBUG:GEOparse:SAMPLE: GSM980115\n",
            "20-Dec-2025 15:56:14 DEBUG GEOparse - SAMPLE: GSM980116\n",
            "DEBUG:GEOparse:SAMPLE: GSM980116\n",
            "20-Dec-2025 15:56:14 DEBUG GEOparse - SAMPLE: GSM980117\n",
            "DEBUG:GEOparse:SAMPLE: GSM980117\n",
            "20-Dec-2025 15:56:14 DEBUG GEOparse - SAMPLE: GSM980118\n",
            "DEBUG:GEOparse:SAMPLE: GSM980118\n",
            "20-Dec-2025 15:56:14 DEBUG GEOparse - SAMPLE: GSM980119\n",
            "DEBUG:GEOparse:SAMPLE: GSM980119\n",
            "20-Dec-2025 15:56:15 DEBUG GEOparse - SAMPLE: GSM980120\n",
            "DEBUG:GEOparse:SAMPLE: GSM980120\n",
            "20-Dec-2025 15:56:15 DEBUG GEOparse - SAMPLE: GSM980121\n",
            "DEBUG:GEOparse:SAMPLE: GSM980121\n",
            "20-Dec-2025 15:56:15 DEBUG GEOparse - SAMPLE: GSM980122\n",
            "DEBUG:GEOparse:SAMPLE: GSM980122\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Successfully loaded GSE39833\n",
            "  - GSM samples: 99\n",
            "  - GPL platforms: 1\n",
            "\n",
            "================================================================================\n",
            "STEP 2: EXTRACT EXPRESSION MATRIX AND ASSIGN LABELS\n",
            "================================================================================\n",
            "[INFO] Successfully extracted expression data for 99 samples\n",
            "  - Healthy controls (label=0): 11\n",
            "  - CRC patients (label=1): 88\n",
            "\n",
            "================================================================================\n",
            "STEP 3: BUILD EXPRESSION DATAFRAME\n",
            "================================================================================\n",
            "[INFO] Validating probe consistency across all samples...\n",
            "[INFO] ✓ Probe consistency verified across 4 samples\n",
            "[INFO] All samples contain 15739 probes in identical order\n",
            "\n",
            "[INFO] Expression DataFrame created\n",
            "  - Shape: (99, 15740)\n",
            "  - Samples: 99\n",
            "  - Probes (features): 15739\n",
            "\n",
            "[INFO] Label distribution:\n",
            "label\n",
            "1    88\n",
            "0    11\n",
            "\n",
            "[INFO] Label assignment log saved to: /content/drive/MyDrive/geoexosome_results/label_assignment_log.csv\n",
            "[NOTE] Include this file in Supplementary Materials for full transparency\n",
            "\n",
            "================================================================================\n",
            "STEP 4: PROBE-TO-miRNA MAPPING\n",
            "================================================================================\n",
            "[INFO] Loading platform (GPL) annotation...\n",
            "[INFO] Using miRNA annotation column: 'miRNA_ID'\n",
            "\n",
            "[INFO] Mapping Coverage:\n",
            "  - Total probes: 15739\n",
            "  - Successfully mapped: 15024 (95.5%)\n",
            "  - Unmapped probes: 715 (4.5%)\n",
            "\n",
            "[INFO] Probe-to-miRNA mapping saved to: /content/drive/MyDrive/geoexosome_results/probe_to_miRNA_mapping.csv\n",
            "[INFO] Unmapped probes list saved to: /content/drive/MyDrive/geoexosome_results/unmapped_probes.txt\n",
            "[NOTE] Include in Supplementary Materials to justify feature exclusion\n",
            "\n",
            "================================================================================\n",
            "STEP 5: DATA QUALITY SUMMARY\n",
            "================================================================================\n",
            "\n",
            "[Dataset Dimensions]\n",
            "  - Total samples: 99\n",
            "  - Total probes (features): 15739\n",
            "  - Healthy controls: 11\n",
            "  - CRC patients: 88\n",
            "\n",
            "[Data Quality Metrics]\n",
            "  - Missing values: 0 (0.0000% of all measurements)\n",
            "  - Constant probes (variance = 0): 0\n",
            "  - Expression value range: [-19.79, 781593.70]\n",
            "  - Mean expression (global): 7.67\n",
            "  - SD (global): 1224.52\n",
            "  - Median SD across features: 2.24\n",
            "\n",
            "[Quality Control Assessment]\n",
            "  ✓ PASS: No constant probes detected.\n",
            "  ✓ PASS: Negligible missing values (0.0000%).\n",
            "  ✓ PASS: Good miRNA mapping coverage (95.5%).\n",
            "\n",
            "[INFO] Comprehensive quality report saved to: /content/drive/MyDrive/geoexosome_results/data_quality_report.txt\n",
            "[NOTE] Use this report when writing the Methods section\n",
            "\n",
            "================================================================================\n",
            "✓ DATASET LOADING AND QUALITY CONTROL COMPLETE\n",
            "================================================================================\n",
            "\n",
            "Files saved to: /content/drive/MyDrive/geoexosome_results\n",
            "  1. label_assignment_log.csv - Sample label traceability\n",
            "  2. probe_to_miRNA_mapping.csv - Probe annotation mapping\n",
            "  3. unmapped_probes.txt - Probes without miRNA annotation\n",
            "  4. data_quality_report.txt - Comprehensive QC summary\n",
            "\n",
            "[Summary Statistics]\n",
            "  - Dataset: GSE39833\n",
            "  - Samples: 99 (11 controls, 88 CRC)\n",
            "  - Features: 15739 probes\n",
            "  - Mapped to miRNA: 15024/15739 (95.5%)\n",
            "  - Data quality: 0 constant, 0.0000% missing\n",
            "\n",
            "Next step: Proceed to Cell 2 for preprocessing and feature selection\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# Cell 1: GEO Dataset Loading and Quality Control\n",
        "# ==============================================================================\n",
        "\"\"\"\n",
        "Purpose:\n",
        "    Download and parse the GSE39833 dataset from NCBI GEO, extract expression\n",
        "    matrices, assign sample labels with full traceability, and perform\n",
        "    comprehensive quality control checks.\n",
        "\n",
        "Dataset:\n",
        "    GSE39833 - Serum exosome miRNA microarray from colorectal cancer patients\n",
        "    Platform: GPL14767 (Exiqon miRNA microarray)\n",
        "    Samples: 99 total (11 healthy controls, 88 CRC patients)\n",
        "\n",
        "Outputs:\n",
        "    1. df_expression: DataFrame with probe-level expression values and labels\n",
        "    2. mapping_df: Probe-to-miRNA mapping table\n",
        "    3. label_assignment_log.csv: Full traceability of label assignments\n",
        "    4. probe_to_miRNA_mapping.csv: Complete annotation mapping\n",
        "    5. unmapped_probes.txt: List of probes without miRNA annotation\n",
        "    6. data_quality_report.txt: Comprehensive QC summary for Methods section\n",
        "\n",
        "Quality Controls Implemented:\n",
        "    - Probe ID consistency validation across all samples\n",
        "    - Missing value detection and quantification\n",
        "    - Constant feature detection\n",
        "    - miRNA mapping coverage assessment\n",
        "    - Expression value range verification\n",
        "\n",
        "Author: [Jungho Sohn]\n",
        "Date: 2025-12-20\n",
        "Version: 1.0\n",
        "\"\"\"\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 1: LOAD GEO DATASET FROM NCBI\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 1: LOAD GEO DATASET (GSE39833)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "gse_id = \"GSE39833\"\n",
        "print(f\"[INFO] Downloading GEO dataset: {gse_id}\")\n",
        "print(\"[INFO] This may take 1-2 minutes depending on network speed...\")\n",
        "\n",
        "# Download and parse GEO dataset with platform annotation\n",
        "# annotate_gpl=True ensures GPL annotation is included for probe-to-miRNA mapping\n",
        "gse = GEOparse.get_GEO(\n",
        "    geo=gse_id,\n",
        "    destdir=\"./data\",\n",
        "    annotate_gpl=True\n",
        ")\n",
        "\n",
        "print(f\"[INFO] Successfully loaded {gse_id}\")\n",
        "print(f\"  - GSM samples: {len(gse.gsms)}\")\n",
        "print(f\"  - GPL platforms: {len(gse.gpls)}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 2: EXTRACT EXPRESSION MATRIX AND ASSIGN SAMPLE LABELS\n",
        "# ==============================================================================\n",
        "\"\"\"\n",
        "Label Assignment Strategy:\n",
        "    Priority 1: Sample title parsing (most reliable for this dataset)\n",
        "        - \"hc_*\" → Healthy control (label=0)\n",
        "        - \"crc*\" → CRC patient (label=1)\n",
        "\n",
        "    Priority 2: Metadata characteristics (fallback)\n",
        "        - Cancer keywords: tnm, stage, cancer, adenocarcinoma, tumor\n",
        "        - Healthy keywords: healthy, control, normal\n",
        "\n",
        "    All label assignments are logged with their source for transparency and\n",
        "    reproducibility. This log can be included in Supplementary Materials.\n",
        "\"\"\"\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 2: EXTRACT EXPRESSION MATRIX AND ASSIGN LABELS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Initialize containers for expression data and labels\n",
        "samples = []           # Sample IDs (GSM accessions)\n",
        "expression_rows = []   # Expression values for each sample\n",
        "labels = []            # Binary labels (0=healthy, 1=CRC)\n",
        "label_assignment_log = []  # Traceability log for label assignments\n",
        "\n",
        "# Iterate through all samples in the GEO dataset\n",
        "for gsm_name, gsm in gse.gsms.items():\n",
        "\n",
        "    # Validate that expression data is available\n",
        "    tbl = gsm.table\n",
        "    if \"VALUE\" not in tbl.columns:\n",
        "        print(f\"[WARNING] {gsm_name} missing VALUE column. Skipping...\")\n",
        "        continue\n",
        "\n",
        "    # Extract raw expression values and convert to float\n",
        "    expr_vals = tbl[\"VALUE\"].astype(float).values\n",
        "    expression_rows.append(expr_vals)\n",
        "    samples.append(gsm_name)\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Label Assignment with Source Tracking\n",
        "    # -------------------------------------------------------------------------\n",
        "    title_list = gsm.metadata.get(\"title\", [\"\"])\n",
        "    title = title_list[0].lower()\n",
        "    label_value = None\n",
        "    label_source = None\n",
        "\n",
        "    # PRIMARY METHOD: Title-based labeling\n",
        "    # This is the most reliable method for GSE39833 as samples follow\n",
        "    # a consistent naming convention\n",
        "    if title.startswith(\"hc_\"):\n",
        "        # Healthy control samples\n",
        "        label_value = 0\n",
        "        label_source = f\"title (starts with 'hc_')\"\n",
        "    elif title.startswith(\"crc\"):\n",
        "        # CRC patient samples (includes CRC1, CRC2, CRC3a, CRC3b, CRC4 stages)\n",
        "        label_value = 1\n",
        "        label_source = f\"title (starts with 'crc')\"\n",
        "\n",
        "    # FALLBACK METHOD: Metadata-based labeling\n",
        "    # Used only if title-based labeling fails\n",
        "    # This ensures robustness against potential metadata inconsistencies\n",
        "    if label_value is None:\n",
        "        characteristics = (\n",
        "            gsm.metadata.get(\"characteristics_ch1\", []) +\n",
        "            gsm.metadata.get(\"characteristics_ch2\", [])\n",
        "        )\n",
        "        chars_low = [c.lower() for c in characteristics]\n",
        "\n",
        "        # Define keyword lists for pattern matching\n",
        "        cancer_keywords = [\"tnm\", \"stage\", \"cancer\", \"adenocarcinoma\", \"tumor\"]\n",
        "        healthy_keywords = [\"healthy\", \"control\", \"normal\"]\n",
        "\n",
        "        # Check for cancer indicators in metadata\n",
        "        if any(keyword in c for keyword in cancer_keywords for c in chars_low):\n",
        "            label_value = 1\n",
        "            label_source = \"metadata (cancer-related keywords detected)\"\n",
        "\n",
        "        # Check for healthy control indicators in metadata\n",
        "        elif any(keyword in c for keyword in healthy_keywords for c in chars_low):\n",
        "            label_value = 0\n",
        "            label_source = \"metadata (healthy control keywords detected)\"\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Error Handling: Failed Label Assignment\n",
        "    # -------------------------------------------------------------------------\n",
        "    # If both primary and fallback methods fail, halt execution and display\n",
        "    # detailed metadata to enable manual verification and rule updates\n",
        "    if label_value is None:\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(f\"[ERROR] Unable to determine label for sample: {gsm_name}\")\n",
        "        print(\"=\" * 80)\n",
        "        print(f\"\\nSample Metadata:\")\n",
        "        print(f\"  - Title: {title_list}\")\n",
        "        print(f\"  - Characteristics (ch1): {gsm.metadata.get('characteristics_ch1', [])}\")\n",
        "        print(f\"  - Characteristics (ch2): {gsm.metadata.get('characteristics_ch2', [])}\")\n",
        "        print(f\"  - Source: {gsm.metadata.get('source_name_ch1', ['N/A'])}\")\n",
        "        print(f\"  - Description: {gsm.metadata.get('description', ['N/A'])}\")\n",
        "        print(f\"\\nPossible Causes:\")\n",
        "        print(f\"  1. Unexpected metadata format (not matching expected patterns)\")\n",
        "        print(f\"  2. Sample naming convention differs from other samples\")\n",
        "        print(f\"  3. Ambiguous or missing label information in metadata\")\n",
        "        print(f\"\\nAction Required:\")\n",
        "        print(f\"  Please verify the sample metadata above and update the label\")\n",
        "        print(f\"  assignment logic in this cell accordingly.\")\n",
        "        print(\"=\" * 80 + \"\\n\")\n",
        "        raise ValueError(f\"Label assignment failed for {gsm_name}\")\n",
        "\n",
        "    # Record successful label assignment with source for transparency\n",
        "    labels.append(label_value)\n",
        "    label_assignment_log.append({\n",
        "        \"Sample_ID\": gsm_name,\n",
        "        \"Label\": label_value,\n",
        "        \"Label_Name\": \"Healthy_Control\" if label_value == 0 else \"CRC_Patient\",\n",
        "        \"Assignment_Source\": label_source,\n",
        "        \"Sample_Title\": title_list[0]\n",
        "    })\n",
        "\n",
        "# Convert list of expression arrays to 2D numpy array\n",
        "# Shape: (n_samples, n_probes)\n",
        "expression_data = np.vstack(expression_rows)\n",
        "\n",
        "print(f\"[INFO] Successfully extracted expression data for {len(samples)} samples\")\n",
        "print(f\"  - Healthy controls (label=0): {labels.count(0)}\")\n",
        "print(f\"  - CRC patients (label=1): {labels.count(1)}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 3: BUILD EXPRESSION DATAFRAME WITH PROBE-LEVEL DATA\n",
        "# ==============================================================================\n",
        "\"\"\"\n",
        "Data Structure:\n",
        "    - Rows: Samples (GSM IDs)\n",
        "    - Columns: Probe IDs + 'label' column\n",
        "    - Values: Raw microarray intensity values\n",
        "\n",
        "Quality Check:\n",
        "    Validate that all samples have identical probe IDs in the same order.\n",
        "    This is critical for ensuring data integrity in downstream analyses.\n",
        "\"\"\"\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 3: BUILD EXPRESSION DATAFRAME\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Extract probe IDs from the first sample as reference\n",
        "first_gsm = gse.gsms[samples[0]]\n",
        "probe_ids = first_gsm.table[\"ID_REF\"].tolist()\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# Quality Control: Validate Probe ID Consistency\n",
        "# -------------------------------------------------------------------------\n",
        "# Verify that all samples have identical probe IDs in identical order\n",
        "# This check prevents silent errors from probe ID mismatches\n",
        "print(\"[INFO] Validating probe consistency across all samples...\")\n",
        "\n",
        "# Check first 3 samples and last sample for efficiency\n",
        "# Full validation is computationally expensive for large datasets\n",
        "samples_to_check = samples[:3] + [samples[-1]] if len(samples) > 3 else samples\n",
        "for gsm_name in samples_to_check:\n",
        "    current_probes = gse.gsms[gsm_name].table[\"ID_REF\"].tolist()\n",
        "\n",
        "    # Check probe count\n",
        "    if len(current_probes) != len(probe_ids):\n",
        "        raise ValueError(\n",
        "            f\"[ERROR] Probe count mismatch detected in {gsm_name}\\n\"\n",
        "            f\"Expected {len(probe_ids)} probes matching {samples[0]}, \"\n",
        "            f\"but found {len(current_probes)} probes.\"\n",
        "        )\n",
        "\n",
        "    # Check probe order\n",
        "    if current_probes != probe_ids:\n",
        "        raise ValueError(\n",
        "            f\"[ERROR] Probe order mismatch detected in {gsm_name}\\n\"\n",
        "            f\"Probe IDs do not match the reference sample {samples[0]}.\"\n",
        "        )\n",
        "\n",
        "print(f\"[INFO] ✓ Probe consistency verified across {len(samples_to_check)} samples\")\n",
        "print(f\"[INFO] All samples contain {len(probe_ids)} probes in identical order\")\n",
        "\n",
        "# Create DataFrame with probe IDs as columns and sample IDs as index\n",
        "df_expression = pd.DataFrame(\n",
        "    expression_data,\n",
        "    columns=probe_ids,\n",
        "    index=samples\n",
        ")\n",
        "df_expression[\"label\"] = labels\n",
        "\n",
        "print(f\"\\n[INFO] Expression DataFrame created\")\n",
        "print(f\"  - Shape: {df_expression.shape}\")\n",
        "print(f\"  - Samples: {df_expression.shape[0]}\")\n",
        "print(f\"  - Probes (features): {df_expression.shape[1] - 1}\")  # Excluding 'label' column\n",
        "print(f\"\\n[INFO] Label distribution:\")\n",
        "print(df_expression[\"label\"].value_counts().to_string())\n",
        "\n",
        "# Save label assignment log for manuscript transparency\n",
        "# This file should be included in Supplementary Materials\n",
        "label_log_df = pd.DataFrame(label_assignment_log)\n",
        "label_log_path = os.path.join(base_save_path, \"label_assignment_log.csv\")\n",
        "label_log_df.to_csv(label_log_path, index=False)\n",
        "print(f\"\\n[INFO] Label assignment log saved to: {label_log_path}\")\n",
        "print(\"[NOTE] Include this file in Supplementary Materials for full transparency\")\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 4: BUILD PROBE-TO-miRNA MAPPING FROM PLATFORM ANNOTATION\n",
        "# ==============================================================================\n",
        "\"\"\"\n",
        "Purpose:\n",
        "    Map microarray probe IDs to known miRNA identifiers using the GPL\n",
        "    platform annotation file. This enables biological interpretation of\n",
        "    features in downstream analysis.\n",
        "\n",
        "Coverage Assessment:\n",
        "    Calculate and report the percentage of probes successfully mapped to\n",
        "    miRNAs. Low coverage (<60%) may indicate platform compatibility issues.\n",
        "\n",
        "Unmapped Probes:\n",
        "    Probes without miRNA annotation will be excluded from downstream analysis\n",
        "    to ensure all features have biological interpretability. The list of\n",
        "    excluded probes is saved for transparency.\n",
        "\"\"\"\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 4: PROBE-TO-miRNA MAPPING\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"[INFO] Loading platform (GPL) annotation...\")\n",
        "\n",
        "# Extract platform annotation table\n",
        "gpl = list(gse.gpls.values())[0]\n",
        "gpl_table = gpl.table\n",
        "\n",
        "# Validate GPL table structure\n",
        "if \"ID\" not in gpl_table.columns:\n",
        "    raise KeyError(\"[ERROR] GPL table missing 'ID' column. Cannot build mapping.\")\n",
        "\n",
        "# Identify miRNA annotation column\n",
        "# Look for columns containing 'mir' (case-insensitive)\n",
        "mirna_cols = [c for c in gpl_table.columns if \"mir\" in c.lower()]\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# Handle Missing miRNA Annotation\n",
        "# -------------------------------------------------------------------------\n",
        "# Initialize coverage_pct to prevent NameError in QC section\n",
        "if len(mirna_cols) == 0:\n",
        "    print(\"[WARNING] No miRNA annotation column detected in GPL table.\")\n",
        "    print(\"[WARNING] Probe-to-miRNA mapping will be unavailable.\")\n",
        "    mapping_df = None\n",
        "    n_mapped = 0\n",
        "    n_total = len(probe_ids)\n",
        "    coverage_pct = 0.0\n",
        "else:\n",
        "    # Use the first miRNA annotation column found\n",
        "    mirna_col = mirna_cols[0]\n",
        "    print(f\"[INFO] Using miRNA annotation column: '{mirna_col}'\")\n",
        "\n",
        "    # Build probe-to-miRNA dictionary for fast lookup\n",
        "    probe_to_mirna = dict(zip(gpl_table[\"ID\"], gpl_table[mirna_col]))\n",
        "\n",
        "    # Map all probe IDs to miRNA names (NaN if not found)\n",
        "    mirna_names = [probe_to_mirna.get(pid, np.nan) for pid in probe_ids]\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Calculate Mapping Coverage Statistics\n",
        "    # -------------------------------------------------------------------------\n",
        "    n_mapped = sum(pd.notna(m) for m in mirna_names)\n",
        "    n_total = len(probe_ids)\n",
        "    coverage_pct = 100.0 * n_mapped / n_total\n",
        "\n",
        "    print(f\"\\n[INFO] Mapping Coverage:\")\n",
        "    print(f\"  - Total probes: {n_total}\")\n",
        "    print(f\"  - Successfully mapped: {n_mapped} ({coverage_pct:.1f}%)\")\n",
        "    print(f\"  - Unmapped probes: {n_total - n_mapped} ({100 - coverage_pct:.1f}%)\")\n",
        "\n",
        "    # Create mapping DataFrame\n",
        "    mapping_df = pd.DataFrame({\n",
        "        \"Probe_ID\": probe_ids,\n",
        "        \"miRNA\": mirna_names\n",
        "    })\n",
        "\n",
        "    # Save complete mapping table\n",
        "    mapping_path = os.path.join(base_save_path, \"probe_to_miRNA_mapping.csv\")\n",
        "    mapping_df.to_csv(mapping_path, index=False)\n",
        "    print(f\"\\n[INFO] Probe-to-miRNA mapping saved to: {mapping_path}\")\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Save Unmapped Probes for Transparency\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Document which probes were excluded and why\n",
        "    # This justifies feature exclusion in the manuscript\n",
        "    unmapped_probes = [\n",
        "        probe for probe, mirna in zip(probe_ids, mirna_names)\n",
        "        if pd.isna(mirna)\n",
        "    ]\n",
        "\n",
        "    if unmapped_probes:\n",
        "        unmapped_path = os.path.join(base_save_path, \"unmapped_probes.txt\")\n",
        "        with open(unmapped_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(f\"Unmapped Probes (no miRNA annotation): {len(unmapped_probes)} total\\n\")\n",
        "            f.write(\"=\" * 80 + \"\\n\\n\")\n",
        "            f.write(\"These probes will be excluded in downstream preprocessing (Cell 2) \")\n",
        "            f.write(\"due to lack of miRNA annotation in the platform GPL file.\\n\\n\")\n",
        "            f.write(\"This exclusion ensures that all analyzed features have biological \")\n",
        "            f.write(\"interpretability as known miRNAs.\\n\\n\")\n",
        "            f.write(\"List of unmapped probe IDs:\\n\")\n",
        "            f.write(\"-\" * 80 + \"\\n\")\n",
        "            for probe in unmapped_probes:\n",
        "                f.write(f\"{probe}\\n\")\n",
        "        print(f\"[INFO] Unmapped probes list saved to: {unmapped_path}\")\n",
        "        print(\"[NOTE] Include in Supplementary Materials to justify feature exclusion\")\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 5: COMPREHENSIVE DATA QUALITY VALIDATION\n",
        "# ==============================================================================\n",
        "\"\"\"\n",
        "Quality Control Metrics:\n",
        "    1. Missing values: Count and percentage of NaN/null values\n",
        "    2. Constant probes: Features with zero variance (uninformative)\n",
        "    3. Expression range: Min/max values to detect outliers or errors\n",
        "    4. Summary statistics: Mean, SD for manuscript reporting\n",
        "\n",
        "Assessment Criteria:\n",
        "    - Missing values: PASS if <0.01%, WARNING if 0.01-5%, FAIL if >5%\n",
        "    - Constant probes: PASS if 0, WARNING otherwise\n",
        "    - Mapping coverage: PASS if ≥75%, NOTICE if 60-75%, WARNING if <60%\n",
        "\n",
        "Output:\n",
        "    Comprehensive report file (data_quality_report.txt) formatted for\n",
        "    direct use in manuscript Methods section.\n",
        "\"\"\"\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 5: DATA QUALITY SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Extract feature columns (exclude 'label' column)\n",
        "feature_cols = [col for col in df_expression.columns if col != \"label\"]\n",
        "expr_matrix = df_expression[feature_cols]\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# Calculate Quality Metrics\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "# Dataset dimensions\n",
        "n_samples = df_expression.shape[0]\n",
        "n_features = len(feature_cols)\n",
        "\n",
        "# Missing value analysis\n",
        "n_missing = expr_matrix.isna().sum().sum()\n",
        "total_values = n_samples * n_features\n",
        "missing_pct = 100.0 * n_missing / total_values\n",
        "\n",
        "# Constant feature detection (variance = 0)\n",
        "expr_var = expr_matrix.var(axis=0)\n",
        "n_constant = (expr_var == 0).sum()\n",
        "\n",
        "# Expression value statistics\n",
        "# Use global statistics rather than feature-wise averages for clarity\n",
        "expr_min = expr_matrix.min().min()\n",
        "expr_max = expr_matrix.max().max()\n",
        "expr_mean_global = expr_matrix.mean().mean()        # Mean of all values\n",
        "expr_std_global = expr_matrix.to_numpy().std()      # SD of all values\n",
        "expr_feature_std_median = expr_matrix.std().median()  # Median SD across features\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# Display Quality Control Summary\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "print(f\"\\n[Dataset Dimensions]\")\n",
        "print(f\"  - Total samples: {n_samples}\")\n",
        "print(f\"  - Total probes (features): {n_features}\")\n",
        "print(f\"  - Healthy controls: {(df_expression['label'] == 0).sum()}\")\n",
        "print(f\"  - CRC patients: {(df_expression['label'] == 1).sum()}\")\n",
        "\n",
        "print(f\"\\n[Data Quality Metrics]\")\n",
        "print(f\"  - Missing values: {n_missing} ({missing_pct:.4f}% of all measurements)\")\n",
        "print(f\"  - Constant probes (variance = 0): {n_constant}\")\n",
        "print(f\"  - Expression value range: [{expr_min:.2f}, {expr_max:.2f}]\")\n",
        "print(f\"  - Mean expression (global): {expr_mean_global:.2f}\")\n",
        "print(f\"  - SD (global): {expr_std_global:.2f}\")\n",
        "print(f\"  - Median SD across features: {expr_feature_std_median:.2f}\")\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# Quality Control Assessment with Pass/Warning/Fail Criteria\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "print(f\"\\n[Quality Control Assessment]\")\n",
        "\n",
        "# Check 1: Constant probes\n",
        "if n_constant > 0:\n",
        "    print(f\"  ⚠ WARNING: {n_constant} constant probes detected.\")\n",
        "    print(f\"     → These should be removed before feature selection.\")\n",
        "else:\n",
        "    print(f\"  ✓ PASS: No constant probes detected.\")\n",
        "\n",
        "# Check 2: Missing values\n",
        "if missing_pct > 5.0:\n",
        "    print(f\"  ⚠ WARNING: Missing values exceed 5% threshold ({missing_pct:.2f}%).\")\n",
        "    print(f\"     → Consider imputation or removal of problematic probes.\")\n",
        "elif missing_pct > 0.01:\n",
        "    print(f\"  ⚠ NOTICE: Low level of missing values detected ({missing_pct:.4f}%).\")\n",
        "    print(f\"     → Acceptable for most analyses without imputation.\")\n",
        "else:\n",
        "    print(f\"  ✓ PASS: Negligible missing values ({missing_pct:.4f}%).\")\n",
        "\n",
        "# Check 3: miRNA mapping coverage\n",
        "if coverage_pct < 60:\n",
        "    print(f\"  ⚠ WARNING: miRNA mapping coverage is low ({coverage_pct:.1f}%).\")\n",
        "    print(f\"     → Verify platform annotation compatibility.\")\n",
        "elif coverage_pct < 75:\n",
        "    print(f\"  ⚠ NOTICE: Moderate miRNA mapping coverage ({coverage_pct:.1f}%).\")\n",
        "    print(f\"     → Acceptable for most downstream analyses.\")\n",
        "else:\n",
        "    print(f\"  ✓ PASS: Good miRNA mapping coverage ({coverage_pct:.1f}%).\")\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# Save Comprehensive Quality Report for Manuscript\n",
        "# -------------------------------------------------------------------------\n",
        "# This report is formatted for direct use in the Methods section\n",
        "# and provides all necessary QC information for reproducibility\n",
        "\n",
        "qc_report_path = os.path.join(base_save_path, \"data_quality_report.txt\")\n",
        "with open(qc_report_path, 'w', encoding='utf-8') as f:\n",
        "    f.write(\"=\" * 80 + \"\\n\")\n",
        "    f.write(\"DATA QUALITY REPORT - GSE39833\\n\")\n",
        "    f.write(\"=\" * 80 + \"\\n\\n\")\n",
        "    f.write(f\"Dataset: {gse_id}\\n\")\n",
        "    f.write(f\"Analysis Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
        "\n",
        "    f.write(\"DATASET DIMENSIONS\\n\")\n",
        "    f.write(\"-\" * 80 + \"\\n\")\n",
        "    f.write(f\"Total samples: {n_samples}\\n\")\n",
        "    f.write(f\"Total probes: {n_features}\\n\")\n",
        "    f.write(f\"Healthy controls: {(df_expression['label'] == 0).sum()}\\n\")\n",
        "    f.write(f\"CRC patients: {(df_expression['label'] == 1).sum()}\\n\\n\")\n",
        "\n",
        "    f.write(\"DATA QUALITY METRICS\\n\")\n",
        "    f.write(\"-\" * 80 + \"\\n\")\n",
        "    f.write(f\"Missing values: {n_missing} ({missing_pct:.4f}%)\\n\")\n",
        "    f.write(f\"Constant probes: {n_constant}\\n\")\n",
        "    f.write(f\"Expression range: [{expr_min:.2f}, {expr_max:.2f}]\\n\")\n",
        "    f.write(f\"Mean (global): {expr_mean_global:.2f}\\n\")\n",
        "    f.write(f\"SD (global): {expr_std_global:.2f}\\n\")\n",
        "    f.write(f\"Median SD across features: {expr_feature_std_median:.2f}\\n\\n\")\n",
        "\n",
        "    f.write(\"miRNA MAPPING COVERAGE\\n\")\n",
        "    f.write(\"-\" * 80 + \"\\n\")\n",
        "    f.write(f\"Total probes: {n_total}\\n\")\n",
        "    f.write(f\"Mapped probes: {n_mapped} ({coverage_pct:.1f}%)\\n\")\n",
        "    f.write(f\"Unmapped probes: {n_total - n_mapped} ({100 - coverage_pct:.1f}%)\\n\\n\")\n",
        "\n",
        "    f.write(\"QUALITY CONTROL ASSESSMENT\\n\")\n",
        "    f.write(\"-\" * 80 + \"\\n\")\n",
        "    f.write(f\"Constant probes: {'PASS' if n_constant == 0 else 'WARNING'}\\n\")\n",
        "    f.write(f\"Missing values: {'PASS' if missing_pct < 0.01 else 'WARNING/NOTICE'}\\n\")\n",
        "    f.write(f\"Mapping coverage: {'PASS' if coverage_pct >= 75 else 'WARNING/NOTICE'}\\n\\n\")\n",
        "\n",
        "    f.write(\"NOTES FOR MANUSCRIPT (Methods Section)\\n\")\n",
        "    f.write(\"-\" * 80 + \"\\n\")\n",
        "    f.write(\"Use the following information when writing the Methods section:\\n\\n\")\n",
        "\n",
        "    f.write(f\"1. Sample Composition:\\n\")\n",
        "    f.write(f\"   The GSE39833 dataset comprised {n_samples} serum exosome samples, \")\n",
        "    f.write(f\"including {(df_expression['label'] == 0).sum()} healthy controls and \")\n",
        "    f.write(f\"{(df_expression['label'] == 1).sum()} colorectal cancer (CRC) patients.\\n\\n\")\n",
        "\n",
        "    f.write(f\"2. Data Quality:\\n\")\n",
        "    f.write(f\"   Data quality was verified prior to analysis. \")\n",
        "    if n_constant == 0:\n",
        "        f.write(f\"No constant features were detected. \")\n",
        "    else:\n",
        "        f.write(f\"{n_constant} constant features were identified and removed. \")\n",
        "    f.write(f\"Missing values accounted for {missing_pct:.4f}% of all measurements\")\n",
        "    if missing_pct < 0.01:\n",
        "        f.write(f\"; no imputation was performed due to negligible missingness.\\n\\n\")\n",
        "    else:\n",
        "        f.write(f\".\\n\\n\")\n",
        "\n",
        "    f.write(f\"3. Feature Annotation:\\n\")\n",
        "    f.write(f\"   Of the {n_total} microarray probes, {n_mapped} ({coverage_pct:.1f}%) were \")\n",
        "    f.write(f\"successfully mapped to known miRNAs in the platform annotation file. \")\n",
        "    f.write(f\"Unmapped probes were excluded from downstream analysis to ensure \")\n",
        "    f.write(f\"biological interpretability of all features.\\n\\n\")\n",
        "\n",
        "    f.write(f\"4. Label Assignment:\\n\")\n",
        "    f.write(f\"   Sample labels were assigned based on standardized metadata fields \")\n",
        "    f.write(f\"(sample titles and characteristics). All label assignments were recorded \")\n",
        "    f.write(f\"in a traceability log (label_assignment_log.csv) to ensure transparency \")\n",
        "    f.write(f\"and reproducibility.\\n\")\n",
        "\n",
        "print(f\"\\n[INFO] Comprehensive quality report saved to: {qc_report_path}\")\n",
        "print(\"[NOTE] Use this report when writing the Methods section\")\n",
        "\n",
        "# ==============================================================================\n",
        "# FINAL SUMMARY\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"✓ DATASET LOADING AND QUALITY CONTROL COMPLETE\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nFiles saved to: {base_save_path}\")\n",
        "print(f\"  1. label_assignment_log.csv - Sample label traceability\")\n",
        "print(f\"  2. probe_to_miRNA_mapping.csv - Probe annotation mapping\")\n",
        "if n_total - n_mapped > 0:\n",
        "    print(f\"  3. unmapped_probes.txt - Probes without miRNA annotation\")\n",
        "    print(f\"  4. data_quality_report.txt - Comprehensive QC summary\")\n",
        "else:\n",
        "    print(f\"  3. data_quality_report.txt - Comprehensive QC summary\")\n",
        "\n",
        "print(f\"\\n[Summary Statistics]\")\n",
        "print(f\"  - Dataset: {gse_id}\")\n",
        "print(f\"  - Samples: {n_samples} ({(df_expression['label'] == 0).sum()} controls, {(df_expression['label'] == 1).sum()} CRC)\")\n",
        "print(f\"  - Features: {n_features} probes\")\n",
        "print(f\"  - Mapped to miRNA: {n_mapped}/{n_total} ({coverage_pct:.1f}%)\")\n",
        "print(f\"  - Data quality: {n_constant} constant, {missing_pct:.4f}% missing\")\n",
        "\n",
        "print(f\"\\nNext step: Proceed to Cell 2 for preprocessing and feature selection\")\n",
        "print(\"=\" * 80 + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKoOrJcKObxL",
        "outputId": "2d6dc765-9090-4adc-99da-de2850456c3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=====================================================================\n",
            "Cell 4: Nested Cross-Validation with Fold-wise Feature Selection\n",
            "\n",
            "Strategy:\n",
            "  - 5-fold outer CV for unbiased performance estimation\n",
            "  - 3-fold inner CV for hyperparameter tuning\n",
            "  - INDEPENDENT feature selection within each training fold\n",
            "  - NO information leakage from test folds\n",
            "  \n",
            "This provides rigorous, unbiased estimates of model generalization\n",
            "suitable for publication in peer-reviewed journals.\n",
            "=====================================================================\n",
            "\n",
            "\n",
            "================================================================================\n",
            "STEP 1: Load Expression Data from Cell 1\n",
            "================================================================================\n",
            "[INFO] Applying log2(x + 1) transformation to raw expression values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pandas/core/internals/blocks.py:393: RuntimeWarning: invalid value encountered in log2\n",
            "  result = func(self.values, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Expression matrix loaded: (99, 15739)\n",
            "  Samples: 99\n",
            "  Probes: 15739\n",
            "  Healthy controls: 11\n",
            "  CRC patients: 88\n",
            "  Class imbalance ratio: 1:8.0\n",
            "\n",
            "[VERIFICATION] Checking for data leakage risks:\n",
            "  Max value: 19.58\n",
            "  Min value: -16.45\n",
            "  Global mean: 0.88\n",
            "  Global std: 1.62\n",
            "  ✓ Data is NOT globally normalized. Safe for fold-wise processing.\n",
            "\n",
            "================================================================================\n",
            "STEP 2: Configure Cross-Validation Strategy\n",
            "================================================================================\n",
            "[INFO] Outer CV: 5-fold stratified\n",
            "[INFO] Inner CV: 3-fold stratified\n",
            "[INFO] Random seed: 42\n",
            "[INFO] Results will be saved to: /content/drive/MyDrive/geoexosome_results\n",
            "\n",
            "================================================================================\n",
            "STEP 3: Define Model Configurations\n",
            "================================================================================\n",
            "[INFO] Configured 4 model variants:\n",
            "  - RandomForest_SMOTE\n",
            "  - RandomForest_Weighted\n",
            "  - SVM\n",
            "  - LogisticRegression\n",
            "\n",
            "================================================================================\n",
            "STEP 4: Execute Nested Cross-Validation\n",
            "================================================================================\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Model: RandomForest_SMOTE\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "  Fold 1 class distribution:\n",
            "    Train: 71/79 (89.87%) CRC\n",
            "    Test:  17/20 (85.00%) CRC\n",
            "\n",
            "    [Fold 1] Feature selection on training data only\n",
            "      Train samples: 79 (8 HC, 71 CRC)\n",
            "      Total probes: 15739\n",
            "      Stage 1: 152 probes (|log2FC| > 1.0, FDR q < 0.05)\n",
            "      Stage 2: Multi-method selection (LASSO, SVM-RFE, RF)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        LASSO: 10 features\n",
            "        SVM-RFE: 50 features\n",
            "        Random Forest: 152 features\n",
            "      Stage 3: 51 features (≥2/3 consensus)\n",
            "\n",
            "    [Fold 1] Training with 51 features\n",
            "    Best inner CV AUC: 0.9965\n",
            "    Train AUC: 1.0000\n",
            "    Test AUC:  1.0000\n",
            "\n",
            "  Fold 2 class distribution:\n",
            "    Train: 70/79 (88.61%) CRC\n",
            "    Test:  18/20 (90.00%) CRC\n",
            "\n",
            "    [Fold 2] Feature selection on training data only\n",
            "      Train samples: 79 (9 HC, 70 CRC)\n",
            "      Total probes: 15739\n",
            "      Stage 1: 131 probes (|log2FC| > 1.0, FDR q < 0.05)\n",
            "      Stage 2: Multi-method selection (LASSO, SVM-RFE, RF)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        LASSO: 26 features\n",
            "        SVM-RFE: 43 features\n",
            "        Random Forest: 131 features\n",
            "      Stage 3: 51 features (≥2/3 consensus)\n",
            "\n",
            "    [Fold 2] Training with 51 features\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# Cell 2: Nested Cross-Validation with Fold-wise Feature Selection\n",
        "# ==============================================================================\n",
        "\"\"\"\n",
        "Purpose:\n",
        "    Rigorous nested cross-validation (5-fold outer, 3-fold inner) with\n",
        "    independent feature selection within each training fold to prevent\n",
        "    information leakage and ensure unbiased performance estimation.\n",
        "\n",
        "Methodology:\n",
        "    1. Outer CV (5-fold): Unbiased performance estimation\n",
        "    2. Inner CV (3-fold): Hyperparameter tuning\n",
        "    3. Fold-wise feature selection: Independent selection per fold\n",
        "    4. Train-only normalization: Test data never influences preprocessing\n",
        "\n",
        "Models Compared:\n",
        "    - Random Forest with SMOTE\n",
        "    - Random Forest with class weighting\n",
        "    - Support Vector Machine with SMOTE\n",
        "    - Logistic Regression with SMOTE\n",
        "\n",
        "Feature Selection Strategy (per fold):\n",
        "    Stage 1: Biological filtering (fold change + Mann-Whitney U test)\n",
        "    Stage 2: Multi-method consensus (LASSO, SVM-RFE, Random Forest)\n",
        "    Stage 3: 2-of-3 consensus voting\n",
        "\n",
        "Outputs:\n",
        "    1. Out-of-fold (OOF) performance metrics with 95% CI\n",
        "    2. Feature stability analysis across folds\n",
        "    3. Final model trained on full dataset for external validation\n",
        "    4. Comprehensive results JSON and visualization\n",
        "\n",
        "Author: Jungho Sohn\n",
        "Date: 2025-12-20\n",
        "Version: 2.0 (Data leakage eliminated)\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from scipy import stats\n",
        "from scipy.stats import mannwhitneyu\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score,\n",
        "    accuracy_score,\n",
        "    balanced_accuracy_score,\n",
        "    precision_recall_fscore_support,\n",
        "    confusion_matrix,\n",
        "    roc_curve\n",
        ")\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression, LassoCV\n",
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "\n",
        "print(\"\"\"\n",
        "=====================================================================\n",
        "Cell 4: Nested Cross-Validation with Fold-wise Feature Selection\n",
        "\n",
        "Strategy:\n",
        "  - 5-fold outer CV for unbiased performance estimation\n",
        "  - 3-fold inner CV for hyperparameter tuning\n",
        "  - INDEPENDENT feature selection within each training fold\n",
        "  - NO information leakage from test folds\n",
        "\n",
        "This provides rigorous, unbiased estimates of model generalization\n",
        "suitable for publication in peer-reviewed journals.\n",
        "=====================================================================\n",
        "\"\"\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. Load Expression Data from Cell 1 (NO Normalization Applied!)\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 1: Load Expression Data from Cell 1\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Extract raw expression matrix (should be log2-transformed, NO quantile norm!)\n",
        "expr_raw = df_expression.drop(columns=['label'])\n",
        "labels_full = df_expression['label'].values\n",
        "sample_ids_full = df_expression.index.tolist()\n",
        "\n",
        "# Apply log2 transformation if not already done\n",
        "if expr_raw.values.max() > 20:\n",
        "    print(\"[INFO] Applying log2(x + 1) transformation to raw expression values\")\n",
        "    expr_log_full = np.log2(expr_raw + 1.0)\n",
        "else:\n",
        "    print(\"[INFO] Data appears log2-transformed. Using as-is.\")\n",
        "    expr_log_full = expr_raw.copy()\n",
        "\n",
        "# Handle infinite values from log2 transformation\n",
        "expr_log_full = expr_log_full.replace([np.inf, -np.inf], np.nan)\n",
        "expr_log_full = expr_log_full.fillna(expr_log_full.median(axis=0))\n",
        "\n",
        "# Convert to DataFrame with proper indices\n",
        "expr_log_full = pd.DataFrame(\n",
        "    expr_log_full.values,\n",
        "    index=sample_ids_full,\n",
        "    columns=expr_raw.columns\n",
        ")\n",
        "\n",
        "print(f\"[INFO] Expression matrix loaded: {expr_log_full.shape}\")\n",
        "print(f\"  Samples: {len(sample_ids_full)}\")\n",
        "print(f\"  Probes: {expr_log_full.shape[1]}\")\n",
        "print(f\"  Healthy controls: {sum(labels_full == 0)}\")\n",
        "print(f\"  CRC patients: {sum(labels_full == 1)}\")\n",
        "print(f\"  Class imbalance ratio: 1:{sum(labels_full == 1)/sum(labels_full == 0):.1f}\")\n",
        "\n",
        "# CRITICAL: Ensure NO normalization has been applied globally\n",
        "print(\"\\n[VERIFICATION] Checking for data leakage risks:\")\n",
        "print(f\"  Max value: {expr_log_full.values.max():.2f}\")\n",
        "print(f\"  Min value: {expr_log_full.values.min():.2f}\")\n",
        "print(f\"  Global mean: {expr_log_full.values.mean():.2f}\")\n",
        "print(f\"  Global std: {expr_log_full.values.std():.2f}\")\n",
        "if abs(expr_log_full.values.mean()) < 0.1 and abs(expr_log_full.values.std() - 1.0) < 0.1:\n",
        "    print(\"  ⚠️  WARNING: Data appears globally normalized! Risk of data leakage!\")\n",
        "else:\n",
        "    print(\"  ✓ Data is NOT globally normalized. Safe for fold-wise processing.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. Cross-Validation Configuration\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 2: Configure Cross-Validation Strategy\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Outer CV: 5-fold stratified for unbiased performance estimation\n",
        "outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "\n",
        "# Inner CV: 3-fold stratified for hyperparameter tuning\n",
        "inner_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=SEED)\n",
        "\n",
        "print(f\"[INFO] Outer CV: {outer_cv.get_n_splits()}-fold stratified\")\n",
        "print(f\"[INFO] Inner CV: {inner_cv.get_n_splits()}-fold stratified\")\n",
        "print(f\"[INFO] Random seed: {SEED}\")\n",
        "\n",
        "# Create results directory\n",
        "RESULT_DIR = base_save_path\n",
        "print(f\"[INFO] Results will be saved to: {RESULT_DIR}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. Model Configurations\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 3: Define Model Configurations\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "model_configs = {}\n",
        "\n",
        "# Random Forest with SMOTE oversampling\n",
        "model_configs[\"RandomForest_SMOTE\"] = {\n",
        "    \"use_smote\": True,\n",
        "    \"use_scaler\": True,  # RF doesn't require scaling\n",
        "    \"classifier\": \"rf\",\n",
        "    \"param_grid\": {\n",
        "        \"clf__n_estimators\": [200, 500],\n",
        "        \"clf__max_depth\": [None, 5, 10],\n",
        "        \"clf__max_features\": [0.3, 0.5, \"sqrt\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Random Forest with class weighting (no SMOTE)\n",
        "model_configs[\"RandomForest_Weighted\"] = {\n",
        "    \"use_smote\": False,\n",
        "    \"use_scaler\": True,\n",
        "    \"classifier\": \"rf\",\n",
        "    \"param_grid\": {\n",
        "        \"clf__n_estimators\": [200, 500],\n",
        "        \"clf__max_depth\": [None, 5, 10],\n",
        "        \"clf__max_features\": [0.3, 0.5, \"sqrt\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Support Vector Machine with SMOTE\n",
        "model_configs[\"SVM\"] = {\n",
        "    \"use_smote\": True,\n",
        "    \"use_scaler\": True,  # SVM requires scaling\n",
        "    \"classifier\": \"svm\",\n",
        "    \"param_grid\": {\n",
        "        \"clf__C\": [0.1, 1, 10],\n",
        "        \"clf__gamma\": [\"scale\", \"auto\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Logistic Regression with SMOTE\n",
        "model_configs[\"LogisticRegression\"] = {\n",
        "    \"use_smote\": True,\n",
        "    \"use_scaler\": True,  # LR benefits from scaling\n",
        "    \"classifier\": \"lr\",\n",
        "    \"param_grid\": {\n",
        "        \"clf__C\": [0.1, 1, 10],\n",
        "        \"clf__penalty\": [\"l2\"],\n",
        "        \"clf__solver\": [\"lbfgs\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "print(f\"[INFO] Configured {len(model_configs)} model variants:\")\n",
        "for model_name in model_configs.keys():\n",
        "    print(f\"  - {model_name}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. Utility Functions\n",
        "# ==============================================================================\n",
        "\n",
        "def bootstrap_auc_ci(y_true, y_proba, n_bootstrap=1000, alpha=0.05, random_state=SEED):\n",
        "    \"\"\"\n",
        "    Compute bootstrap confidence interval for ROC-AUC.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y_true : array-like\n",
        "        True binary labels\n",
        "    y_proba : array-like\n",
        "        Predicted probabilities for positive class\n",
        "    n_bootstrap : int, default=1000\n",
        "        Number of bootstrap iterations\n",
        "    alpha : float, default=0.05\n",
        "        Significance level (0.05 for 95% CI)\n",
        "    random_state : int, default=SEED\n",
        "        Random seed for reproducibility\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    lower : float\n",
        "        Lower bound of confidence interval\n",
        "    upper : float\n",
        "        Upper bound of confidence interval\n",
        "    \"\"\"\n",
        "    rng = np.random.RandomState(random_state)\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_proba = np.asarray(y_proba)\n",
        "    n = len(y_true)\n",
        "\n",
        "    aucs = []\n",
        "    for _ in range(n_bootstrap):\n",
        "        # Bootstrap sample with replacement\n",
        "        indices = rng.choice(n, n, replace=True)\n",
        "\n",
        "        # Skip if bootstrap sample doesn't contain both classes\n",
        "        if len(np.unique(y_true[indices])) < 2:\n",
        "            continue\n",
        "\n",
        "        aucs.append(roc_auc_score(y_true[indices], y_proba[indices]))\n",
        "\n",
        "    if len(aucs) == 0:\n",
        "        return np.nan, np.nan\n",
        "\n",
        "    # Compute percentile-based confidence interval\n",
        "    lower = np.percentile(aucs, 100 * (alpha / 2))\n",
        "    upper = np.percentile(aucs, 100 * (1 - alpha / 2))\n",
        "\n",
        "    return float(lower), float(upper)\n",
        "\n",
        "\n",
        "def perform_fold_feature_selection(expr_train, y_train, fold_idx, verbose=True):\n",
        "    \"\"\"\n",
        "    Three-stage feature selection pipeline on training data only.\n",
        "\n",
        "    CRITICAL: This function operates ONLY on training data to prevent\n",
        "    information leakage. Test data must never influence feature selection.\n",
        "\n",
        "    Stage 1: Biological filtering (fold change + Mann-Whitney U test)\n",
        "        - Criteria: |log2FC| > 0.585 (≈1.5-fold) AND p-value < 0.05\n",
        "        - Ensures biological significance and statistical validity\n",
        "\n",
        "    Stage 2: Multi-method consensus (LASSO, SVM-RFE, Random Forest)\n",
        "        - LASSO: L1 regularization for sparse selection\n",
        "        - SVM-RFE: Recursive feature elimination with linear SVM\n",
        "        - Random Forest: Tree-based importance with SMOTE balancing\n",
        "\n",
        "    Stage 3: 2-of-3 consensus voting\n",
        "        - Features selected by at least 2 of 3 methods\n",
        "        - Reduces method-specific bias\n",
        "        - Increases robustness of biomarker selection\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    expr_train : pd.DataFrame\n",
        "        Training expression data (samples × probes)\n",
        "    y_train : array-like\n",
        "        Training labels (0=healthy, 1=cancer)\n",
        "    fold_idx : int or str\n",
        "        Current fold index for logging\n",
        "    verbose : bool, default=True\n",
        "        Whether to print progress information\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    selected_features : list\n",
        "        Probe IDs selected by consensus\n",
        "    feature_info : dict\n",
        "        Detailed statistics about feature selection process\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(f\"\\n    [Fold {fold_idx}] Feature selection on training data only\")\n",
        "        print(f\"      Train samples: {len(expr_train)} ({sum(y_train==0)} HC, {sum(y_train==1)} CRC)\")\n",
        "        print(f\"      Total probes: {expr_train.shape[1]}\")\n",
        "\n",
        "    # Separate control and cancer samples\n",
        "    expr_control = expr_train[y_train == 0]\n",
        "    expr_cancer = expr_train[y_train == 1]\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Stage 1: Biological Filtering\n",
        "    # -------------------------------------------------------------------------\n",
        "    fold_changes = {}\n",
        "    p_values = {}\n",
        "    log2_fold_changes = {}\n",
        "\n",
        "    probe_list = list(expr_train.columns)\n",
        "    raw_pvals = []\n",
        "\n",
        "    # Step 1: Calculate fold changes and raw p-values for all probes\n",
        "    for probe in probe_list:\n",
        "        control_vals = expr_control[probe].values\n",
        "        cancer_vals = expr_cancer[probe].values\n",
        "\n",
        "        control_mean = control_vals.mean()\n",
        "        cancer_mean = cancer_vals.mean()\n",
        "\n",
        "        # Compute log2 fold change\n",
        "        log2fc = cancer_mean - control_mean\n",
        "        log2_fold_changes[probe] = log2fc\n",
        "        fold_changes[probe] = 2 ** log2fc\n",
        "\n",
        "        # Mann-Whitney U test (non-parametric, robust to outliers)\n",
        "        try:\n",
        "            _, pval = mannwhitneyu(control_vals, cancer_vals, alternative='two-sided')\n",
        "        except:\n",
        "            pval = 1.0\n",
        "\n",
        "        raw_pvals.append(pval)\n",
        "        p_values[probe] = pval\n",
        "\n",
        "    # Step 2: Apply Benjamini-Hochberg FDR correction\n",
        "    raw_pvals = np.array(raw_pvals)\n",
        "    n_tests = len(raw_pvals)\n",
        "\n",
        "    # Sort p-values and compute adjusted p-values (q-values)\n",
        "    sorted_indices = np.argsort(raw_pvals)\n",
        "    sorted_pvals = raw_pvals[sorted_indices]\n",
        "\n",
        "    # Benjamini-Hochberg procedure\n",
        "    adjusted_pvals = np.zeros(n_tests)\n",
        "    for i, idx in enumerate(sorted_indices):\n",
        "        rank = i + 1\n",
        "        adjusted_pvals[idx] = min(1.0, raw_pvals[idx] * n_tests / rank)\n",
        "\n",
        "    # Ensure monotonicity (cumulative minimum from the end)\n",
        "    for i in range(n_tests - 2, -1, -1):\n",
        "        if adjusted_pvals[sorted_indices[i]] > adjusted_pvals[sorted_indices[i + 1]]:\n",
        "            adjusted_pvals[sorted_indices[i]] = adjusted_pvals[sorted_indices[i + 1]]\n",
        "\n",
        "    # Store adjusted p-values (q-values)\n",
        "    q_values = {probe: adjusted_pvals[i] for i, probe in enumerate(probe_list)}\n",
        "\n",
        "    # Step 3: Apply biological significance thresholds with FDR-corrected q-values\n",
        "    selected_bio = []\n",
        "    for probe in probe_list:\n",
        "        log2fc = log2_fold_changes[probe]\n",
        "        qval = q_values[probe]\n",
        "\n",
        "        # Criteria: |log2FC| > 1.0 AND FDR-adjusted q-value < 0.05\n",
        "        if abs(log2fc) > 1.0 and qval < 0.05:\n",
        "            selected_bio.append(probe)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"      Stage 1: {len(selected_bio)} probes (|log2FC| > 1.0, FDR q < 0.05)\")\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Stage 2: Multi-method Consensus\n",
        "    # -------------------------------------------------------------------------\n",
        "    if verbose:\n",
        "        print(f\"      Stage 2: Multi-method selection (LASSO, SVM-RFE, RF)\")\n",
        "\n",
        "    X_bio = expr_train[selected_bio].values\n",
        "\n",
        "    # Standardize for methods that require it (LASSO, SVM-RFE)\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X_bio)\n",
        "\n",
        "    # ─────────────────────────────────────────────────────────────────────────\n",
        "    # Method A: LASSO Regularization\n",
        "    # ─────────────────────────────────────────────────────────────────────────\n",
        "    try:\n",
        "        lasso = LassoCV(\n",
        "            cv=3,\n",
        "            random_state=SEED,\n",
        "            max_iter=20000,\n",
        "            n_jobs=-1,\n",
        "            tol=1e-3\n",
        "        )\n",
        "        lasso.fit(X_scaled, y_train)\n",
        "\n",
        "        # Select features with non-zero coefficients\n",
        "        lasso_feat = [selected_bio[i] for i, c in enumerate(lasso.coef_) if c != 0]\n",
        "\n",
        "        # Fallback: select top features by coefficient magnitude if none selected\n",
        "        if len(lasso_feat) == 0:\n",
        "            lasso_coef_abs = np.abs(lasso.coef_)\n",
        "            top_idx = np.argsort(lasso_coef_abs)[-200:]\n",
        "            lasso_feat = [selected_bio[i] for i in top_idx]\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"        LASSO: {len(lasso_feat)} features\")\n",
        "\n",
        "    except Exception as e:\n",
        "        if verbose:\n",
        "            print(f\"        LASSO failed ({str(e)}), using variance fallback\")\n",
        "        var_bio = expr_train[selected_bio].var(axis=0)\n",
        "        lasso_feat = var_bio.nlargest(min(200, len(selected_bio))).index.tolist()\n",
        "\n",
        "    # ─────────────────────────────────────────────────────────────────────────\n",
        "    # Method B: SVM-RFE (Recursive Feature Elimination)\n",
        "    # ─────────────────────────────────────────────────────────────────────────\n",
        "    try:\n",
        "        n_features_target = min(150, len(selected_bio) // 3)\n",
        "\n",
        "        svc = SVC(kernel='linear', class_weight='balanced', random_state=SEED)\n",
        "        rfe = RFE(\n",
        "            estimator=svc,\n",
        "            n_features_to_select=n_features_target,\n",
        "            step=10,\n",
        "            verbose=0\n",
        "        )\n",
        "        rfe.fit(X_scaled, y_train)\n",
        "\n",
        "        svm_rfe_feat = [selected_bio[i] for i in np.where(rfe.support_)[0]]\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"        SVM-RFE: {len(svm_rfe_feat)} features\")\n",
        "\n",
        "    except Exception as e:\n",
        "        if verbose:\n",
        "            print(f\"        SVM-RFE failed ({str(e)}), using variance fallback\")\n",
        "        var_bio = expr_train[selected_bio].var(axis=0)\n",
        "        svm_rfe_feat = var_bio.nlargest(min(150, len(selected_bio))).index.tolist()\n",
        "\n",
        "    # ─────────────────────────────────────────────────────────────────────────\n",
        "    # Method C: Random Forest Feature Importance\n",
        "    # ─────────────────────────────────────────────────────────────────────────\n",
        "    try:\n",
        "        rf = RandomForestClassifier(\n",
        "            n_estimators=200,\n",
        "            random_state=SEED,\n",
        "            n_jobs=-1,\n",
        "            class_weight='balanced'\n",
        "        )\n",
        "        rf.fit(X_bio, y_train)\n",
        "\n",
        "        # Select top features by importance\n",
        "        importances = rf.feature_importances_\n",
        "        n_rf = min(200, len(selected_bio))\n",
        "        top_idx = np.argsort(importances)[-n_rf:]\n",
        "        rf_feat = [selected_bio[i] for i in top_idx]\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"        Random Forest: {len(rf_feat)} features\")\n",
        "\n",
        "    except Exception as e:\n",
        "        if verbose:\n",
        "            print(f\"        Random Forest failed ({str(e)}), using variance fallback\")\n",
        "        var_bio = expr_train[selected_bio].var(axis=0)\n",
        "        rf_feat = var_bio.nlargest(min(200, len(selected_bio))).index.tolist()\n",
        "\n",
        "    # ─────────────────────────────────────────────────────────────────────────\n",
        "    # Stage 3: Consensus Voting (2-of-3)\n",
        "    # ─────────────────────────────────────────────────────────────────────────\n",
        "    feature_votes = Counter()\n",
        "    for feat_set in [set(lasso_feat), set(svm_rfe_feat), set(rf_feat)]:\n",
        "        feature_votes.update(feat_set)\n",
        "\n",
        "    # Select features appearing in at least 2 methods\n",
        "    selected_consensus = [f for f, count in feature_votes.items() if count >= 2]\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"      Stage 3: {len(selected_consensus)} features (≥2/3 consensus)\")\n",
        "\n",
        "    # Ensure minimum feature count for stable model training\n",
        "    if len(selected_consensus) < 20:\n",
        "        sorted_features = sorted(feature_votes.items(), key=lambda x: x[1], reverse=True)\n",
        "        selected_consensus = [f for f, _ in sorted_features[:max(20, len(sorted_features))]]\n",
        "        if verbose:\n",
        "            print(f\"      Adjusted to {len(selected_consensus)} features (minimum threshold)\")\n",
        "\n",
        "    # Compile feature selection statistics\n",
        "    feature_info = {\n",
        "        'n_stage1': len(selected_bio),\n",
        "        'n_lasso': len(lasso_feat),\n",
        "        'n_svm_rfe': len(svm_rfe_feat),\n",
        "        'n_rf': len(rf_feat),\n",
        "        'n_consensus': len(selected_consensus),\n",
        "        'fold_changes': {k: float(v) for k, v in fold_changes.items() if k in selected_consensus},\n",
        "        'p_values': {k: float(v) for k, v in p_values.items() if k in selected_consensus}\n",
        "    }\n",
        "\n",
        "    return selected_consensus, feature_info\n",
        "\n",
        "\n",
        "def create_pipeline(config, n_features):\n",
        "    \"\"\"\n",
        "    Construct sklearn/imblearn pipeline based on model configuration.\n",
        "\n",
        "    Pipeline components are added conditionally based on model requirements:\n",
        "    - SMOTE: For class imbalance handling (optional)\n",
        "    - StandardScaler: For feature normalization (optional)\n",
        "    - Classifier: Final estimator (required)\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    config : dict\n",
        "        Model configuration dictionary with keys:\n",
        "        - use_smote: bool\n",
        "        - use_scaler: bool\n",
        "        - classifier: str ('rf', 'svm', or 'lr')\n",
        "        - param_grid: dict of hyperparameter ranges\n",
        "    n_features : int\n",
        "        Number of features (for SMOTE k_neighbors validation)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pipeline : imblearn.pipeline.Pipeline\n",
        "        Configured pipeline\n",
        "    param_grid : dict\n",
        "        Grid search parameter space\n",
        "    \"\"\"\n",
        "    steps = []\n",
        "\n",
        "    # Add SMOTE if requested (must be first in pipeline)\n",
        "    if config[\"use_smote\"]:\n",
        "        steps.append((\"smote\", SMOTE(random_state=SEED, k_neighbors=3)))\n",
        "\n",
        "    # Add StandardScaler if requested\n",
        "    if config[\"use_scaler\"]:\n",
        "        steps.append((\"scaler\", StandardScaler()))\n",
        "\n",
        "    # Add classifier (always last in pipeline)\n",
        "    if config[\"classifier\"] == \"rf\":\n",
        "        clf = RandomForestClassifier(\n",
        "            random_state=SEED,\n",
        "            n_jobs=-1,\n",
        "            class_weight=\"balanced_subsample\"\n",
        "        )\n",
        "    elif config[\"classifier\"] == \"svm\":\n",
        "        clf = SVC(\n",
        "            probability=True,\n",
        "            random_state=SEED,\n",
        "            class_weight=\"balanced\"\n",
        "        )\n",
        "    elif config[\"classifier\"] == \"lr\":\n",
        "        clf = LogisticRegression(\n",
        "            max_iter=500,\n",
        "            random_state=SEED,\n",
        "            class_weight=\"balanced\"\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown classifier: {config['classifier']}\")\n",
        "\n",
        "    steps.append((\"clf\", clf))\n",
        "\n",
        "    # Create pipeline\n",
        "    pipeline = ImbPipeline(steps)\n",
        "    param_grid = config[\"param_grid\"].copy()\n",
        "\n",
        "    return pipeline, param_grid\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. Nested Cross-Validation with Fold-wise Feature Selection\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 4: Execute Nested Cross-Validation\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Initialize result storage\n",
        "results = {}\n",
        "oof_predictions = {}\n",
        "fold_feature_lists = {}\n",
        "fold_feature_info = {}\n",
        "\n",
        "# Iterate through each model configuration\n",
        "for model_name, model_cfg in model_configs.items():\n",
        "\n",
        "    print(\"\\n\" + \"-\" * 80)\n",
        "    print(f\"Model: {model_name}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # Initialize out-of-fold (OOF) prediction arrays\n",
        "    oof_proba = np.zeros_like(labels_full, dtype=float)\n",
        "    oof_pred = np.zeros_like(labels_full, dtype=int)\n",
        "\n",
        "    # Track performance across folds\n",
        "    train_auc_folds = []\n",
        "    test_auc_folds = []\n",
        "\n",
        "    fold_feature_lists[model_name] = {}\n",
        "    fold_feature_info[model_name] = {}\n",
        "\n",
        "    # ─────────────────────────────────────────────────────────────────────────\n",
        "    # Outer CV Loop: Iterate through train/test splits\n",
        "    # ─────────────────────────────────────────────────────────────────────────\n",
        "    for fold_idx, (train_idx, test_idx) in enumerate(outer_cv.split(expr_log_full, labels_full), start=1):\n",
        "\n",
        "        # Extract training fold data\n",
        "        train_sample_ids = [sample_ids_full[i] for i in train_idx]\n",
        "        expr_train_fold = expr_log_full.loc[train_sample_ids].copy()\n",
        "        y_train_fold = labels_full[train_idx]\n",
        "\n",
        "        # Extract test fold data\n",
        "        test_sample_ids = [sample_ids_full[i] for i in test_idx]\n",
        "        y_test_fold = labels_full[test_idx]\n",
        "\n",
        "        # Report class distribution\n",
        "        train_pos = int(y_train_fold.sum())\n",
        "        test_pos = int(y_test_fold.sum())\n",
        "        print(f\"\\n  Fold {fold_idx} class distribution:\")\n",
        "        print(f\"    Train: {train_pos}/{len(y_train_fold)} ({train_pos/len(y_train_fold):.2%}) CRC\")\n",
        "        print(f\"    Test:  {test_pos}/{len(y_test_fold)} ({test_pos/len(y_test_fold):.2%}) CRC\")\n",
        "\n",
        "        # ─────────────────────────────────────────────────────────────────────\n",
        "        # CRITICAL: Feature selection on training fold ONLY\n",
        "        # ─────────────────────────────────────────────────────────────────────\n",
        "        selected_features, feat_info = perform_fold_feature_selection(\n",
        "            expr_train_fold,\n",
        "            y_train_fold,\n",
        "            fold_idx,\n",
        "            verbose=True\n",
        "        )\n",
        "\n",
        "        # Store selected features for stability analysis\n",
        "        fold_feature_lists[model_name][f'fold_{fold_idx}'] = selected_features\n",
        "        fold_feature_info[model_name][f'fold_{fold_idx}'] = feat_info\n",
        "\n",
        "        # ─────────────────────────────────────────────────────────────────────\n",
        "        # CRITICAL: Normalization using training statistics ONLY\n",
        "        # ─────────────────────────────────────────────────────────────────────\n",
        "        # Extract selected features (log2 scale, NO normalization here!)\n",
        "        # Normalization will be handled INSIDE the pipeline for consistency\n",
        "        # ─────────────────────────────────────────────────────────────────────\n",
        "        X_train_fold = expr_train_fold[selected_features].values\n",
        "        X_test_fold = expr_log_full.loc[test_sample_ids][selected_features].values\n",
        "\n",
        "        print(f\"\\n    [Fold {fold_idx}] Training with {len(selected_features)} features\")\n",
        "\n",
        "        # ─────────────────────────────────────────────────────────────────────\n",
        "        # Inner CV: Hyperparameter tuning via Grid Search\n",
        "        # ─────────────────────────────────────────────────────────────────────\n",
        "        pipeline, param_grid = create_pipeline(model_cfg, len(selected_features))\n",
        "\n",
        "        gs = GridSearchCV(\n",
        "            estimator=pipeline,\n",
        "            param_grid=param_grid,\n",
        "            scoring=\"roc_auc\",\n",
        "            cv=inner_cv,\n",
        "            n_jobs=-1,\n",
        "            refit=True,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        # Train model with hyperparameter optimization\n",
        "        gs.fit(X_train_fold, y_train_fold)\n",
        "\n",
        "        # ─────────────────────────────────────────────────────────────────────\n",
        "        # Evaluate on training set (for overfitting detection)\n",
        "        # ─────────────────────────────────────────────────────────────────────\n",
        "        train_proba = gs.predict_proba(X_train_fold)[:, 1]\n",
        "        train_auc = roc_auc_score(y_train_fold, train_proba)\n",
        "        train_auc_folds.append(train_auc)\n",
        "\n",
        "        # ─────────────────────────────────────────────────────────────────────\n",
        "        # Evaluate on test set (unbiased performance)\n",
        "        # ─────────────────────────────────────────────────────────────────────\n",
        "        test_proba = gs.predict_proba(X_test_fold)[:, 1]\n",
        "        test_pred = (test_proba >= 0.5).astype(int)\n",
        "\n",
        "        test_auc = roc_auc_score(y_test_fold, test_proba)\n",
        "        test_auc_folds.append(test_auc)\n",
        "\n",
        "        # Store out-of-fold predictions for final aggregation\n",
        "        oof_proba[test_idx] = test_proba\n",
        "        oof_pred[test_idx] = test_pred\n",
        "\n",
        "        # Report fold performance\n",
        "        print(f\"    Best inner CV AUC: {gs.best_score_:.4f}\")\n",
        "        print(f\"    Train AUC: {train_auc:.4f}\")\n",
        "        print(f\"    Test AUC:  {test_auc:.4f}\")\n",
        "\n",
        "        # Warn if significant overfitting detected\n",
        "        if train_auc - test_auc > 0.15:\n",
        "            print(f\"    ⚠️  WARNING: Potential overfitting (gap = {train_auc - test_auc:.4f})\")\n",
        "\n",
        "    # ─────────────────────────────────────────────────────────────────────────\n",
        "    # Aggregate Out-of-Fold Performance Metrics\n",
        "    # ─────────────────────────────────────────────────────────────────────────\n",
        "    cm = confusion_matrix(labels_full, oof_pred)\n",
        "    auc = roc_auc_score(labels_full, oof_proba)\n",
        "    acc = accuracy_score(labels_full, oof_pred)\n",
        "    bal_acc = balanced_accuracy_score(labels_full, oof_pred)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        labels_full, oof_pred, average=\"binary\", zero_division=0\n",
        "    )\n",
        "\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
        "    sensitivity = recall\n",
        "\n",
        "    # Compute 95% confidence interval via bootstrap\n",
        "    ci_lower, ci_upper = bootstrap_auc_ci(\n",
        "        labels_full, oof_proba, n_bootstrap=1000, alpha=0.05\n",
        "    )\n",
        "\n",
        "    train_auc_mean = float(np.mean(train_auc_folds))\n",
        "    test_auc_mean = float(np.mean(test_auc_folds))\n",
        "\n",
        "    # Report aggregated performance\n",
        "    print(\"\\n  \" + \"=\" * 76)\n",
        "    print(\"  OUT-OF-FOLD PERFORMANCE (Unbiased Estimate)\")\n",
        "    print(\"  \" + \"=\" * 76)\n",
        "    print(f\"    ROC-AUC:           {auc:.4f}\")\n",
        "    print(f\"    95% CI:            [{ci_lower:.4f}, {ci_upper:.4f}]\")\n",
        "    print(f\"    Accuracy:          {acc:.4f}\")\n",
        "    print(f\"    Balanced Accuracy: {bal_acc:.4f}\")\n",
        "    print(f\"    Precision:         {precision:.4f}\")\n",
        "    print(f\"    Sensitivity:       {sensitivity:.4f}\")\n",
        "    print(f\"    Specificity:       {specificity:.4f}\")\n",
        "    print(f\"    F1-score:          {f1:.4f}\")\n",
        "    print(f\"    Mean Train AUC:    {train_auc_mean:.4f}\")\n",
        "    print(f\"    Mean Test AUC:     {test_auc_mean:.4f}\")\n",
        "    print(\"    Confusion Matrix:\")\n",
        "    print(\"    \" + str(cm).replace('\\n', '\\n    '))\n",
        "\n",
        "    # ─────────────────────────────────────────────────────────────────────────\n",
        "    # Feature Stability Analysis\n",
        "    # ─────────────────────────────────────────────────────────────────────────\n",
        "    feature_frequency = Counter()\n",
        "    for features in fold_feature_lists[model_name].values():\n",
        "        feature_frequency.update(features)\n",
        "\n",
        "    n_folds = len(fold_feature_lists[model_name])\n",
        "    highly_stable = [f for f, count in feature_frequency.items() if count >= 4]\n",
        "\n",
        "    print(f\"\\n  Feature Stability Across {n_folds} Folds:\")\n",
        "    print(f\"    Selected 5/5: {sum(1 for c in feature_frequency.values() if c == 5)}\")\n",
        "    print(f\"    Selected 4/5: {sum(1 for c in feature_frequency.values() if c == 4)}\")\n",
        "    print(f\"    Selected 3/5: {sum(1 for c in feature_frequency.values() if c == 3)}\")\n",
        "    print(f\"    Highly stable (≥4/5): {len(highly_stable)}\")\n",
        "\n",
        "    # Store comprehensive results\n",
        "    results[model_name] = {\n",
        "        \"oof_auc\": float(auc),\n",
        "        \"oof_auc_ci\": [ci_lower, ci_upper],\n",
        "        \"oof_accuracy\": float(acc),\n",
        "        \"oof_balanced_accuracy\": float(bal_acc),\n",
        "        \"precision\": float(precision),\n",
        "        \"recall_sensitivity\": float(sensitivity),\n",
        "        \"specificity\": float(specificity),\n",
        "        \"f1_score\": float(f1),\n",
        "        \"confusion_matrix\": cm.tolist(),\n",
        "        \"train_auc_mean\": train_auc_mean,\n",
        "        \"train_auc_std\": float(np.std(train_auc_folds)),\n",
        "        \"test_auc_mean\": test_auc_mean,\n",
        "        \"test_auc_std\": float(np.std(test_auc_folds)),\n",
        "        \"feature_stability\": {\n",
        "            \"n_folds\": n_folds,\n",
        "            \"n_highly_stable\": len(highly_stable),\n",
        "            \"highly_stable_features\": highly_stable,\n",
        "            \"feature_frequency\": {str(k): int(v) for k, v in feature_frequency.most_common(50)}\n",
        "        }\n",
        "    }\n",
        "\n",
        "    oof_predictions[model_name] = {\n",
        "        \"y_true\": labels_full.copy(),\n",
        "        \"y_proba\": oof_proba.copy(),\n",
        "        \"y_pred\": oof_pred.copy()\n",
        "    }\n",
        "\n",
        "# ==============================================================================\n",
        "# 6. Model Selection and Comparison\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 5: Model Selection Summary\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Select best model by OOF AUC\n",
        "best_name = max(results, key=lambda k: results[k][\"oof_auc\"])\n",
        "\n",
        "print(\"\\nAll Models (ranked by OOF AUC):\")\n",
        "for m_name in sorted(results, key=lambda k: results[k][\"oof_auc\"], reverse=True):\n",
        "    res = results[m_name]\n",
        "    print(f\"  {m_name:25s}: AUC = {res['oof_auc']:.4f} \"\n",
        "          f\"(95% CI [{res['oof_auc_ci'][0]:.4f}, {res['oof_auc_ci'][1]:.4f}])\")\n",
        "\n",
        "print(f\"\\n{'=' * 80}\")\n",
        "print(f\"BEST MODEL: {best_name}\")\n",
        "print(f\"{'=' * 80}\")\n",
        "print(f\"  Out-of-fold AUC:       {results[best_name]['oof_auc']:.4f}\")\n",
        "print(f\"  95% Confidence Interval: [{results[best_name]['oof_auc_ci'][0]:.4f}, \"\n",
        "      f\"{results[best_name]['oof_auc_ci'][1]:.4f}]\")\n",
        "print(f\"  Accuracy:              {results[best_name]['oof_accuracy']:.4f}\")\n",
        "print(f\"  Balanced Accuracy:     {results[best_name]['oof_balanced_accuracy']:.4f}\")\n",
        "print(f\"  Sensitivity:           {results[best_name]['recall_sensitivity']:.4f}\")\n",
        "print(f\"  Specificity:           {results[best_name]['specificity']:.4f}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 7. Final Model Training on Full Dataset\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 6: Train Final Model on Full Dataset\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\n[NOTE] Performance metrics reported above are from nested CV.\")\n",
        "print(\"        This final model is for:\")\n",
        "print(\"        1. Biological interpretation (feature importance, SHAP)\")\n",
        "print(\"        2. External validation on independent datasets\")\n",
        "print(\"        3. Future clinical deployment\")\n",
        "\n",
        "# Perform feature selection on full dataset\n",
        "print(\"\\n[INFO] Performing feature selection on full dataset...\")\n",
        "expr_full_df = pd.DataFrame(\n",
        "    expr_log_full.values,\n",
        "    index=sample_ids_full,\n",
        "    columns=expr_log_full.columns\n",
        ")\n",
        "\n",
        "final_features, final_feat_info = perform_fold_feature_selection(\n",
        "    expr_full_df,\n",
        "    labels_full,\n",
        "    fold_idx=\"FULL\",\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(f\"\\n[INFO] Selected {len(final_features)} features from full dataset\")\n",
        "\n",
        "X_final = expr_full_df[final_features].values\n",
        "\n",
        "# Store for downstream analysis\n",
        "feature_cols = final_features\n",
        "X = X_final\n",
        "y = labels_full\n",
        "\n",
        "# Train final model with hyperparameter optimization\n",
        "best_cfg = model_configs[best_name]\n",
        "final_pipeline, final_param_grid = create_pipeline(best_cfg, len(final_features))\n",
        "\n",
        "final_model = GridSearchCV(\n",
        "    estimator=final_pipeline,\n",
        "    param_grid=final_param_grid,\n",
        "    scoring=\"roc_auc\",\n",
        "    cv=inner_cv,\n",
        "    n_jobs=-1,\n",
        "    refit=True,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "final_model.fit(X_final, labels_full)\n",
        "\n",
        "# Store best model and parameters\n",
        "best_full_model = final_model.best_estimator_\n",
        "best_full_params = final_model.best_params_\n",
        "\n",
        "results[best_name][\"best_estimator\"] = best_full_model\n",
        "results[best_name][\"best_params\"] = best_full_params\n",
        "\n",
        "print(f\"\\nFinal Model Configuration:\")\n",
        "print(f\"  Model type: {best_name}\")\n",
        "print(f\"  Features: {len(feature_cols)}\")\n",
        "print(f\"  Hyperparameters:\")\n",
        "for k, v in best_full_params.items():\n",
        "    print(f\"    {k}: {v}\")\n",
        "\n",
        "print(f\"\\n[INFO] Variables ready for downstream analysis:\")\n",
        "print(f\"  - best_full_model: Trained classifier\")\n",
        "print(f\"  - feature_cols: Selected feature list ({len(feature_cols)} probes)\")\n",
        "print(f\"  - X: Normalized feature matrix ({X.shape})\")\n",
        "print(f\"  - y: Binary labels ({len(y)})\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 8. Save Comprehensive Results\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 7: Save Results and Visualizations\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Save nested CV results as JSON\n",
        "json_path = os.path.join(RESULT_DIR, \"nested_cv_results.json\")\n",
        "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    # Convert non-serializable objects\n",
        "    json_results = {}\n",
        "    for model_name, res in results.items():\n",
        "        json_results[model_name] = {\n",
        "            k: v for k, v in res.items()\n",
        "            if k not in [\"best_estimator\"]  # Skip non-serializable objects\n",
        "        }\n",
        "    json.dump(json_results, f, indent=2, ensure_ascii=False)\n",
        "print(f\"[INFO] Results saved to: {json_path}\")\n",
        "\n",
        "# Save feature stability analysis\n",
        "for model_name in results:\n",
        "    feature_freq = results[model_name][\"feature_stability\"][\"feature_frequency\"]\n",
        "    df_stability = pd.DataFrame(\n",
        "        list(feature_freq.items()),\n",
        "        columns=[\"Probe_ID\", \"Selection_Frequency\"]\n",
        "    ).sort_values(\"Selection_Frequency\", ascending=False)\n",
        "\n",
        "    stability_path = os.path.join(RESULT_DIR, f\"feature_stability_{model_name}.csv\")\n",
        "    df_stability.to_csv(stability_path, index=False)\n",
        "\n",
        "print(f\"[INFO] Feature stability tables saved for all models\")\n",
        "\n",
        "# Generate ROC curve plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "for model_name in results:\n",
        "    y_true = oof_predictions[model_name][\"y_true\"]\n",
        "    y_proba = oof_predictions[model_name][\"y_proba\"]\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_proba)\n",
        "    auc_val = results[model_name][\"oof_auc\"]\n",
        "    ci_low, ci_high = results[model_name][\"oof_auc_ci\"]\n",
        "\n",
        "    plt.plot(\n",
        "        fpr, tpr,\n",
        "        label=f'{model_name}: AUC={auc_val:.3f} [{ci_low:.3f}-{ci_high:.3f}]',\n",
        "        linewidth=2\n",
        "    )\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random', linewidth=1)\n",
        "plt.xlabel('False Positive Rate', fontsize=12)\n",
        "plt.ylabel('True Positive Rate', fontsize=12)\n",
        "plt.title('ROC Curves - Out-of-Fold Predictions (Nested CV)', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc='lower right', fontsize=9)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "\n",
        "roc_path = os.path.join(RESULT_DIR, \"roc_curves_nested_cv.png\")\n",
        "plt.savefig(roc_path, dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(f\"[INFO] ROC curves saved to: {roc_path}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"✓ NESTED CROSS-VALIDATION COMPLETE\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nBest model: {best_name}\")\n",
        "print(f\"Unbiased OOF AUC: {results[best_name]['oof_auc']:.4f}\")\n",
        "print(f\"95% CI: [{results[best_name]['oof_auc_ci'][0]:.4f}, {results[best_name]['oof_auc_ci'][1]:.4f}]\")\n",
        "print(f\"\\nResults saved to: {RESULT_DIR}\")\n",
        "print(\"=\" * 80 + \"\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPfY6yljyofkgVmtlAlS2is",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}