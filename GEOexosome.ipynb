{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jay99Sohn/GEOexosome/blob/main/GEOexosome.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73GjvE8uJRGI",
        "outputId": "08d783b5-57ba-4e46-82cd-04d8a59863ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting GEOparse\n",
            "  Downloading GEOparse-2.0.4-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.12/dist-packages (0.14.0)\n",
            "Requirement already satisfied: shap in /usr/local/lib/python3.12/dist-packages (0.50.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.12/dist-packages (from GEOparse) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.17 in /usr/local/lib/python3.12/dist-packages (from GEOparse) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from GEOparse) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.12/dist-packages (from GEOparse) (4.67.1)\n",
            "Requirement already satisfied: scipy<2,>=1.11.4 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn) (1.16.3)\n",
            "Requirement already satisfied: scikit-learn<2,>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn) (1.6.1)\n",
            "Requirement already satisfied: joblib<2,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn) (3.6.0)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.12/dist-packages (from shap) (25.0)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.12/dist-packages (from shap) (0.0.8)\n",
            "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.12/dist-packages (from shap) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from shap) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from shap) (4.15.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.54->shap) (0.43.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.17->GEOparse) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.17->GEOparse) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.21.0->GEOparse) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.21.0->GEOparse) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.21.0->GEOparse) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.21.0->GEOparse) (2025.11.12)\n",
            "Downloading GEOparse-2.0.4-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: GEOparse\n",
            "Successfully installed GEOparse-2.0.4\n",
            "\n",
            "============================================================\n",
            "ENVIRONMENT & LIBRARY SETUP\n",
            "============================================================\n",
            "[INFO] Google Colab detected. Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "[INFO] Drive mounted. Saving results to: /content/drive/MyDrive/geoexosome_results\n",
            "\n",
            "============================================================\n",
            "✓ Setup Complete\n",
            "  - Random seed: 42\n",
            "  - Output path: /content/drive/MyDrive/geoexosome_results\n",
            "============================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Cell 0: Environment & Library Setup\n",
        "\n",
        "# Optional: install required packages when running on Colab\n",
        "# Uncomment the line below if running on Google Colab for the first time:\n",
        "!pip install GEOparse imbalanced-learn shap seaborn matplotlib\n",
        "\n",
        "# ============================================================\n",
        "# Standard Library & Third-party Imports\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import GEOparse\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    roc_auc_score,\n",
        "    roc_curve,\n",
        "    accuracy_score\n",
        ")\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import shap\n",
        "\n",
        "# ============================================================\n",
        "# Configuration & Reproducibility\n",
        "# ============================================================\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "\n",
        "# Set visualization style for reproducible plots\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams[\"figure.dpi\"] = 100\n",
        "plt.rcParams[\"savefig.dpi\"] = 300\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ENVIRONMENT & LIBRARY SETUP\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ============================================================\n",
        "# Environment Detection & Path Configuration\n",
        "# ============================================================\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import drive\n",
        "    print(\"[INFO] Google Colab detected. Mounting Google Drive...\")\n",
        "    drive.mount(\"/content/drive\")\n",
        "    base_save_path = \"/content/drive/MyDrive/geoexosome_results\"\n",
        "    print(f\"[INFO] Drive mounted. Saving results to: {base_save_path}\")\n",
        "else:\n",
        "    base_save_path = \"./geoexosome_results\"\n",
        "    print(f\"[INFO] Local environment detected. Saving results to: {base_save_path}\")\n",
        "\n",
        "os.makedirs(base_save_path, exist_ok=True)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"✓ Setup Complete\")\n",
        "print(f\"  - Random seed: {SEED}\")\n",
        "print(f\"  - Output path: {base_save_path}\")\n",
        "print(\"=\" * 60 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFGnFGcHLgNz",
        "outputId": "6d6db5ea-9636-41db-9aef-0f67108461b0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "27-Dec-2025 06:16:32 INFO GEOparse - Downloading ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE39nnn/GSE39833/soft/GSE39833_family.soft.gz to ./data/GSE39833_family.soft.gz\n",
            "INFO:GEOparse:Downloading ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE39nnn/GSE39833/soft/GSE39833_family.soft.gz to ./data/GSE39833_family.soft.gz\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 1: LOAD GEO DATASET (GSE39833)\n",
            "================================================================================\n",
            "[INFO] Downloading GEO dataset: GSE39833\n",
            "[INFO] This may take 1-2 minutes depending on network speed...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 11.4M/11.4M [00:00<00:00, 38.4MB/s]\n",
            "27-Dec-2025 06:16:32 DEBUG downloader - Size validation passed\n",
            "DEBUG:GEOparse:Size validation passed\n",
            "27-Dec-2025 06:16:32 DEBUG downloader - Moving /tmp/tmpm3_xvvc3 to /content/data/GSE39833_family.soft.gz\n",
            "DEBUG:GEOparse:Moving /tmp/tmpm3_xvvc3 to /content/data/GSE39833_family.soft.gz\n",
            "27-Dec-2025 06:16:32 DEBUG downloader - Successfully downloaded ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE39nnn/GSE39833/soft/GSE39833_family.soft.gz\n",
            "DEBUG:GEOparse:Successfully downloaded ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE39nnn/GSE39833/soft/GSE39833_family.soft.gz\n",
            "27-Dec-2025 06:16:32 INFO GEOparse - Parsing ./data/GSE39833_family.soft.gz: \n",
            "INFO:GEOparse:Parsing ./data/GSE39833_family.soft.gz: \n",
            "27-Dec-2025 06:16:32 DEBUG GEOparse - DATABASE: GeoMiame\n",
            "DEBUG:GEOparse:DATABASE: GeoMiame\n",
            "27-Dec-2025 06:16:32 DEBUG GEOparse - SERIES: GSE39833\n",
            "DEBUG:GEOparse:SERIES: GSE39833\n",
            "27-Dec-2025 06:16:32 DEBUG GEOparse - PLATFORM: GPL14767\n",
            "DEBUG:GEOparse:PLATFORM: GPL14767\n",
            "27-Dec-2025 06:16:32 DEBUG GEOparse - SAMPLE: GSM980024\n",
            "DEBUG:GEOparse:SAMPLE: GSM980024\n",
            "27-Dec-2025 06:16:32 DEBUG GEOparse - SAMPLE: GSM980025\n",
            "DEBUG:GEOparse:SAMPLE: GSM980025\n",
            "27-Dec-2025 06:16:32 DEBUG GEOparse - SAMPLE: GSM980026\n",
            "DEBUG:GEOparse:SAMPLE: GSM980026\n",
            "27-Dec-2025 06:16:33 DEBUG GEOparse - SAMPLE: GSM980027\n",
            "DEBUG:GEOparse:SAMPLE: GSM980027\n",
            "27-Dec-2025 06:16:33 DEBUG GEOparse - SAMPLE: GSM980028\n",
            "DEBUG:GEOparse:SAMPLE: GSM980028\n",
            "27-Dec-2025 06:16:33 DEBUG GEOparse - SAMPLE: GSM980029\n",
            "DEBUG:GEOparse:SAMPLE: GSM980029\n",
            "27-Dec-2025 06:16:33 DEBUG GEOparse - SAMPLE: GSM980030\n",
            "DEBUG:GEOparse:SAMPLE: GSM980030\n",
            "27-Dec-2025 06:16:33 DEBUG GEOparse - SAMPLE: GSM980031\n",
            "DEBUG:GEOparse:SAMPLE: GSM980031\n",
            "27-Dec-2025 06:16:33 DEBUG GEOparse - SAMPLE: GSM980032\n",
            "DEBUG:GEOparse:SAMPLE: GSM980032\n",
            "27-Dec-2025 06:16:33 DEBUG GEOparse - SAMPLE: GSM980033\n",
            "DEBUG:GEOparse:SAMPLE: GSM980033\n",
            "27-Dec-2025 06:16:33 DEBUG GEOparse - SAMPLE: GSM980034\n",
            "DEBUG:GEOparse:SAMPLE: GSM980034\n",
            "27-Dec-2025 06:16:33 DEBUG GEOparse - SAMPLE: GSM980035\n",
            "DEBUG:GEOparse:SAMPLE: GSM980035\n",
            "27-Dec-2025 06:16:33 DEBUG GEOparse - SAMPLE: GSM980036\n",
            "DEBUG:GEOparse:SAMPLE: GSM980036\n",
            "27-Dec-2025 06:16:33 DEBUG GEOparse - SAMPLE: GSM980037\n",
            "DEBUG:GEOparse:SAMPLE: GSM980037\n",
            "27-Dec-2025 06:16:33 DEBUG GEOparse - SAMPLE: GSM980038\n",
            "DEBUG:GEOparse:SAMPLE: GSM980038\n",
            "27-Dec-2025 06:16:33 DEBUG GEOparse - SAMPLE: GSM980039\n",
            "DEBUG:GEOparse:SAMPLE: GSM980039\n",
            "27-Dec-2025 06:16:33 DEBUG GEOparse - SAMPLE: GSM980040\n",
            "DEBUG:GEOparse:SAMPLE: GSM980040\n",
            "27-Dec-2025 06:16:33 DEBUG GEOparse - SAMPLE: GSM980041\n",
            "DEBUG:GEOparse:SAMPLE: GSM980041\n",
            "27-Dec-2025 06:16:33 DEBUG GEOparse - SAMPLE: GSM980042\n",
            "DEBUG:GEOparse:SAMPLE: GSM980042\n",
            "27-Dec-2025 06:16:33 DEBUG GEOparse - SAMPLE: GSM980043\n",
            "DEBUG:GEOparse:SAMPLE: GSM980043\n",
            "27-Dec-2025 06:16:33 DEBUG GEOparse - SAMPLE: GSM980044\n",
            "DEBUG:GEOparse:SAMPLE: GSM980044\n",
            "27-Dec-2025 06:16:33 DEBUG GEOparse - SAMPLE: GSM980045\n",
            "DEBUG:GEOparse:SAMPLE: GSM980045\n",
            "27-Dec-2025 06:16:33 DEBUG GEOparse - SAMPLE: GSM980046\n",
            "DEBUG:GEOparse:SAMPLE: GSM980046\n",
            "27-Dec-2025 06:16:33 DEBUG GEOparse - SAMPLE: GSM980047\n",
            "DEBUG:GEOparse:SAMPLE: GSM980047\n",
            "27-Dec-2025 06:16:33 DEBUG GEOparse - SAMPLE: GSM980048\n",
            "DEBUG:GEOparse:SAMPLE: GSM980048\n",
            "27-Dec-2025 06:16:33 DEBUG GEOparse - SAMPLE: GSM980049\n",
            "DEBUG:GEOparse:SAMPLE: GSM980049\n",
            "27-Dec-2025 06:16:33 DEBUG GEOparse - SAMPLE: GSM980050\n",
            "DEBUG:GEOparse:SAMPLE: GSM980050\n",
            "27-Dec-2025 06:16:33 DEBUG GEOparse - SAMPLE: GSM980051\n",
            "DEBUG:GEOparse:SAMPLE: GSM980051\n",
            "27-Dec-2025 06:16:33 DEBUG GEOparse - SAMPLE: GSM980052\n",
            "DEBUG:GEOparse:SAMPLE: GSM980052\n",
            "27-Dec-2025 06:16:33 DEBUG GEOparse - SAMPLE: GSM980053\n",
            "DEBUG:GEOparse:SAMPLE: GSM980053\n",
            "27-Dec-2025 06:16:33 DEBUG GEOparse - SAMPLE: GSM980054\n",
            "DEBUG:GEOparse:SAMPLE: GSM980054\n",
            "27-Dec-2025 06:16:34 DEBUG GEOparse - SAMPLE: GSM980055\n",
            "DEBUG:GEOparse:SAMPLE: GSM980055\n",
            "27-Dec-2025 06:16:34 DEBUG GEOparse - SAMPLE: GSM980056\n",
            "DEBUG:GEOparse:SAMPLE: GSM980056\n",
            "27-Dec-2025 06:16:34 DEBUG GEOparse - SAMPLE: GSM980057\n",
            "DEBUG:GEOparse:SAMPLE: GSM980057\n",
            "27-Dec-2025 06:16:34 DEBUG GEOparse - SAMPLE: GSM980058\n",
            "DEBUG:GEOparse:SAMPLE: GSM980058\n",
            "27-Dec-2025 06:16:34 DEBUG GEOparse - SAMPLE: GSM980059\n",
            "DEBUG:GEOparse:SAMPLE: GSM980059\n",
            "27-Dec-2025 06:16:34 DEBUG GEOparse - SAMPLE: GSM980060\n",
            "DEBUG:GEOparse:SAMPLE: GSM980060\n",
            "27-Dec-2025 06:16:34 DEBUG GEOparse - SAMPLE: GSM980061\n",
            "DEBUG:GEOparse:SAMPLE: GSM980061\n",
            "27-Dec-2025 06:16:34 DEBUG GEOparse - SAMPLE: GSM980062\n",
            "DEBUG:GEOparse:SAMPLE: GSM980062\n",
            "27-Dec-2025 06:16:34 DEBUG GEOparse - SAMPLE: GSM980063\n",
            "DEBUG:GEOparse:SAMPLE: GSM980063\n",
            "27-Dec-2025 06:16:34 DEBUG GEOparse - SAMPLE: GSM980064\n",
            "DEBUG:GEOparse:SAMPLE: GSM980064\n",
            "27-Dec-2025 06:16:34 DEBUG GEOparse - SAMPLE: GSM980065\n",
            "DEBUG:GEOparse:SAMPLE: GSM980065\n",
            "27-Dec-2025 06:16:34 DEBUG GEOparse - SAMPLE: GSM980066\n",
            "DEBUG:GEOparse:SAMPLE: GSM980066\n",
            "27-Dec-2025 06:16:34 DEBUG GEOparse - SAMPLE: GSM980067\n",
            "DEBUG:GEOparse:SAMPLE: GSM980067\n",
            "27-Dec-2025 06:16:34 DEBUG GEOparse - SAMPLE: GSM980068\n",
            "DEBUG:GEOparse:SAMPLE: GSM980068\n",
            "27-Dec-2025 06:16:34 DEBUG GEOparse - SAMPLE: GSM980069\n",
            "DEBUG:GEOparse:SAMPLE: GSM980069\n",
            "27-Dec-2025 06:16:34 DEBUG GEOparse - SAMPLE: GSM980070\n",
            "DEBUG:GEOparse:SAMPLE: GSM980070\n",
            "27-Dec-2025 06:16:34 DEBUG GEOparse - SAMPLE: GSM980071\n",
            "DEBUG:GEOparse:SAMPLE: GSM980071\n",
            "27-Dec-2025 06:16:34 DEBUG GEOparse - SAMPLE: GSM980072\n",
            "DEBUG:GEOparse:SAMPLE: GSM980072\n",
            "27-Dec-2025 06:16:34 DEBUG GEOparse - SAMPLE: GSM980073\n",
            "DEBUG:GEOparse:SAMPLE: GSM980073\n",
            "27-Dec-2025 06:16:34 DEBUG GEOparse - SAMPLE: GSM980074\n",
            "DEBUG:GEOparse:SAMPLE: GSM980074\n",
            "27-Dec-2025 06:16:35 DEBUG GEOparse - SAMPLE: GSM980075\n",
            "DEBUG:GEOparse:SAMPLE: GSM980075\n",
            "27-Dec-2025 06:16:35 DEBUG GEOparse - SAMPLE: GSM980076\n",
            "DEBUG:GEOparse:SAMPLE: GSM980076\n",
            "27-Dec-2025 06:16:35 DEBUG GEOparse - SAMPLE: GSM980077\n",
            "DEBUG:GEOparse:SAMPLE: GSM980077\n",
            "27-Dec-2025 06:16:35 DEBUG GEOparse - SAMPLE: GSM980078\n",
            "DEBUG:GEOparse:SAMPLE: GSM980078\n",
            "27-Dec-2025 06:16:35 DEBUG GEOparse - SAMPLE: GSM980079\n",
            "DEBUG:GEOparse:SAMPLE: GSM980079\n",
            "27-Dec-2025 06:16:35 DEBUG GEOparse - SAMPLE: GSM980080\n",
            "DEBUG:GEOparse:SAMPLE: GSM980080\n",
            "27-Dec-2025 06:16:35 DEBUG GEOparse - SAMPLE: GSM980081\n",
            "DEBUG:GEOparse:SAMPLE: GSM980081\n",
            "27-Dec-2025 06:16:35 DEBUG GEOparse - SAMPLE: GSM980082\n",
            "DEBUG:GEOparse:SAMPLE: GSM980082\n",
            "27-Dec-2025 06:16:35 DEBUG GEOparse - SAMPLE: GSM980083\n",
            "DEBUG:GEOparse:SAMPLE: GSM980083\n",
            "27-Dec-2025 06:16:35 DEBUG GEOparse - SAMPLE: GSM980084\n",
            "DEBUG:GEOparse:SAMPLE: GSM980084\n",
            "27-Dec-2025 06:16:35 DEBUG GEOparse - SAMPLE: GSM980085\n",
            "DEBUG:GEOparse:SAMPLE: GSM980085\n",
            "27-Dec-2025 06:16:35 DEBUG GEOparse - SAMPLE: GSM980086\n",
            "DEBUG:GEOparse:SAMPLE: GSM980086\n",
            "27-Dec-2025 06:16:36 DEBUG GEOparse - SAMPLE: GSM980087\n",
            "DEBUG:GEOparse:SAMPLE: GSM980087\n",
            "27-Dec-2025 06:16:36 DEBUG GEOparse - SAMPLE: GSM980088\n",
            "DEBUG:GEOparse:SAMPLE: GSM980088\n",
            "27-Dec-2025 06:16:36 DEBUG GEOparse - SAMPLE: GSM980089\n",
            "DEBUG:GEOparse:SAMPLE: GSM980089\n",
            "27-Dec-2025 06:16:36 DEBUG GEOparse - SAMPLE: GSM980090\n",
            "DEBUG:GEOparse:SAMPLE: GSM980090\n",
            "27-Dec-2025 06:16:36 DEBUG GEOparse - SAMPLE: GSM980091\n",
            "DEBUG:GEOparse:SAMPLE: GSM980091\n",
            "27-Dec-2025 06:16:36 DEBUG GEOparse - SAMPLE: GSM980092\n",
            "DEBUG:GEOparse:SAMPLE: GSM980092\n",
            "27-Dec-2025 06:16:36 DEBUG GEOparse - SAMPLE: GSM980093\n",
            "DEBUG:GEOparse:SAMPLE: GSM980093\n",
            "27-Dec-2025 06:16:36 DEBUG GEOparse - SAMPLE: GSM980094\n",
            "DEBUG:GEOparse:SAMPLE: GSM980094\n",
            "27-Dec-2025 06:16:36 DEBUG GEOparse - SAMPLE: GSM980095\n",
            "DEBUG:GEOparse:SAMPLE: GSM980095\n",
            "27-Dec-2025 06:16:36 DEBUG GEOparse - SAMPLE: GSM980096\n",
            "DEBUG:GEOparse:SAMPLE: GSM980096\n",
            "27-Dec-2025 06:16:36 DEBUG GEOparse - SAMPLE: GSM980097\n",
            "DEBUG:GEOparse:SAMPLE: GSM980097\n",
            "27-Dec-2025 06:16:36 DEBUG GEOparse - SAMPLE: GSM980098\n",
            "DEBUG:GEOparse:SAMPLE: GSM980098\n",
            "27-Dec-2025 06:16:37 DEBUG GEOparse - SAMPLE: GSM980099\n",
            "DEBUG:GEOparse:SAMPLE: GSM980099\n",
            "27-Dec-2025 06:16:37 DEBUG GEOparse - SAMPLE: GSM980100\n",
            "DEBUG:GEOparse:SAMPLE: GSM980100\n",
            "27-Dec-2025 06:16:37 DEBUG GEOparse - SAMPLE: GSM980101\n",
            "DEBUG:GEOparse:SAMPLE: GSM980101\n",
            "27-Dec-2025 06:16:37 DEBUG GEOparse - SAMPLE: GSM980102\n",
            "DEBUG:GEOparse:SAMPLE: GSM980102\n",
            "27-Dec-2025 06:16:37 DEBUG GEOparse - SAMPLE: GSM980103\n",
            "DEBUG:GEOparse:SAMPLE: GSM980103\n",
            "27-Dec-2025 06:16:37 DEBUG GEOparse - SAMPLE: GSM980104\n",
            "DEBUG:GEOparse:SAMPLE: GSM980104\n",
            "27-Dec-2025 06:16:37 DEBUG GEOparse - SAMPLE: GSM980105\n",
            "DEBUG:GEOparse:SAMPLE: GSM980105\n",
            "27-Dec-2025 06:16:37 DEBUG GEOparse - SAMPLE: GSM980106\n",
            "DEBUG:GEOparse:SAMPLE: GSM980106\n",
            "27-Dec-2025 06:16:37 DEBUG GEOparse - SAMPLE: GSM980107\n",
            "DEBUG:GEOparse:SAMPLE: GSM980107\n",
            "27-Dec-2025 06:16:37 DEBUG GEOparse - SAMPLE: GSM980108\n",
            "DEBUG:GEOparse:SAMPLE: GSM980108\n",
            "27-Dec-2025 06:16:37 DEBUG GEOparse - SAMPLE: GSM980109\n",
            "DEBUG:GEOparse:SAMPLE: GSM980109\n",
            "27-Dec-2025 06:16:37 DEBUG GEOparse - SAMPLE: GSM980110\n",
            "DEBUG:GEOparse:SAMPLE: GSM980110\n",
            "27-Dec-2025 06:16:37 DEBUG GEOparse - SAMPLE: GSM980111\n",
            "DEBUG:GEOparse:SAMPLE: GSM980111\n",
            "27-Dec-2025 06:16:38 DEBUG GEOparse - SAMPLE: GSM980112\n",
            "DEBUG:GEOparse:SAMPLE: GSM980112\n",
            "27-Dec-2025 06:16:38 DEBUG GEOparse - SAMPLE: GSM980113\n",
            "DEBUG:GEOparse:SAMPLE: GSM980113\n",
            "27-Dec-2025 06:16:38 DEBUG GEOparse - SAMPLE: GSM980114\n",
            "DEBUG:GEOparse:SAMPLE: GSM980114\n",
            "27-Dec-2025 06:16:38 DEBUG GEOparse - SAMPLE: GSM980115\n",
            "DEBUG:GEOparse:SAMPLE: GSM980115\n",
            "27-Dec-2025 06:16:38 DEBUG GEOparse - SAMPLE: GSM980116\n",
            "DEBUG:GEOparse:SAMPLE: GSM980116\n",
            "27-Dec-2025 06:16:38 DEBUG GEOparse - SAMPLE: GSM980117\n",
            "DEBUG:GEOparse:SAMPLE: GSM980117\n",
            "27-Dec-2025 06:16:38 DEBUG GEOparse - SAMPLE: GSM980118\n",
            "DEBUG:GEOparse:SAMPLE: GSM980118\n",
            "27-Dec-2025 06:16:38 DEBUG GEOparse - SAMPLE: GSM980119\n",
            "DEBUG:GEOparse:SAMPLE: GSM980119\n",
            "27-Dec-2025 06:16:38 DEBUG GEOparse - SAMPLE: GSM980120\n",
            "DEBUG:GEOparse:SAMPLE: GSM980120\n",
            "27-Dec-2025 06:16:38 DEBUG GEOparse - SAMPLE: GSM980121\n",
            "DEBUG:GEOparse:SAMPLE: GSM980121\n",
            "27-Dec-2025 06:16:38 DEBUG GEOparse - SAMPLE: GSM980122\n",
            "DEBUG:GEOparse:SAMPLE: GSM980122\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Successfully loaded GSE39833\n",
            "  - GSM samples: 99\n",
            "  - GPL platforms: 1\n",
            "\n",
            "================================================================================\n",
            "STEP 2: EXTRACT EXPRESSION MATRIX AND ASSIGN LABELS\n",
            "================================================================================\n",
            "[INFO] Successfully extracted expression data for 99 samples\n",
            "  - Healthy controls (label=0): 11\n",
            "  - CRC patients (label=1): 88\n",
            "\n",
            "================================================================================\n",
            "STEP 3: BUILD EXPRESSION DATAFRAME\n",
            "================================================================================\n",
            "[INFO] Validating probe consistency across all samples...\n",
            "[INFO] ✓ Probe consistency verified across 4 samples\n",
            "[INFO] All samples contain 15739 probes in identical order\n",
            "\n",
            "[INFO] Expression DataFrame created\n",
            "  - Shape: (99, 15740)\n",
            "  - Samples: 99\n",
            "  - Probes (features): 15739\n",
            "\n",
            "[INFO] Label distribution:\n",
            "label\n",
            "1    88\n",
            "0    11\n",
            "\n",
            "[INFO] Label assignment log saved to: /content/drive/MyDrive/geoexosome_results/label_assignment_log.csv\n",
            "[NOTE] Include this file in Supplementary Materials for full transparency\n",
            "\n",
            "================================================================================\n",
            "STEP 4: PROBE-TO-miRNA MAPPING\n",
            "================================================================================\n",
            "[INFO] Loading platform (GPL) annotation...\n",
            "[INFO] Using miRNA annotation column: 'miRNA_ID'\n",
            "\n",
            "[INFO] Mapping Coverage:\n",
            "  - Total probes: 15739\n",
            "  - Successfully mapped: 15024 (95.5%)\n",
            "  - Unmapped probes: 715 (4.5%)\n",
            "\n",
            "[INFO] Probe-to-miRNA mapping saved to: /content/drive/MyDrive/geoexosome_results/probe_to_miRNA_mapping.csv\n",
            "[INFO] Unmapped probes list saved to: /content/drive/MyDrive/geoexosome_results/unmapped_probes.txt\n",
            "[NOTE] Include in Supplementary Materials to justify feature exclusion\n",
            "\n",
            "================================================================================\n",
            "STEP 5: DATA QUALITY SUMMARY\n",
            "================================================================================\n",
            "\n",
            "[Dataset Dimensions]\n",
            "  - Total samples: 99\n",
            "  - Total probes (features): 15739\n",
            "  - Healthy controls: 11\n",
            "  - CRC patients: 88\n",
            "\n",
            "[Data Quality Metrics]\n",
            "  - Missing values: 0 (0.0000% of all measurements)\n",
            "  - Constant probes (variance = 0): 0\n",
            "  - Expression value range: [-19.79, 781593.70]\n",
            "  - Mean expression (global): 7.67\n",
            "  - SD (global): 1224.52\n",
            "  - Median SD across features: 2.24\n",
            "\n",
            "[Quality Control Assessment]\n",
            "  ✓ PASS: No constant probes detected.\n",
            "  ✓ PASS: Negligible missing values (0.0000%).\n",
            "  ✓ PASS: Good miRNA mapping coverage (95.5%).\n",
            "\n",
            "[INFO] Comprehensive quality report saved to: /content/drive/MyDrive/geoexosome_results/data_quality_report.txt\n",
            "[NOTE] Use this report when writing the Methods section\n",
            "\n",
            "================================================================================\n",
            "✓ DATASET LOADING AND QUALITY CONTROL COMPLETE\n",
            "================================================================================\n",
            "\n",
            "Files saved to: /content/drive/MyDrive/geoexosome_results\n",
            "  1. label_assignment_log.csv - Sample label traceability\n",
            "  2. probe_to_miRNA_mapping.csv - Probe annotation mapping\n",
            "  3. unmapped_probes.txt - Probes without miRNA annotation\n",
            "  4. data_quality_report.txt - Comprehensive QC summary\n",
            "\n",
            "[Summary Statistics]\n",
            "  - Dataset: GSE39833\n",
            "  - Samples: 99 (11 controls, 88 CRC)\n",
            "  - Features: 15739 probes\n",
            "  - Mapped to miRNA: 15024/15739 (95.5%)\n",
            "  - Data quality: 0 constant, 0.0000% missing\n",
            "\n",
            "Next step: Proceed to Cell 2 for preprocessing and feature selection\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# Cell 1: GEO Dataset Loading and Quality Control\n",
        "# ==============================================================================\n",
        "\"\"\"\n",
        "Purpose:\n",
        "    Download and parse the GSE39833 dataset from NCBI GEO, extract expression\n",
        "    matrices, assign sample labels with full traceability, and perform\n",
        "    comprehensive quality control checks.\n",
        "\n",
        "Dataset:\n",
        "    GSE39833 - Serum exosome miRNA microarray from colorectal cancer patients\n",
        "    Platform: GPL14767 (Exiqon miRNA microarray)\n",
        "    Samples: 99 total (11 healthy controls, 88 CRC patients)\n",
        "\n",
        "Outputs:\n",
        "    1. df_expression: DataFrame with probe-level expression values and labels\n",
        "    2. mapping_df: Probe-to-miRNA mapping table\n",
        "    3. label_assignment_log.csv: Full traceability of label assignments\n",
        "    4. probe_to_miRNA_mapping.csv: Complete annotation mapping\n",
        "    5. unmapped_probes.txt: List of probes without miRNA annotation\n",
        "    6. data_quality_report.txt: Comprehensive QC summary for Methods section\n",
        "\n",
        "Quality Controls Implemented:\n",
        "    - Probe ID consistency validation across all samples\n",
        "    - Missing value detection and quantification\n",
        "    - Constant feature detection\n",
        "    - miRNA mapping coverage assessment\n",
        "    - Expression value range verification\n",
        "\n",
        "Author: [Jungho Sohn]\n",
        "Date: 2025-12-20\n",
        "Version: 1.0\n",
        "\"\"\"\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 1: LOAD GEO DATASET FROM NCBI\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 1: LOAD GEO DATASET (GSE39833)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "gse_id = \"GSE39833\"\n",
        "print(f\"[INFO] Downloading GEO dataset: {gse_id}\")\n",
        "print(\"[INFO] This may take 1-2 minutes depending on network speed...\")\n",
        "\n",
        "# Download and parse GEO dataset with platform annotation\n",
        "# annotate_gpl=True ensures GPL annotation is included for probe-to-miRNA mapping\n",
        "gse = GEOparse.get_GEO(\n",
        "    geo=gse_id,\n",
        "    destdir=\"./data\",\n",
        "    annotate_gpl=True\n",
        ")\n",
        "\n",
        "print(f\"[INFO] Successfully loaded {gse_id}\")\n",
        "print(f\"  - GSM samples: {len(gse.gsms)}\")\n",
        "print(f\"  - GPL platforms: {len(gse.gpls)}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 2: EXTRACT EXPRESSION MATRIX AND ASSIGN SAMPLE LABELS\n",
        "# ==============================================================================\n",
        "\"\"\"\n",
        "Label Assignment Strategy:\n",
        "    Priority 1: Sample title parsing (most reliable for this dataset)\n",
        "        - \"hc_*\" → Healthy control (label=0)\n",
        "        - \"crc*\" → CRC patient (label=1)\n",
        "\n",
        "    Priority 2: Metadata characteristics (fallback)\n",
        "        - Cancer keywords: tnm, stage, cancer, adenocarcinoma, tumor\n",
        "        - Healthy keywords: healthy, control, normal\n",
        "\n",
        "    All label assignments are logged with their source for transparency and\n",
        "    reproducibility. This log can be included in Supplementary Materials.\n",
        "\"\"\"\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 2: EXTRACT EXPRESSION MATRIX AND ASSIGN LABELS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Initialize containers for expression data and labels\n",
        "samples = []           # Sample IDs (GSM accessions)\n",
        "expression_rows = []   # Expression values for each sample\n",
        "labels = []            # Binary labels (0=healthy, 1=CRC)\n",
        "label_assignment_log = []  # Traceability log for label assignments\n",
        "\n",
        "# Iterate through all samples in the GEO dataset\n",
        "for gsm_name, gsm in gse.gsms.items():\n",
        "\n",
        "    # Validate that expression data is available\n",
        "    tbl = gsm.table\n",
        "    if \"VALUE\" not in tbl.columns:\n",
        "        print(f\"[WARNING] {gsm_name} missing VALUE column. Skipping...\")\n",
        "        continue\n",
        "\n",
        "    # Extract raw expression values and convert to float\n",
        "    expr_vals = tbl[\"VALUE\"].astype(float).values\n",
        "    expression_rows.append(expr_vals)\n",
        "    samples.append(gsm_name)\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Label Assignment with Source Tracking\n",
        "    # -------------------------------------------------------------------------\n",
        "    title_list = gsm.metadata.get(\"title\", [\"\"])\n",
        "    title = title_list[0].lower()\n",
        "    label_value = None\n",
        "    label_source = None\n",
        "\n",
        "    # PRIMARY METHOD: Title-based labeling\n",
        "    # This is the most reliable method for GSE39833 as samples follow\n",
        "    # a consistent naming convention\n",
        "    if title.startswith(\"hc_\"):\n",
        "        # Healthy control samples\n",
        "        label_value = 0\n",
        "        label_source = f\"title (starts with 'hc_')\"\n",
        "    elif title.startswith(\"crc\"):\n",
        "        # CRC patient samples (includes CRC1, CRC2, CRC3a, CRC3b, CRC4 stages)\n",
        "        label_value = 1\n",
        "        label_source = f\"title (starts with 'crc')\"\n",
        "\n",
        "    # FALLBACK METHOD: Metadata-based labeling\n",
        "    # Used only if title-based labeling fails\n",
        "    # This ensures robustness against potential metadata inconsistencies\n",
        "    if label_value is None:\n",
        "        characteristics = (\n",
        "            gsm.metadata.get(\"characteristics_ch1\", []) +\n",
        "            gsm.metadata.get(\"characteristics_ch2\", [])\n",
        "        )\n",
        "        chars_low = [c.lower() for c in characteristics]\n",
        "\n",
        "        # Define keyword lists for pattern matching\n",
        "        cancer_keywords = [\"tnm\", \"stage\", \"cancer\", \"adenocarcinoma\", \"tumor\"]\n",
        "        healthy_keywords = [\"healthy\", \"control\", \"normal\"]\n",
        "\n",
        "        # Check for cancer indicators in metadata\n",
        "        if any(keyword in c for keyword in cancer_keywords for c in chars_low):\n",
        "            label_value = 1\n",
        "            label_source = \"metadata (cancer-related keywords detected)\"\n",
        "\n",
        "        # Check for healthy control indicators in metadata\n",
        "        elif any(keyword in c for keyword in healthy_keywords for c in chars_low):\n",
        "            label_value = 0\n",
        "            label_source = \"metadata (healthy control keywords detected)\"\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Error Handling: Failed Label Assignment\n",
        "    # -------------------------------------------------------------------------\n",
        "    # If both primary and fallback methods fail, halt execution and display\n",
        "    # detailed metadata to enable manual verification and rule updates\n",
        "    if label_value is None:\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(f\"[ERROR] Unable to determine label for sample: {gsm_name}\")\n",
        "        print(\"=\" * 80)\n",
        "        print(f\"\\nSample Metadata:\")\n",
        "        print(f\"  - Title: {title_list}\")\n",
        "        print(f\"  - Characteristics (ch1): {gsm.metadata.get('characteristics_ch1', [])}\")\n",
        "        print(f\"  - Characteristics (ch2): {gsm.metadata.get('characteristics_ch2', [])}\")\n",
        "        print(f\"  - Source: {gsm.metadata.get('source_name_ch1', ['N/A'])}\")\n",
        "        print(f\"  - Description: {gsm.metadata.get('description', ['N/A'])}\")\n",
        "        print(f\"\\nPossible Causes:\")\n",
        "        print(f\"  1. Unexpected metadata format (not matching expected patterns)\")\n",
        "        print(f\"  2. Sample naming convention differs from other samples\")\n",
        "        print(f\"  3. Ambiguous or missing label information in metadata\")\n",
        "        print(f\"\\nAction Required:\")\n",
        "        print(f\"  Please verify the sample metadata above and update the label\")\n",
        "        print(f\"  assignment logic in this cell accordingly.\")\n",
        "        print(\"=\" * 80 + \"\\n\")\n",
        "        raise ValueError(f\"Label assignment failed for {gsm_name}\")\n",
        "\n",
        "    # Record successful label assignment with source for transparency\n",
        "    labels.append(label_value)\n",
        "    label_assignment_log.append({\n",
        "        \"Sample_ID\": gsm_name,\n",
        "        \"Label\": label_value,\n",
        "        \"Label_Name\": \"Healthy_Control\" if label_value == 0 else \"CRC_Patient\",\n",
        "        \"Assignment_Source\": label_source,\n",
        "        \"Sample_Title\": title_list[0]\n",
        "    })\n",
        "\n",
        "# Convert list of expression arrays to 2D numpy array\n",
        "# Shape: (n_samples, n_probes)\n",
        "expression_data = np.vstack(expression_rows)\n",
        "\n",
        "print(f\"[INFO] Successfully extracted expression data for {len(samples)} samples\")\n",
        "print(f\"  - Healthy controls (label=0): {labels.count(0)}\")\n",
        "print(f\"  - CRC patients (label=1): {labels.count(1)}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 3: BUILD EXPRESSION DATAFRAME WITH PROBE-LEVEL DATA\n",
        "# ==============================================================================\n",
        "\"\"\"\n",
        "Data Structure:\n",
        "    - Rows: Samples (GSM IDs)\n",
        "    - Columns: Probe IDs + 'label' column\n",
        "    - Values: Raw microarray intensity values\n",
        "\n",
        "Quality Check:\n",
        "    Validate that all samples have identical probe IDs in the same order.\n",
        "    This is critical for ensuring data integrity in downstream analyses.\n",
        "\"\"\"\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 3: BUILD EXPRESSION DATAFRAME\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Extract probe IDs from the first sample as reference\n",
        "first_gsm = gse.gsms[samples[0]]\n",
        "probe_ids = first_gsm.table[\"ID_REF\"].tolist()\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# Quality Control: Validate Probe ID Consistency\n",
        "# -------------------------------------------------------------------------\n",
        "# Verify that all samples have identical probe IDs in identical order\n",
        "# This check prevents silent errors from probe ID mismatches\n",
        "print(\"[INFO] Validating probe consistency across all samples...\")\n",
        "\n",
        "# Check first 3 samples and last sample for efficiency\n",
        "# Full validation is computationally expensive for large datasets\n",
        "samples_to_check = samples[:3] + [samples[-1]] if len(samples) > 3 else samples\n",
        "for gsm_name in samples_to_check:\n",
        "    current_probes = gse.gsms[gsm_name].table[\"ID_REF\"].tolist()\n",
        "\n",
        "    # Check probe count\n",
        "    if len(current_probes) != len(probe_ids):\n",
        "        raise ValueError(\n",
        "            f\"[ERROR] Probe count mismatch detected in {gsm_name}\\n\"\n",
        "            f\"Expected {len(probe_ids)} probes matching {samples[0]}, \"\n",
        "            f\"but found {len(current_probes)} probes.\"\n",
        "        )\n",
        "\n",
        "    # Check probe order\n",
        "    if current_probes != probe_ids:\n",
        "        raise ValueError(\n",
        "            f\"[ERROR] Probe order mismatch detected in {gsm_name}\\n\"\n",
        "            f\"Probe IDs do not match the reference sample {samples[0]}.\"\n",
        "        )\n",
        "\n",
        "print(f\"[INFO] ✓ Probe consistency verified across {len(samples_to_check)} samples\")\n",
        "print(f\"[INFO] All samples contain {len(probe_ids)} probes in identical order\")\n",
        "\n",
        "# Create DataFrame with probe IDs as columns and sample IDs as index\n",
        "df_expression = pd.DataFrame(\n",
        "    expression_data,\n",
        "    columns=probe_ids,\n",
        "    index=samples\n",
        ")\n",
        "df_expression[\"label\"] = labels\n",
        "\n",
        "print(f\"\\n[INFO] Expression DataFrame created\")\n",
        "print(f\"  - Shape: {df_expression.shape}\")\n",
        "print(f\"  - Samples: {df_expression.shape[0]}\")\n",
        "print(f\"  - Probes (features): {df_expression.shape[1] - 1}\")  # Excluding 'label' column\n",
        "print(f\"\\n[INFO] Label distribution:\")\n",
        "print(df_expression[\"label\"].value_counts().to_string())\n",
        "\n",
        "# Save label assignment log for manuscript transparency\n",
        "# This file should be included in Supplementary Materials\n",
        "label_log_df = pd.DataFrame(label_assignment_log)\n",
        "label_log_path = os.path.join(base_save_path, \"label_assignment_log.csv\")\n",
        "label_log_df.to_csv(label_log_path, index=False)\n",
        "print(f\"\\n[INFO] Label assignment log saved to: {label_log_path}\")\n",
        "print(\"[NOTE] Include this file in Supplementary Materials for full transparency\")\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 4: BUILD PROBE-TO-miRNA MAPPING FROM PLATFORM ANNOTATION\n",
        "# ==============================================================================\n",
        "\"\"\"\n",
        "Purpose:\n",
        "    Map microarray probe IDs to known miRNA identifiers using the GPL\n",
        "    platform annotation file. This enables biological interpretation of\n",
        "    features in downstream analysis.\n",
        "\n",
        "Coverage Assessment:\n",
        "    Calculate and report the percentage of probes successfully mapped to\n",
        "    miRNAs. Low coverage (<60%) may indicate platform compatibility issues.\n",
        "\n",
        "Unmapped Probes:\n",
        "    Probes without miRNA annotation will be excluded from downstream analysis\n",
        "    to ensure all features have biological interpretability. The list of\n",
        "    excluded probes is saved for transparency.\n",
        "\"\"\"\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 4: PROBE-TO-miRNA MAPPING\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"[INFO] Loading platform (GPL) annotation...\")\n",
        "\n",
        "# Extract platform annotation table\n",
        "gpl = list(gse.gpls.values())[0]\n",
        "gpl_table = gpl.table\n",
        "\n",
        "# Validate GPL table structure\n",
        "if \"ID\" not in gpl_table.columns:\n",
        "    raise KeyError(\"[ERROR] GPL table missing 'ID' column. Cannot build mapping.\")\n",
        "\n",
        "# Identify miRNA annotation column\n",
        "# Look for columns containing 'mir' (case-insensitive)\n",
        "mirna_cols = [c for c in gpl_table.columns if \"mir\" in c.lower()]\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# Handle Missing miRNA Annotation\n",
        "# -------------------------------------------------------------------------\n",
        "# Initialize coverage_pct to prevent NameError in QC section\n",
        "if len(mirna_cols) == 0:\n",
        "    print(\"[WARNING] No miRNA annotation column detected in GPL table.\")\n",
        "    print(\"[WARNING] Probe-to-miRNA mapping will be unavailable.\")\n",
        "    mapping_df = None\n",
        "    n_mapped = 0\n",
        "    n_total = len(probe_ids)\n",
        "    coverage_pct = 0.0\n",
        "else:\n",
        "    # Use the first miRNA annotation column found\n",
        "    mirna_col = mirna_cols[0]\n",
        "    print(f\"[INFO] Using miRNA annotation column: '{mirna_col}'\")\n",
        "\n",
        "    # Build probe-to-miRNA dictionary for fast lookup\n",
        "    probe_to_mirna = dict(zip(gpl_table[\"ID\"], gpl_table[mirna_col]))\n",
        "\n",
        "    # Map all probe IDs to miRNA names (NaN if not found)\n",
        "    mirna_names = [probe_to_mirna.get(pid, np.nan) for pid in probe_ids]\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Calculate Mapping Coverage Statistics\n",
        "    # -------------------------------------------------------------------------\n",
        "    n_mapped = sum(pd.notna(m) for m in mirna_names)\n",
        "    n_total = len(probe_ids)\n",
        "    coverage_pct = 100.0 * n_mapped / n_total\n",
        "\n",
        "    print(f\"\\n[INFO] Mapping Coverage:\")\n",
        "    print(f\"  - Total probes: {n_total}\")\n",
        "    print(f\"  - Successfully mapped: {n_mapped} ({coverage_pct:.1f}%)\")\n",
        "    print(f\"  - Unmapped probes: {n_total - n_mapped} ({100 - coverage_pct:.1f}%)\")\n",
        "\n",
        "    # Create mapping DataFrame\n",
        "    mapping_df = pd.DataFrame({\n",
        "        \"Probe_ID\": probe_ids,\n",
        "        \"miRNA\": mirna_names\n",
        "    })\n",
        "\n",
        "    # Save complete mapping table\n",
        "    mapping_path = os.path.join(base_save_path, \"probe_to_miRNA_mapping.csv\")\n",
        "    mapping_df.to_csv(mapping_path, index=False)\n",
        "    print(f\"\\n[INFO] Probe-to-miRNA mapping saved to: {mapping_path}\")\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Save Unmapped Probes for Transparency\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Document which probes were excluded and why\n",
        "    # This justifies feature exclusion in the manuscript\n",
        "    unmapped_probes = [\n",
        "        probe for probe, mirna in zip(probe_ids, mirna_names)\n",
        "        if pd.isna(mirna)\n",
        "    ]\n",
        "\n",
        "    if unmapped_probes:\n",
        "        unmapped_path = os.path.join(base_save_path, \"unmapped_probes.txt\")\n",
        "        with open(unmapped_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(f\"Unmapped Probes (no miRNA annotation): {len(unmapped_probes)} total\\n\")\n",
        "            f.write(\"=\" * 80 + \"\\n\\n\")\n",
        "            f.write(\"These probes will be excluded in downstream preprocessing (Cell 2) \")\n",
        "            f.write(\"due to lack of miRNA annotation in the platform GPL file.\\n\\n\")\n",
        "            f.write(\"This exclusion ensures that all analyzed features have biological \")\n",
        "            f.write(\"interpretability as known miRNAs.\\n\\n\")\n",
        "            f.write(\"List of unmapped probe IDs:\\n\")\n",
        "            f.write(\"-\" * 80 + \"\\n\")\n",
        "            for probe in unmapped_probes:\n",
        "                f.write(f\"{probe}\\n\")\n",
        "        print(f\"[INFO] Unmapped probes list saved to: {unmapped_path}\")\n",
        "        print(\"[NOTE] Include in Supplementary Materials to justify feature exclusion\")\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 5: COMPREHENSIVE DATA QUALITY VALIDATION\n",
        "# ==============================================================================\n",
        "\"\"\"\n",
        "Quality Control Metrics:\n",
        "    1. Missing values: Count and percentage of NaN/null values\n",
        "    2. Constant probes: Features with zero variance (uninformative)\n",
        "    3. Expression range: Min/max values to detect outliers or errors\n",
        "    4. Summary statistics: Mean, SD for manuscript reporting\n",
        "\n",
        "Assessment Criteria:\n",
        "    - Missing values: PASS if <0.01%, WARNING if 0.01-5%, FAIL if >5%\n",
        "    - Constant probes: PASS if 0, WARNING otherwise\n",
        "    - Mapping coverage: PASS if ≥75%, NOTICE if 60-75%, WARNING if <60%\n",
        "\n",
        "Output:\n",
        "    Comprehensive report file (data_quality_report.txt) formatted for\n",
        "    direct use in manuscript Methods section.\n",
        "\"\"\"\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 5: DATA QUALITY SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Extract feature columns (exclude 'label' column)\n",
        "feature_cols = [col for col in df_expression.columns if col != \"label\"]\n",
        "expr_matrix = df_expression[feature_cols]\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# Calculate Quality Metrics\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "# Dataset dimensions\n",
        "n_samples = df_expression.shape[0]\n",
        "n_features = len(feature_cols)\n",
        "\n",
        "# Missing value analysis\n",
        "n_missing = expr_matrix.isna().sum().sum()\n",
        "total_values = n_samples * n_features\n",
        "missing_pct = 100.0 * n_missing / total_values\n",
        "\n",
        "# Constant feature detection (variance = 0)\n",
        "expr_var = expr_matrix.var(axis=0)\n",
        "n_constant = (expr_var == 0).sum()\n",
        "\n",
        "# Expression value statistics\n",
        "# Use global statistics rather than feature-wise averages for clarity\n",
        "expr_min = expr_matrix.min().min()\n",
        "expr_max = expr_matrix.max().max()\n",
        "expr_mean_global = expr_matrix.mean().mean()        # Mean of all values\n",
        "expr_std_global = expr_matrix.to_numpy().std()      # SD of all values\n",
        "expr_feature_std_median = expr_matrix.std().median()  # Median SD across features\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# Display Quality Control Summary\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "print(f\"\\n[Dataset Dimensions]\")\n",
        "print(f\"  - Total samples: {n_samples}\")\n",
        "print(f\"  - Total probes (features): {n_features}\")\n",
        "print(f\"  - Healthy controls: {(df_expression['label'] == 0).sum()}\")\n",
        "print(f\"  - CRC patients: {(df_expression['label'] == 1).sum()}\")\n",
        "\n",
        "print(f\"\\n[Data Quality Metrics]\")\n",
        "print(f\"  - Missing values: {n_missing} ({missing_pct:.4f}% of all measurements)\")\n",
        "print(f\"  - Constant probes (variance = 0): {n_constant}\")\n",
        "print(f\"  - Expression value range: [{expr_min:.2f}, {expr_max:.2f}]\")\n",
        "print(f\"  - Mean expression (global): {expr_mean_global:.2f}\")\n",
        "print(f\"  - SD (global): {expr_std_global:.2f}\")\n",
        "print(f\"  - Median SD across features: {expr_feature_std_median:.2f}\")\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# Quality Control Assessment with Pass/Warning/Fail Criteria\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "print(f\"\\n[Quality Control Assessment]\")\n",
        "\n",
        "# Check 1: Constant probes\n",
        "if n_constant > 0:\n",
        "    print(f\"  ⚠ WARNING: {n_constant} constant probes detected.\")\n",
        "    print(f\"     → These should be removed before feature selection.\")\n",
        "else:\n",
        "    print(f\"  ✓ PASS: No constant probes detected.\")\n",
        "\n",
        "# Check 2: Missing values\n",
        "if missing_pct > 5.0:\n",
        "    print(f\"  ⚠ WARNING: Missing values exceed 5% threshold ({missing_pct:.2f}%).\")\n",
        "    print(f\"     → Consider imputation or removal of problematic probes.\")\n",
        "elif missing_pct > 0.01:\n",
        "    print(f\"  ⚠ NOTICE: Low level of missing values detected ({missing_pct:.4f}%).\")\n",
        "    print(f\"     → Acceptable for most analyses without imputation.\")\n",
        "else:\n",
        "    print(f\"  ✓ PASS: Negligible missing values ({missing_pct:.4f}%).\")\n",
        "\n",
        "# Check 3: miRNA mapping coverage\n",
        "if coverage_pct < 60:\n",
        "    print(f\"  ⚠ WARNING: miRNA mapping coverage is low ({coverage_pct:.1f}%).\")\n",
        "    print(f\"     → Verify platform annotation compatibility.\")\n",
        "elif coverage_pct < 75:\n",
        "    print(f\"  ⚠ NOTICE: Moderate miRNA mapping coverage ({coverage_pct:.1f}%).\")\n",
        "    print(f\"     → Acceptable for most downstream analyses.\")\n",
        "else:\n",
        "    print(f\"  ✓ PASS: Good miRNA mapping coverage ({coverage_pct:.1f}%).\")\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# Save Comprehensive Quality Report for Manuscript\n",
        "# -------------------------------------------------------------------------\n",
        "# This report is formatted for direct use in the Methods section\n",
        "# and provides all necessary QC information for reproducibility\n",
        "\n",
        "qc_report_path = os.path.join(base_save_path, \"data_quality_report.txt\")\n",
        "with open(qc_report_path, 'w', encoding='utf-8') as f:\n",
        "    f.write(\"=\" * 80 + \"\\n\")\n",
        "    f.write(\"DATA QUALITY REPORT - GSE39833\\n\")\n",
        "    f.write(\"=\" * 80 + \"\\n\\n\")\n",
        "    f.write(f\"Dataset: {gse_id}\\n\")\n",
        "    f.write(f\"Analysis Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
        "\n",
        "    f.write(\"DATASET DIMENSIONS\\n\")\n",
        "    f.write(\"-\" * 80 + \"\\n\")\n",
        "    f.write(f\"Total samples: {n_samples}\\n\")\n",
        "    f.write(f\"Total probes: {n_features}\\n\")\n",
        "    f.write(f\"Healthy controls: {(df_expression['label'] == 0).sum()}\\n\")\n",
        "    f.write(f\"CRC patients: {(df_expression['label'] == 1).sum()}\\n\\n\")\n",
        "\n",
        "    f.write(\"DATA QUALITY METRICS\\n\")\n",
        "    f.write(\"-\" * 80 + \"\\n\")\n",
        "    f.write(f\"Missing values: {n_missing} ({missing_pct:.4f}%)\\n\")\n",
        "    f.write(f\"Constant probes: {n_constant}\\n\")\n",
        "    f.write(f\"Expression range: [{expr_min:.2f}, {expr_max:.2f}]\\n\")\n",
        "    f.write(f\"Mean (global): {expr_mean_global:.2f}\\n\")\n",
        "    f.write(f\"SD (global): {expr_std_global:.2f}\\n\")\n",
        "    f.write(f\"Median SD across features: {expr_feature_std_median:.2f}\\n\\n\")\n",
        "\n",
        "    f.write(\"miRNA MAPPING COVERAGE\\n\")\n",
        "    f.write(\"-\" * 80 + \"\\n\")\n",
        "    f.write(f\"Total probes: {n_total}\\n\")\n",
        "    f.write(f\"Mapped probes: {n_mapped} ({coverage_pct:.1f}%)\\n\")\n",
        "    f.write(f\"Unmapped probes: {n_total - n_mapped} ({100 - coverage_pct:.1f}%)\\n\\n\")\n",
        "\n",
        "    f.write(\"QUALITY CONTROL ASSESSMENT\\n\")\n",
        "    f.write(\"-\" * 80 + \"\\n\")\n",
        "    f.write(f\"Constant probes: {'PASS' if n_constant == 0 else 'WARNING'}\\n\")\n",
        "    f.write(f\"Missing values: {'PASS' if missing_pct < 0.01 else 'WARNING/NOTICE'}\\n\")\n",
        "    f.write(f\"Mapping coverage: {'PASS' if coverage_pct >= 75 else 'WARNING/NOTICE'}\\n\\n\")\n",
        "\n",
        "    f.write(\"NOTES FOR MANUSCRIPT (Methods Section)\\n\")\n",
        "    f.write(\"-\" * 80 + \"\\n\")\n",
        "    f.write(\"Use the following information when writing the Methods section:\\n\\n\")\n",
        "\n",
        "    f.write(f\"1. Sample Composition:\\n\")\n",
        "    f.write(f\"   The GSE39833 dataset comprised {n_samples} serum exosome samples, \")\n",
        "    f.write(f\"including {(df_expression['label'] == 0).sum()} healthy controls and \")\n",
        "    f.write(f\"{(df_expression['label'] == 1).sum()} colorectal cancer (CRC) patients.\\n\\n\")\n",
        "\n",
        "    f.write(f\"2. Data Quality:\\n\")\n",
        "    f.write(f\"   Data quality was verified prior to analysis. \")\n",
        "    if n_constant == 0:\n",
        "        f.write(f\"No constant features were detected. \")\n",
        "    else:\n",
        "        f.write(f\"{n_constant} constant features were identified and removed. \")\n",
        "    f.write(f\"Missing values accounted for {missing_pct:.4f}% of all measurements\")\n",
        "    if missing_pct < 0.01:\n",
        "        f.write(f\"; no imputation was performed due to negligible missingness.\\n\\n\")\n",
        "    else:\n",
        "        f.write(f\".\\n\\n\")\n",
        "\n",
        "    f.write(f\"3. Feature Annotation:\\n\")\n",
        "    f.write(f\"   Of the {n_total} microarray probes, {n_mapped} ({coverage_pct:.1f}%) were \")\n",
        "    f.write(f\"successfully mapped to known miRNAs in the platform annotation file. \")\n",
        "    f.write(f\"Unmapped probes were excluded from downstream analysis to ensure \")\n",
        "    f.write(f\"biological interpretability of all features.\\n\\n\")\n",
        "\n",
        "    f.write(f\"4. Label Assignment:\\n\")\n",
        "    f.write(f\"   Sample labels were assigned based on standardized metadata fields \")\n",
        "    f.write(f\"(sample titles and characteristics). All label assignments were recorded \")\n",
        "    f.write(f\"in a traceability log (label_assignment_log.csv) to ensure transparency \")\n",
        "    f.write(f\"and reproducibility.\\n\")\n",
        "\n",
        "print(f\"\\n[INFO] Comprehensive quality report saved to: {qc_report_path}\")\n",
        "print(\"[NOTE] Use this report when writing the Methods section\")\n",
        "\n",
        "# ==============================================================================\n",
        "# FINAL SUMMARY\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"✓ DATASET LOADING AND QUALITY CONTROL COMPLETE\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nFiles saved to: {base_save_path}\")\n",
        "print(f\"  1. label_assignment_log.csv - Sample label traceability\")\n",
        "print(f\"  2. probe_to_miRNA_mapping.csv - Probe annotation mapping\")\n",
        "if n_total - n_mapped > 0:\n",
        "    print(f\"  3. unmapped_probes.txt - Probes without miRNA annotation\")\n",
        "    print(f\"  4. data_quality_report.txt - Comprehensive QC summary\")\n",
        "else:\n",
        "    print(f\"  3. data_quality_report.txt - Comprehensive QC summary\")\n",
        "\n",
        "print(f\"\\n[Summary Statistics]\")\n",
        "print(f\"  - Dataset: {gse_id}\")\n",
        "print(f\"  - Samples: {n_samples} ({(df_expression['label'] == 0).sum()} controls, {(df_expression['label'] == 1).sum()} CRC)\")\n",
        "print(f\"  - Features: {n_features} probes\")\n",
        "print(f\"  - Mapped to miRNA: {n_mapped}/{n_total} ({coverage_pct:.1f}%)\")\n",
        "print(f\"  - Data quality: {n_constant} constant, {missing_pct:.4f}% missing\")\n",
        "\n",
        "print(f\"\\nNext step: Proceed to Cell 2 for preprocessing and feature selection\")\n",
        "print(\"=\" * 80 + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKoOrJcKObxL",
        "outputId": "7257330d-4a39-4f7c-d22c-eb8876ecd821"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=====================================================================\n",
            "Cell 2: Nested Cross-Validation with REPEATED Stratified K-Fold\n",
            "\n",
            "CONFIGURATION:\n",
            "  Outer CV:            5-fold × 10 repeats = 50 iterations\n",
            "  Inner CV:            3-fold (hyperparameter tuning)\n",
            "\n",
            "  Feature Selection:\n",
            "    |log2FC| threshold:  > 1.0\n",
            "    FDR q-value:         < 0.05\n",
            "    Consensus:           2-of-3 methods (unchanged)\n",
            "\n",
            "This provides more stable performance estimates with variance.\n",
            "=====================================================================\n",
            "\n",
            "\n",
            "================================================================================\n",
            "STEP 1: Load Expression Data from Cell 1\n",
            "================================================================================\n",
            "[INFO] Applying log2(x + 1) transformation to raw expression values\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pandas/core/internals/blocks.py:393: RuntimeWarning: invalid value encountered in log2\n",
            "  result = func(self.values, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Expression matrix loaded: (99, 15739)\n",
            "  Samples: 99\n",
            "  Probes: 15739\n",
            "  Healthy controls: 11\n",
            "  CRC patients: 88\n",
            "  Class imbalance ratio: 1:8.0\n",
            "\n",
            "[VERIFICATION] Checking for data leakage risks:\n",
            "  Max value: 19.58\n",
            "  Min value: -16.45\n",
            "  Global mean: 0.88\n",
            "  Global std: 1.62\n",
            "  ✓ Data is NOT globally normalized. Safe for fold-wise processing.\n",
            "\n",
            "================================================================================\n",
            "STEP 2: Configure Cross-Validation Strategy\n",
            "================================================================================\n",
            "[INFO] Outer CV: 5-fold × 10 repeats = 50 iterations\n",
            "[INFO] Inner CV: 3-fold stratified\n",
            "[INFO] Random seed: 42\n",
            "[INFO] Results will be saved to: /content/drive/MyDrive/geoexosome_results\n",
            "\n",
            "================================================================================\n",
            "STEP 3: Define Model Configurations\n",
            "================================================================================\n",
            "[INFO] Configured 4 model variants:\n",
            "  - RandomForest_SMOTE\n",
            "  - RandomForest_Weighted\n",
            "  - SVM\n",
            "  - LogisticRegression\n",
            "\n",
            "================================================================================\n",
            "STEP 4: Execute Nested Cross-Validation\n",
            "================================================================================\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Model: RandomForest_SMOTE\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "  Running 50 iterations (5-fold × 10 repeats)...\n",
            "\n",
            "    Iteration 1/50 (Repeat 1, Fold 1)\n",
            "      Train: 71/79 CRC | Test: 17/20 CRC\n",
            "\n",
            "    [Fold R1F1] Feature selection on training data only\n",
            "      Train samples: 79 (8 HC, 71 CRC)\n",
            "      Total probes: 15739\n",
            "      Stage 1: 152 probes (|log2FC| > 1.0, FDR q < 0.05)\n",
            "      Stage 2: Multi-method selection (LASSO, SVM-RFE, RF)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        LASSO: 10 features\n",
            "        SVM-RFE: 50 features\n",
            "        Random Forest: 150 features\n",
            "      Stage 3: 51 features (≥2/3 consensus)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    → Repeat 1 complete: OOF AUC = 0.9814\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    → Repeat 2 complete: OOF AUC = 0.9830\n",
            "\n",
            "    Iteration 11/50 (Repeat 3, Fold 1)\n",
            "      Train: 71/79 CRC | Test: 17/20 CRC\n",
            "\n",
            "    [Fold R3F1] Feature selection on training data only\n",
            "      Train samples: 79 (8 HC, 71 CRC)\n",
            "      Total probes: 15739\n",
            "      Stage 1: 160 probes (|log2FC| > 1.0, FDR q < 0.05)\n",
            "      Stage 2: Multi-method selection (LASSO, SVM-RFE, RF)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        LASSO: 33 features\n",
            "        SVM-RFE: 53 features\n",
            "        Random Forest: 150 features\n",
            "      Stage 3: 60 features (≥2/3 consensus)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    → Repeat 3 complete: OOF AUC = 0.9535\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    → Repeat 4 complete: OOF AUC = 0.9824\n",
            "\n",
            "    Iteration 21/50 (Repeat 5, Fold 1)\n",
            "      Train: 71/79 CRC | Test: 17/20 CRC\n",
            "\n",
            "    [Fold R5F1] Feature selection on training data only\n",
            "      Train samples: 79 (8 HC, 71 CRC)\n",
            "      Total probes: 15739\n",
            "      Stage 1: 201 probes (|log2FC| > 1.0, FDR q < 0.05)\n",
            "      Stage 2: Multi-method selection (LASSO, SVM-RFE, RF)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        LASSO: 31 features\n",
            "        SVM-RFE: 67 features\n",
            "        Random Forest: 150 features\n",
            "      Stage 3: 69 features (≥2/3 consensus)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    → Repeat 5 complete: OOF AUC = 0.9824\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    → Repeat 6 complete: OOF AUC = 0.9602\n",
            "\n",
            "    Iteration 31/50 (Repeat 7, Fold 1)\n",
            "      Train: 71/79 CRC | Test: 17/20 CRC\n",
            "\n",
            "    [Fold R7F1] Feature selection on training data only\n",
            "      Train samples: 79 (8 HC, 71 CRC)\n",
            "      Total probes: 15739\n",
            "      Stage 1: 100 probes (|log2FC| > 1.0, FDR q < 0.05)\n",
            "      Stage 2: Multi-method selection (LASSO, SVM-RFE, RF)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        LASSO: 22 features\n",
            "        SVM-RFE: 33 features\n",
            "        Random Forest: 100 features\n",
            "      Stage 3: 39 features (≥2/3 consensus)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    → Repeat 7 complete: OOF AUC = 0.9814\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    → Repeat 8 complete: OOF AUC = 0.9757\n",
            "\n",
            "    Iteration 41/50 (Repeat 9, Fold 1)\n",
            "      Train: 71/79 CRC | Test: 17/20 CRC\n",
            "\n",
            "    [Fold R9F1] Feature selection on training data only\n",
            "      Train samples: 79 (8 HC, 71 CRC)\n",
            "      Total probes: 15739\n",
            "      Stage 1: 159 probes (|log2FC| > 1.0, FDR q < 0.05)\n",
            "      Stage 2: Multi-method selection (LASSO, SVM-RFE, RF)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        LASSO: 35 features\n",
            "        SVM-RFE: 53 features\n",
            "        Random Forest: 150 features\n",
            "      Stage 3: 59 features (≥2/3 consensus)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    → Repeat 9 complete: OOF AUC = 0.9654\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    → Repeat 10 complete: OOF AUC = 0.9385\n",
            "\n",
            "  ============================================================================\n",
            "  OUT-OF-FOLD PERFORMANCE (Unbiased Estimate)\n",
            "  ============================================================================\n",
            "    ROC-AUC (aggregated):  0.9824\n",
            "    ROC-AUC (mean±std):    0.9704 ± 0.0147\n",
            "    95% CI:                [0.9519, 1.0000]\n",
            "    Accuracy:              0.9192\n",
            "    Balanced Accuracy:     0.7159\n",
            "    Precision:             0.9348\n",
            "    Sensitivity:           0.9773\n",
            "    Specificity:           0.4545\n",
            "    F1-score:              0.9556\n",
            "    Mean Train AUC:        1.0000\n",
            "    Mean Test AUC:         0.9772\n",
            "    Confusion Matrix:\n",
            "    [[ 5  6]\n",
            "     [ 2 86]]\n",
            "\n",
            "  Feature Stability Across 50 Iterations:\n",
            "    Selected in 100%:  1\n",
            "    Selected in ≥80%:  6\n",
            "    Selected in ≥50%:  27\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Model: RandomForest_Weighted\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "  Running 50 iterations (5-fold × 10 repeats)...\n",
            "\n",
            "    Iteration 1/50 (Repeat 1, Fold 1)\n",
            "      Train: 71/79 CRC | Test: 17/20 CRC\n",
            "\n",
            "    [Fold R1F1] Feature selection on training data only\n",
            "      Train samples: 79 (8 HC, 71 CRC)\n",
            "      Total probes: 15739\n",
            "      Stage 1: 152 probes (|log2FC| > 1.0, FDR q < 0.05)\n",
            "      Stage 2: Multi-method selection (LASSO, SVM-RFE, RF)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        LASSO: 10 features\n",
            "        SVM-RFE: 50 features\n",
            "        Random Forest: 150 features\n",
            "      Stage 3: 51 features (≥2/3 consensus)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    → Repeat 1 complete: OOF AUC = 0.9902\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    → Repeat 2 complete: OOF AUC = 0.9861\n",
            "\n",
            "    Iteration 11/50 (Repeat 3, Fold 1)\n",
            "      Train: 71/79 CRC | Test: 17/20 CRC\n",
            "\n",
            "    [Fold R3F1] Feature selection on training data only\n",
            "      Train samples: 79 (8 HC, 71 CRC)\n",
            "      Total probes: 15739\n",
            "      Stage 1: 160 probes (|log2FC| > 1.0, FDR q < 0.05)\n",
            "      Stage 2: Multi-method selection (LASSO, SVM-RFE, RF)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        LASSO: 33 features\n",
            "        SVM-RFE: 53 features\n",
            "        Random Forest: 150 features\n",
            "      Stage 3: 60 features (≥2/3 consensus)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    → Repeat 3 complete: OOF AUC = 0.9659\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Cell 2: Nested Cross-Validation with Repeated Stratified K-Fold\n",
        "# =============================================================================\n",
        "\"\"\"\n",
        "VERSION 3.0 MODIFICATIONS (Based on Literature Review):\n",
        "\n",
        "CRITICAL CHANGES:\n",
        "1. SMOTE → Class Weights (safer for n=11 minority class)\n",
        "   - Demircioğlu 2024: SMOTE before CV causes +0.34 AUC bias\n",
        "   - With only 9 controls per training fold, SMOTE's k=5 neighbors is problematic\n",
        "\n",
        "2. Stability Threshold: 80% → 70% (realistic per Liu et al. 2025)\n",
        "   - Kuncheva index 0.50-0.75 is realistic for HDSS data\n",
        "   - 70% provides balance between stringency and practicality\n",
        "\n",
        "3. Dual-Path Feature Selection (Lewis 2023, Parvandeh 2020)\n",
        "   - Path A: Production model (full-data retrained) for external validation\n",
        "   - Path B: CV-stable features (>70% of folds) for biomarker reporting\n",
        "\n",
        "UNCHANGED:\n",
        "- Consensus voting: 2-of-3 (same as before)\n",
        "- Biological filtering thresholds\n",
        "- All other logic\n",
        "\n",
        "Reference:\n",
        "- Demircioğlu A (2024) Sci Rep: Oversampling before CV causes bias\n",
        "- Liu et al. (2025) Comput Methods: Kuncheva stability benchmarks\n",
        "- Lewis et al. (2023) nestedcv R package: Dual-path strategy\n",
        "- Parvandeh et al. (2020) Bioinformatics: Consensus nested CV\n",
        "\n",
        "Author: Jungho Sohn\n",
        "Date: 2025-12-28\n",
        "Version: 3.0 (Class weights + Realistic stability + Dual-path)\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from scipy import stats\n",
        "from scipy.stats import mannwhitneyu\n",
        "\n",
        "# =============================================================================\n",
        "# IMPORTS (unchanged except SMOTE removal from pipeline)\n",
        "# =============================================================================\n",
        "from sklearn.model_selection import (\n",
        "    StratifiedKFold,\n",
        "    RepeatedStratifiedKFold,\n",
        "    GridSearchCV\n",
        ")\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LassoCV, LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score,\n",
        "    accuracy_score,\n",
        "    balanced_accuracy_score,\n",
        "    precision_recall_fscore_support,\n",
        "    confusion_matrix\n",
        ")\n",
        "# ┌─────────────────────────────────────────────────────────────────────────────┐\n",
        "# │ CHANGE 1: Import Pipeline from sklearn instead of imblearn                 │\n",
        "# │ SMOTE is no longer used - we use class_weight='balanced' instead           │\n",
        "# └─────────────────────────────────────────────────────────────────────────────┘\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "\n",
        "# =============================================================================\n",
        "# ┌─────────────────────────────────────────────────────────────────────────────┐\n",
        "# │ CHANGE 2: Adjustable Feature Selection Thresholds                          │\n",
        "# │ Stability threshold lowered from 80% to 70% (per literature review)        │\n",
        "# └─────────────────────────────────────────────────────────────────────────────┘\n",
        "# =============================================================================\n",
        "\n",
        "# Feature Selection Thresholds (ADJUSTABLE)\n",
        "LOG2FC_THRESHOLD = 1.0      # Default: 1.0 (2-fold change), Try: 1.5 for stricter\n",
        "FDR_THRESHOLD = 0.05        # Default: 0.05, Try: 0.01 for stricter\n",
        "\n",
        "# Cross-Validation Configuration\n",
        "N_OUTER_SPLITS = 5          # Number of folds\n",
        "N_REPEATS = 10              # Number of repetitions\n",
        "N_INNER_SPLITS = 3          # Inner CV for hyperparameter tuning\n",
        "\n",
        "# ┌─────────────────────────────────────────────────────────────────────────────┐\n",
        "# │ CHANGE 3: Stability Threshold 80% → 70% (realistic per Liu et al. 2025)    │\n",
        "# └─────────────────────────────────────────────────────────────────────────────┘\n",
        "STABILITY_THRESHOLD = 0.70  # Features must appear in 70%+ of folds (was 0.80)\n",
        "\n",
        "print(f\"\"\"\n",
        "=====================================================================\n",
        "Cell 2: Nested Cross-Validation with REPEATED Stratified K-Fold\n",
        "         [VERSION 3.0 - Class Weights + Realistic Stability]\n",
        "\n",
        "CONFIGURATION:\n",
        "  Outer CV:            {N_OUTER_SPLITS}-fold × {N_REPEATS} repeats = {N_OUTER_SPLITS * N_REPEATS} iterations\n",
        "  Inner CV:            {N_INNER_SPLITS}-fold (hyperparameter tuning)\n",
        "\n",
        "  Feature Selection:\n",
        "    |log2FC| threshold:  > {LOG2FC_THRESHOLD}\n",
        "    FDR q-value:         < {FDR_THRESHOLD}\n",
        "    Consensus:           2-of-3 methods (unchanged)\n",
        "\n",
        "  Stability Threshold:   {STABILITY_THRESHOLD*100:.0f}% (lowered from 80%)\n",
        "\n",
        "  CRITICAL CHANGES (v3.0):\n",
        "    ✓ SMOTE removed (class_weight='balanced' used instead)\n",
        "    ✓ Stability threshold: 80% → 70% (realistic for n=99)\n",
        "    ✓ Dual-path feature selection implemented\n",
        "\n",
        "This provides more stable performance estimates with variance.\n",
        "=====================================================================\n",
        "\"\"\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. Load Expression Data from Cell 1 (NO Normalization Applied!)\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 1: Load Expression Data from Cell 1\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Extract raw expression matrix (should be log2-transformed, NO quantile norm!)\n",
        "expr_raw = df_expression.drop(columns=['label'])\n",
        "labels_full = df_expression['label'].values\n",
        "sample_ids_full = df_expression.index.tolist()\n",
        "\n",
        "# Apply log2 transformation if not already done\n",
        "if expr_raw.values.max() > 20:\n",
        "    print(\"[INFO] Applying log2(x + 1) transformation to raw expression values\")\n",
        "    expr_log_full = np.log2(expr_raw + 1.0)\n",
        "else:\n",
        "    print(\"[INFO] Data appears log2-transformed. Using as-is.\")\n",
        "    expr_log_full = expr_raw.copy()\n",
        "\n",
        "# Handle infinite values from log2 transformation\n",
        "expr_log_full = expr_log_full.replace([np.inf, -np.inf], np.nan)\n",
        "expr_log_full = expr_log_full.fillna(expr_log_full.median(axis=0))\n",
        "\n",
        "# Convert to DataFrame with proper indices\n",
        "expr_log_full = pd.DataFrame(\n",
        "    expr_log_full.values,\n",
        "    index=sample_ids_full,\n",
        "    columns=expr_raw.columns\n",
        ")\n",
        "\n",
        "print(f\"[INFO] Expression matrix loaded: {expr_log_full.shape}\")\n",
        "print(f\"  Samples: {len(sample_ids_full)}\")\n",
        "print(f\"  Probes: {expr_log_full.shape[1]}\")\n",
        "print(f\"  Healthy controls: {sum(labels_full == 0)}\")\n",
        "print(f\"  CRC patients: {sum(labels_full == 1)}\")\n",
        "print(f\"  Class imbalance ratio: 1:{sum(labels_full == 1)/sum(labels_full == 0):.1f}\")\n",
        "\n",
        "# CRITICAL: Ensure NO normalization has been applied globally\n",
        "print(\"\\n[VERIFICATION] Checking for data leakage risks:\")\n",
        "print(f\"  Max value: {expr_log_full.values.max():.2f}\")\n",
        "print(f\"  Min value: {expr_log_full.values.min():.2f}\")\n",
        "print(f\"  Global mean: {expr_log_full.values.mean():.2f}\")\n",
        "print(f\"  Global std: {expr_log_full.values.std():.2f}\")\n",
        "if abs(expr_log_full.values.mean()) < 0.1 and abs(expr_log_full.values.std() - 1.0) < 0.1:\n",
        "    print(\"  ⚠️  WARNING: Data appears globally normalized! Risk of data leakage!\")\n",
        "else:\n",
        "    print(\"  ✓ Data is NOT globally normalized. Safe for fold-wise processing.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. Cross-Validation Configuration\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 2: Configure Cross-Validation Strategy\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "outer_cv = RepeatedStratifiedKFold(\n",
        "    n_splits=N_OUTER_SPLITS,\n",
        "    n_repeats=N_REPEATS,\n",
        "    random_state=SEED\n",
        ")\n",
        "\n",
        "# Inner CV: 3-fold stratified for hyperparameter tuning (unchanged)\n",
        "inner_cv = StratifiedKFold(n_splits=N_INNER_SPLITS, shuffle=True, random_state=SEED)\n",
        "\n",
        "total_iterations = N_OUTER_SPLITS * N_REPEATS\n",
        "print(f\"[INFO] Outer CV: {N_OUTER_SPLITS}-fold × {N_REPEATS} repeats = {total_iterations} iterations\")\n",
        "print(f\"[INFO] Inner CV: {inner_cv.get_n_splits()}-fold stratified\")\n",
        "print(f\"[INFO] Random seed: {SEED}\")\n",
        "\n",
        "# Create results directory\n",
        "RESULT_DIR = base_save_path\n",
        "print(f\"[INFO] Results will be saved to: {RESULT_DIR}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# ┌─────────────────────────────────────────────────────────────────────────────┐\n",
        "# │ CHANGE 4: Model Configurations - ALL use class_weight, NO SMOTE            │\n",
        "# └─────────────────────────────────────────────────────────────────────────────┘\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 3: Define Model Configurations (Class Weights Only)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "model_configs = {}\n",
        "\n",
        "# Random Forest with class weighting (PRIMARY - recommended for small samples)\n",
        "model_configs[\"RandomForest_Weighted\"] = {\n",
        "    \"use_smote\": False,  # CRITICAL: No SMOTE\n",
        "    \"use_scaler\": False,  # RF doesn't require scaling\n",
        "    \"classifier\": \"rf\",\n",
        "    \"param_grid\": {\n",
        "        \"clf__n_estimators\": [200, 500],\n",
        "        \"clf__max_depth\": [None, 5, 10],\n",
        "        \"clf__max_features\": [0.3, 0.5, \"sqrt\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Support Vector Machine with class weighting (No SMOTE)\n",
        "model_configs[\"SVM_Weighted\"] = {\n",
        "    \"use_smote\": False,  # CRITICAL: No SMOTE\n",
        "    \"use_scaler\": True,  # SVM requires scaling\n",
        "    \"classifier\": \"svm\",\n",
        "    \"param_grid\": {\n",
        "        \"clf__C\": [0.1, 1, 10],\n",
        "        \"clf__gamma\": [\"scale\", \"auto\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Logistic Regression with class weighting (No SMOTE)\n",
        "model_configs[\"LogisticRegression_Weighted\"] = {\n",
        "    \"use_smote\": False,  # CRITICAL: No SMOTE\n",
        "    \"use_scaler\": True,  # LR benefits from scaling\n",
        "    \"classifier\": \"lr\",\n",
        "    \"param_grid\": {\n",
        "        \"clf__C\": [0.01, 0.1, 1, 10],\n",
        "        \"clf__penalty\": [\"l2\"],\n",
        "        \"clf__solver\": [\"lbfgs\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "# ┌─────────────────────────────────────────────────────────────────────────────┐\n",
        "# │ OPTIONAL: Keep one SMOTE model for sensitivity analysis                    │\n",
        "# │ (Can be commented out if you want pure class_weight approach)              │\n",
        "# └─────────────────────────────────────────────────────────────────────────────┘\n",
        "# Note: Uncomment below if you want to compare SMOTE vs class_weight\n",
        "# from imblearn.over_sampling import SMOTE\n",
        "# from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "# model_configs[\"RandomForest_SMOTE\"] = {\n",
        "#     \"use_smote\": True,\n",
        "#     \"use_scaler\": False,\n",
        "#     \"classifier\": \"rf\",\n",
        "#     \"param_grid\": {\n",
        "#         \"clf__n_estimators\": [200, 500],\n",
        "#         \"clf__max_depth\": [None, 5, 10],\n",
        "#         \"clf__max_features\": [0.3, 0.5, \"sqrt\"]\n",
        "#     }\n",
        "# }\n",
        "\n",
        "print(f\"[INFO] Configured {len(model_configs)} model variants:\")\n",
        "for model_name, cfg in model_configs.items():\n",
        "    smote_status = \"SMOTE\" if cfg[\"use_smote\"] else \"class_weight\"\n",
        "    print(f\"  - {model_name} ({smote_status})\")\n",
        "\n",
        "print(\"\\n[RATIONALE] Why class_weight over SMOTE:\")\n",
        "print(\"  - With only 11 controls, SMOTE's k=5 neighbors is problematic\")\n",
        "print(\"  - After 5-fold split: ~9 controls per training fold\")\n",
        "print(\"  - Demircioğlu (2024): SMOTE before CV causes up to +0.34 AUC bias\")\n",
        "print(\"  - class_weight='balanced' achieves similar effect without synthetic data\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. Utility Functions\n",
        "# ==============================================================================\n",
        "\n",
        "def bootstrap_auc_ci(y_true, y_proba, n_bootstrap=1000, alpha=0.05, random_state=SEED):\n",
        "    \"\"\"\n",
        "    Compute bootstrap confidence interval for ROC-AUC.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y_true : array-like\n",
        "        True binary labels\n",
        "    y_proba : array-like\n",
        "        Predicted probabilities for positive class\n",
        "    n_bootstrap : int, default=1000\n",
        "        Number of bootstrap iterations\n",
        "    alpha : float, default=0.05\n",
        "        Significance level (0.05 for 95% CI)\n",
        "    random_state : int\n",
        "        Random seed for reproducibility\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    lower, upper : float\n",
        "        Lower and upper bounds of confidence interval\n",
        "    \"\"\"\n",
        "    rng = np.random.RandomState(random_state)\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_proba = np.asarray(y_proba)\n",
        "    n = len(y_true)\n",
        "\n",
        "    aucs = []\n",
        "    for _ in range(n_bootstrap):\n",
        "        indices = rng.choice(n, n, replace=True)\n",
        "\n",
        "        # Ensure both classes are represented in the bootstrap sample\n",
        "        if len(np.unique(y_true[indices])) < 2:\n",
        "            continue\n",
        "\n",
        "        aucs.append(roc_auc_score(y_true[indices], y_proba[indices]))\n",
        "\n",
        "    if len(aucs) == 0:\n",
        "        return np.nan, np.nan\n",
        "\n",
        "    lower = np.percentile(aucs, 100 * (alpha / 2))\n",
        "    upper = np.percentile(aucs, 100 * (1 - alpha / 2))\n",
        "\n",
        "    return float(lower), float(upper)\n",
        "\n",
        "\n",
        "def perform_fold_feature_selection(expr_train, y_train, fold_idx, verbose=True):\n",
        "    \"\"\"\n",
        "    Three-stage feature selection performed ONLY on training data.\n",
        "\n",
        "    Stage 1: Biological filtering (|log2FC|, FDR q-value)\n",
        "    Stage 2: Multi-method selection (LASSO, SVM-RFE, Random Forest)\n",
        "    Stage 3: Consensus voting (2-of-3 agreement) - UNCHANGED\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    expr_train : pd.DataFrame\n",
        "        Expression matrix for training samples (samples × probes)\n",
        "    y_train : np.ndarray\n",
        "        Binary labels for training samples\n",
        "    fold_idx : int or str\n",
        "        Fold identifier for logging\n",
        "    verbose : bool\n",
        "        Whether to print progress information\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    selected_features : list\n",
        "        List of selected probe IDs\n",
        "    feature_info : dict\n",
        "        Statistics about feature selection process\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(f\"\\n    [Fold {fold_idx}] Feature selection on training data only\")\n",
        "        print(f\"      Train samples: {len(expr_train)} ({sum(y_train==0)} HC, {sum(y_train==1)} CRC)\")\n",
        "        print(f\"      Total probes: {expr_train.shape[1]}\")\n",
        "\n",
        "    # Separate control and cancer samples\n",
        "    expr_control = expr_train[y_train == 0]\n",
        "    expr_cancer = expr_train[y_train == 1]\n",
        "\n",
        "    # =========================================================================\n",
        "    # Stage 1: Biological Filtering with ADJUSTABLE Thresholds\n",
        "    # =========================================================================\n",
        "    fold_changes = {}\n",
        "    p_values = {}\n",
        "    log2_fold_changes = {}\n",
        "\n",
        "    probe_list = list(expr_train.columns)\n",
        "    raw_pvals = []\n",
        "\n",
        "    for probe in probe_list:\n",
        "        control_vals = expr_control[probe].values\n",
        "        cancer_vals = expr_cancer[probe].values\n",
        "\n",
        "        control_mean = control_vals.mean()\n",
        "        cancer_mean = cancer_vals.mean()\n",
        "\n",
        "        # log2 fold change (cancer vs control)\n",
        "        log2fc = cancer_mean - control_mean\n",
        "        log2_fold_changes[probe] = log2fc\n",
        "        fold_changes[probe] = 2 ** log2fc\n",
        "\n",
        "        # Mann-Whitney U test (non-parametric)\n",
        "        try:\n",
        "            _, pval = mannwhitneyu(control_vals, cancer_vals, alternative='two-sided')\n",
        "        except:\n",
        "            pval = 1.0\n",
        "\n",
        "        raw_pvals.append(pval)\n",
        "        p_values[probe] = pval\n",
        "\n",
        "    # Benjamini-Hochberg FDR correction\n",
        "    raw_pvals = np.array(raw_pvals)\n",
        "    n_tests = len(raw_pvals)\n",
        "    sorted_indices = np.argsort(raw_pvals)\n",
        "\n",
        "    adjusted_pvals = np.zeros(n_tests)\n",
        "    for i, idx in enumerate(sorted_indices):\n",
        "        rank = i + 1\n",
        "        adjusted_pvals[idx] = min(1.0, raw_pvals[idx] * n_tests / rank)\n",
        "\n",
        "    # Ensure monotonicity (cumulative minimum from the end)\n",
        "    for i in range(n_tests - 2, -1, -1):\n",
        "        if adjusted_pvals[sorted_indices[i]] > adjusted_pvals[sorted_indices[i + 1]]:\n",
        "            adjusted_pvals[sorted_indices[i]] = adjusted_pvals[sorted_indices[i + 1]]\n",
        "\n",
        "    # Store adjusted p-values (q-values)\n",
        "    q_values = {probe: adjusted_pvals[i] for i, probe in enumerate(probe_list)}\n",
        "\n",
        "    # Apply thresholds\n",
        "    selected_bio = []\n",
        "    for probe in probe_list:\n",
        "        log2fc = log2_fold_changes[probe]\n",
        "        qval = q_values[probe]\n",
        "\n",
        "        # Criteria: |log2FC| > threshold AND FDR-adjusted q-value < threshold\n",
        "        if abs(log2fc) > LOG2FC_THRESHOLD and qval < FDR_THRESHOLD:\n",
        "            selected_bio.append(probe)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"      Stage 1: {len(selected_bio)} probes (|log2FC| > {LOG2FC_THRESHOLD}, FDR q < {FDR_THRESHOLD})\")\n",
        "\n",
        "    # =========================================================================\n",
        "    # Stage 2: Multi-method Consensus (UNCHANGED)\n",
        "    # =========================================================================\n",
        "    if verbose:\n",
        "        print(f\"      Stage 2: Multi-method selection (LASSO, SVM-RFE, RF)\")\n",
        "\n",
        "    X_bio = expr_train[selected_bio].values\n",
        "\n",
        "    # Standardize for methods that require it (LASSO, SVM-RFE)\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X_bio)\n",
        "\n",
        "    # ─────────────────────────────────────────────────────────────────────────\n",
        "    # Method A: LASSO Regularization\n",
        "    # ─────────────────────────────────────────────────────────────────────────\n",
        "    try:\n",
        "        lasso = LassoCV(\n",
        "            cv=3,\n",
        "            random_state=SEED,\n",
        "            max_iter=20000,\n",
        "            n_jobs=-1,\n",
        "            tol=1e-3\n",
        "        )\n",
        "        lasso.fit(X_scaled, y_train)\n",
        "\n",
        "        # Select features with non-zero coefficients\n",
        "        lasso_feat = [selected_bio[i] for i, c in enumerate(lasso.coef_) if c != 0]\n",
        "\n",
        "        # Fallback: select top features by coefficient magnitude if none selected\n",
        "        if len(lasso_feat) == 0:\n",
        "            lasso_coef_abs = np.abs(lasso.coef_)\n",
        "            top_idx = np.argsort(lasso_coef_abs)[-200:]\n",
        "            lasso_feat = [selected_bio[i] for i in top_idx]\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"        LASSO: {len(lasso_feat)} features\")\n",
        "\n",
        "    except Exception as e:\n",
        "        if verbose:\n",
        "            print(f\"        LASSO failed ({str(e)}), using variance fallback\")\n",
        "        var_bio = expr_train[selected_bio].var(axis=0)\n",
        "        lasso_feat = var_bio.nlargest(min(200, len(selected_bio))).index.tolist()\n",
        "\n",
        "    # ─────────────────────────────────────────────────────────────────────────\n",
        "    # Method B: SVM-RFE (Recursive Feature Elimination)\n",
        "    # ─────────────────────────────────────────────────────────────────────────\n",
        "    try:\n",
        "        n_features_target = min(150, len(selected_bio) // 3)\n",
        "        n_features_target = max(10, n_features_target)\n",
        "\n",
        "        svc = SVC(kernel='linear', class_weight='balanced', random_state=SEED)\n",
        "        rfe = RFE(\n",
        "            estimator=svc,\n",
        "            n_features_to_select=n_features_target,\n",
        "            step=10,\n",
        "            verbose=0\n",
        "        )\n",
        "        rfe.fit(X_scaled, y_train)\n",
        "\n",
        "        svm_rfe_feat = [selected_bio[i] for i in np.where(rfe.support_)[0]]\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"        SVM-RFE: {len(svm_rfe_feat)} features\")\n",
        "\n",
        "    except Exception as e:\n",
        "        if verbose:\n",
        "            print(f\"        SVM-RFE failed ({str(e)}), using variance fallback\")\n",
        "        var_bio = expr_train[selected_bio].var(axis=0)\n",
        "        svm_rfe_feat = var_bio.nlargest(min(150, len(selected_bio))).index.tolist()\n",
        "\n",
        "    # ─────────────────────────────────────────────────────────────────────────\n",
        "    # Method C: Random Forest Feature Importance\n",
        "    # ─────────────────────────────────────────────────────────────────────────\n",
        "    try:\n",
        "        rf = RandomForestClassifier(\n",
        "            n_estimators=200,\n",
        "            random_state=SEED,\n",
        "            n_jobs=-1,\n",
        "            class_weight='balanced'\n",
        "        )\n",
        "        rf.fit(X_bio, y_train)\n",
        "\n",
        "        importances = rf.feature_importances_\n",
        "        n_rf = min(150, len(selected_bio))\n",
        "        top_idx = np.argsort(importances)[-n_rf:]\n",
        "        rf_feat = [selected_bio[i] for i in top_idx]\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"        Random Forest: {len(rf_feat)} features\")\n",
        "\n",
        "    except Exception as e:\n",
        "        if verbose:\n",
        "            print(f\"        Random Forest failed ({str(e)}), using variance fallback\")\n",
        "        var_bio = expr_train[selected_bio].var(axis=0)\n",
        "        rf_feat = var_bio.nlargest(min(150, len(selected_bio))).index.tolist()\n",
        "\n",
        "    # ─────────────────────────────────────────────────────────────────────────\n",
        "    # Stage 3: Consensus Voting (2-of-3) - UNCHANGED\n",
        "    # ─────────────────────────────────────────────────────────────────────────\n",
        "    feature_votes = Counter()\n",
        "    for feat_set in [set(lasso_feat), set(svm_rfe_feat), set(rf_feat)]:\n",
        "        feature_votes.update(feat_set)\n",
        "\n",
        "    # Select features appearing in at least 2 methods (UNCHANGED)\n",
        "    selected_consensus = [f for f, count in feature_votes.items() if count >= 2]\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"      Stage 3: {len(selected_consensus)} features (≥2/3 consensus)\")\n",
        "\n",
        "    # Ensure minimum feature count for stable model training (UNCHANGED)\n",
        "    if len(selected_consensus) < 20:\n",
        "        sorted_features = sorted(feature_votes.items(), key=lambda x: x[1], reverse=True)\n",
        "        selected_consensus = [f for f, _ in sorted_features[:max(20, len(sorted_features))]]\n",
        "        if verbose:\n",
        "            print(f\"      Adjusted to {len(selected_consensus)} features (minimum threshold)\")\n",
        "\n",
        "    # Compile feature selection statistics\n",
        "    feature_info = {\n",
        "        'n_stage1': len(selected_bio),\n",
        "        'n_lasso': len(lasso_feat),\n",
        "        'n_svm_rfe': len(svm_rfe_feat),\n",
        "        'n_rf': len(rf_feat),\n",
        "        'n_consensus': len(selected_consensus),\n",
        "        'fold_changes': {k: float(v) for k, v in fold_changes.items() if k in selected_consensus},\n",
        "        'p_values': {k: float(v) for k, v in p_values.items() if k in selected_consensus}\n",
        "    }\n",
        "\n",
        "    return selected_consensus, feature_info\n",
        "\n",
        "\n",
        "# ┌─────────────────────────────────────────────────────────────────────────────┐\n",
        "# │ CHANGE 5: create_pipeline now uses sklearn.Pipeline (no SMOTE)             │\n",
        "# └─────────────────────────────────────────────────────────────────────────────┘\n",
        "def create_pipeline(config, n_features):\n",
        "    \"\"\"\n",
        "    Construct sklearn pipeline based on model configuration.\n",
        "\n",
        "    CRITICAL CHANGE (v3.0):\n",
        "        - No longer uses SMOTE (imblearn)\n",
        "        - All classifiers use class_weight='balanced' for imbalance handling\n",
        "        - Uses sklearn.Pipeline instead of imblearn.Pipeline\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    config : dict\n",
        "        Model configuration dictionary\n",
        "    n_features : int\n",
        "        Number of features (for potential future use)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pipeline : sklearn.Pipeline\n",
        "        Fitted pipeline object\n",
        "    param_grid : dict\n",
        "        Hyperparameter grid for GridSearchCV\n",
        "    \"\"\"\n",
        "    steps = []\n",
        "\n",
        "    # Add StandardScaler if requested (always before classifier)\n",
        "    if config[\"use_scaler\"]:\n",
        "        steps.append((\"scaler\", StandardScaler()))\n",
        "\n",
        "    # Add classifier (always last in pipeline)\n",
        "    # ALL classifiers now use class_weight='balanced' for imbalance handling\n",
        "    if config[\"classifier\"] == \"rf\":\n",
        "        clf = RandomForestClassifier(\n",
        "            random_state=SEED,\n",
        "            n_jobs=-1,\n",
        "            class_weight=\"balanced_subsample\"  # Handles imbalance per bootstrap sample\n",
        "        )\n",
        "    elif config[\"classifier\"] == \"svm\":\n",
        "        clf = SVC(\n",
        "            probability=True,\n",
        "            random_state=SEED,\n",
        "            class_weight=\"balanced\"  # Handles imbalance via inverse class frequency\n",
        "        )\n",
        "    elif config[\"classifier\"] == \"lr\":\n",
        "        clf = LogisticRegression(\n",
        "            max_iter=1000,\n",
        "            random_state=SEED,\n",
        "            class_weight=\"balanced\"  # Handles imbalance via inverse class frequency\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown classifier: {config['classifier']}\")\n",
        "\n",
        "    steps.append((\"clf\", clf))\n",
        "\n",
        "    # Create standard sklearn Pipeline (no SMOTE)\n",
        "    pipeline = Pipeline(steps)\n",
        "    param_grid = config[\"param_grid\"].copy()\n",
        "\n",
        "    return pipeline, param_grid\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. Nested Cross-Validation with Fold-wise Feature Selection\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 4: Execute Nested Cross-Validation\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Initialize result storage\n",
        "results = {}\n",
        "oof_predictions = {}\n",
        "fold_feature_lists = {}\n",
        "fold_feature_info = {}\n",
        "\n",
        "# Iterate through each model configuration\n",
        "for model_name, model_cfg in model_configs.items():\n",
        "\n",
        "    print(\"\\n\" + \"-\" * 80)\n",
        "    print(f\"Model: {model_name}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # Store predictions for each repeat separately\n",
        "    oof_proba_repeats = np.zeros((N_REPEATS, len(labels_full)))\n",
        "    oof_pred_repeats = np.zeros((N_REPEATS, len(labels_full)), dtype=int)\n",
        "\n",
        "    # Track performance across all iterations\n",
        "    train_auc_folds = []\n",
        "    test_auc_folds = []\n",
        "\n",
        "    # Track AUC per repeat for variance estimation\n",
        "    repeat_aucs = []\n",
        "\n",
        "    fold_feature_lists[model_name] = {}\n",
        "    fold_feature_info[model_name] = {}\n",
        "\n",
        "    print(f\"\\n  Running {total_iterations} iterations ({N_OUTER_SPLITS}-fold × {N_REPEATS} repeats)...\")\n",
        "\n",
        "    # ─────────────────────────────────────────────────────────────────────────\n",
        "    # Outer CV Loop: Iterate through train/test splits\n",
        "    # ─────────────────────────────────────────────────────────────────────────\n",
        "    for iteration, (train_idx, test_idx) in enumerate(outer_cv.split(expr_log_full, labels_full)):\n",
        "\n",
        "        # Determine which repeat and fold we're in\n",
        "        repeat_idx = iteration // N_OUTER_SPLITS\n",
        "        fold_idx = iteration % N_OUTER_SPLITS + 1\n",
        "\n",
        "        # Extract training fold data\n",
        "        train_sample_ids = [sample_ids_full[i] for i in train_idx]\n",
        "        expr_train_fold = expr_log_full.loc[train_sample_ids].copy()\n",
        "        y_train_fold = labels_full[train_idx]\n",
        "\n",
        "        # Extract test fold data\n",
        "        test_sample_ids = [sample_ids_full[i] for i in test_idx]\n",
        "        y_test_fold = labels_full[test_idx]\n",
        "\n",
        "        # Progress indicator (every 10 iterations)\n",
        "        if iteration % 10 == 0:\n",
        "            train_pos = int(y_train_fold.sum())\n",
        "            test_pos = int(y_test_fold.sum())\n",
        "            print(f\"\\n    Iteration {iteration+1}/{total_iterations} (Repeat {repeat_idx+1}, Fold {fold_idx})\")\n",
        "            print(f\"      Train: {train_pos}/{len(y_train_fold)} CRC | Test: {test_pos}/{len(y_test_fold)} CRC\")\n",
        "\n",
        "        # ─────────────────────────────────────────────────────────────────────\n",
        "        # Feature Selection (WITHIN TRAINING FOLD ONLY)\n",
        "        # ─────────────────────────────────────────────────────────────────────\n",
        "        selected_features, feat_info = perform_fold_feature_selection(\n",
        "            expr_train_fold,\n",
        "            y_train_fold,\n",
        "            fold_idx=f\"R{repeat_idx+1}F{fold_idx}\",\n",
        "            verbose=(iteration % 10 == 0)  # Verbose only every 10 iterations\n",
        "        )\n",
        "\n",
        "        fold_feature_lists[model_name][f'repeat_{repeat_idx+1}_fold_{fold_idx}'] = selected_features\n",
        "        fold_feature_info[model_name][f'repeat_{repeat_idx+1}_fold_{fold_idx}'] = feat_info\n",
        "\n",
        "        # Prepare data matrices with selected features\n",
        "        X_train_fold = expr_train_fold[selected_features].values\n",
        "        X_test_fold = expr_log_full.loc[test_sample_ids][selected_features].values\n",
        "\n",
        "        # ─────────────────────────────────────────────────────────────────────\n",
        "        # Hyperparameter Tuning (Inner CV on training fold)\n",
        "        # ─────────────────────────────────────────────────────────────────────\n",
        "        pipeline, param_grid = create_pipeline(model_cfg, len(selected_features))\n",
        "\n",
        "        gs = GridSearchCV(\n",
        "            estimator=pipeline,\n",
        "            param_grid=param_grid,\n",
        "            scoring=\"roc_auc\",\n",
        "            cv=inner_cv,\n",
        "            n_jobs=-1,\n",
        "            refit=True,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        gs.fit(X_train_fold, y_train_fold)\n",
        "\n",
        "        # ─────────────────────────────────────────────────────────────────────\n",
        "        # Evaluate on Training and Test Folds\n",
        "        # ─────────────────────────────────────────────────────────────────────\n",
        "        # Training performance\n",
        "        train_proba = gs.predict_proba(X_train_fold)[:, 1]\n",
        "        train_auc = roc_auc_score(y_train_fold, train_proba)\n",
        "        train_auc_folds.append(train_auc)\n",
        "\n",
        "        # Test performance (out-of-fold)\n",
        "        test_proba = gs.predict_proba(X_test_fold)[:, 1]\n",
        "        test_pred = (test_proba >= 0.5).astype(int)\n",
        "        test_auc = roc_auc_score(y_test_fold, test_proba)\n",
        "        test_auc_folds.append(test_auc)\n",
        "\n",
        "        # Store OOF predictions for this repeat\n",
        "        oof_proba_repeats[repeat_idx, test_idx] = test_proba\n",
        "        oof_pred_repeats[repeat_idx, test_idx] = test_pred\n",
        "\n",
        "        # At the end of each repeat, calculate that repeat's AUC\n",
        "        if fold_idx == N_OUTER_SPLITS:\n",
        "            rep_auc = roc_auc_score(labels_full, oof_proba_repeats[repeat_idx])\n",
        "            repeat_aucs.append(rep_auc)\n",
        "            print(f\"    → Repeat {repeat_idx+1} complete: OOF AUC = {rep_auc:.4f}\")\n",
        "\n",
        "    # =========================================================================\n",
        "    # Aggregate OOF Predictions Across Repeats\n",
        "    # =========================================================================\n",
        "    # Average predictions across all repeats\n",
        "    oof_proba = oof_proba_repeats.mean(axis=0)\n",
        "    oof_pred = (oof_proba >= 0.5).astype(int)\n",
        "\n",
        "    # ─────────────────────────────────────────────────────────────────────────\n",
        "    # Compute Out-of-Fold Performance Metrics\n",
        "    # ─────────────────────────────────────────────────────────────────────────\n",
        "    auc = roc_auc_score(labels_full, oof_proba)\n",
        "    ci_lower, ci_upper = bootstrap_auc_ci(labels_full, oof_proba)\n",
        "\n",
        "    acc = accuracy_score(labels_full, oof_pred)\n",
        "    bal_acc = balanced_accuracy_score(labels_full, oof_pred)\n",
        "\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        labels_full, oof_pred, average='binary', zero_division=0\n",
        "    )\n",
        "\n",
        "    cm = confusion_matrix(labels_full, oof_pred)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
        "    sensitivity = recall\n",
        "\n",
        "    train_auc_mean = np.mean(train_auc_folds)\n",
        "    test_auc_mean = np.mean(test_auc_folds)\n",
        "\n",
        "    # Report mean ± std across repeats\n",
        "    repeat_auc_mean = np.mean(repeat_aucs)\n",
        "    repeat_auc_std = np.std(repeat_aucs)\n",
        "\n",
        "    print(\"\\n  \" + \"=\" * 76)\n",
        "    print(\"  OUT-OF-FOLD PERFORMANCE (Unbiased Estimate)\")\n",
        "    print(\"  \" + \"=\" * 76)\n",
        "    print(f\"    ROC-AUC (aggregated):  {auc:.4f}\")\n",
        "    print(f\"    ROC-AUC (mean±std):    {repeat_auc_mean:.4f} ± {repeat_auc_std:.4f}\")\n",
        "    print(f\"    95% CI:                [{ci_lower:.4f}, {ci_upper:.4f}]\")\n",
        "    print(f\"    Accuracy:              {acc:.4f}\")\n",
        "    print(f\"    Balanced Accuracy:     {bal_acc:.4f}\")\n",
        "    print(f\"    Precision:             {precision:.4f}\")\n",
        "    print(f\"    Sensitivity:           {sensitivity:.4f}\")\n",
        "    print(f\"    Specificity:           {specificity:.4f}\")\n",
        "    print(f\"    F1-score:              {f1:.4f}\")\n",
        "    print(f\"    Mean Train AUC:        {train_auc_mean:.4f}\")\n",
        "    print(f\"    Mean Test AUC:         {test_auc_mean:.4f}\")\n",
        "    print(\"    Confusion Matrix:\")\n",
        "    print(\"    \" + str(cm).replace('\\n', '\\n    '))\n",
        "\n",
        "    # ─────────────────────────────────────────────────────────────────────────\n",
        "    # ┌─────────────────────────────────────────────────────────────────────────┐\n",
        "    # │ CHANGE 6: Feature Stability Analysis with 70% threshold (was 80%)      │\n",
        "    # └─────────────────────────────────────────────────────────────────────────┘\n",
        "    # ─────────────────────────────────────────────────────────────────────────\n",
        "    feature_frequency = Counter()\n",
        "    for features in fold_feature_lists[model_name].values():\n",
        "        feature_frequency.update(features)\n",
        "\n",
        "    n_folds = len(fold_feature_lists[model_name])\n",
        "\n",
        "    # ┌─────────────────────────────────────────────────────────────────────────┐\n",
        "    # │ Stability thresholds adjusted per Liu et al. (2025)                     │\n",
        "    # └─────────────────────────────────────────────────────────────────────────┘\n",
        "    highly_stable = [f for f, count in feature_frequency.items()\n",
        "                     if count >= n_folds * STABILITY_THRESHOLD]  # 70% (was 80%)\n",
        "    moderately_stable = [f for f, count in feature_frequency.items()\n",
        "                         if count >= n_folds * 0.5]  # 50%\n",
        "\n",
        "    print(f\"\\n  Feature Stability Across {n_folds} Iterations:\")\n",
        "    print(f\"    Selected in 100%:    {sum(1 for c in feature_frequency.values() if c == n_folds)}\")\n",
        "    print(f\"    Selected in ≥{STABILITY_THRESHOLD*100:.0f}%:   {len(highly_stable)}\")\n",
        "    print(f\"    Selected in ≥50%:    {len(moderately_stable)}\")\n",
        "\n",
        "    # Calculate Kuncheva Stability Index (optional, for reporting)\n",
        "    # This measures consistency of feature selection across folds\n",
        "    def calculate_kuncheva_index(feature_lists, total_features):\n",
        "        \"\"\"\n",
        "        Calculate Kuncheva stability index for feature selection.\n",
        "\n",
        "        Reference: Kuncheva LI (2007) A stability index for feature selection.\n",
        "        \"\"\"\n",
        "        k = len(feature_lists)\n",
        "        if k < 2:\n",
        "            return np.nan\n",
        "\n",
        "        # Get all pairwise similarities\n",
        "        similarities = []\n",
        "        lists = list(feature_lists.values())\n",
        "        for i in range(k):\n",
        "            for j in range(i + 1, k):\n",
        "                set_i = set(lists[i])\n",
        "                set_j = set(lists[j])\n",
        "                intersection = len(set_i & set_j)\n",
        "                n_i = len(set_i)\n",
        "                n_j = len(set_j)\n",
        "\n",
        "                if n_i == 0 or n_j == 0:\n",
        "                    continue\n",
        "\n",
        "                # Kuncheva formula\n",
        "                expected = (n_i * n_j) / total_features\n",
        "                max_val = min(n_i, n_j) - expected\n",
        "                if max_val <= 0:\n",
        "                    continue\n",
        "\n",
        "                ksi = (intersection - expected) / max_val\n",
        "                similarities.append(ksi)\n",
        "\n",
        "        return np.mean(similarities) if similarities else np.nan\n",
        "\n",
        "    kuncheva_idx = calculate_kuncheva_index(\n",
        "        fold_feature_lists[model_name],\n",
        "        expr_log_full.shape[1]\n",
        "    )\n",
        "    print(f\"    Kuncheva Stability Index: {kuncheva_idx:.4f}\")\n",
        "\n",
        "    # Store comprehensive results\n",
        "    results[model_name] = {\n",
        "        \"oof_auc\": float(auc),\n",
        "        \"oof_auc_mean\": float(repeat_auc_mean),\n",
        "        \"oof_auc_std\": float(repeat_auc_std),\n",
        "        \"oof_auc_ci\": [ci_lower, ci_upper],\n",
        "        \"oof_accuracy\": float(acc),\n",
        "        \"oof_balanced_accuracy\": float(bal_acc),\n",
        "        \"precision\": float(precision),\n",
        "        \"recall_sensitivity\": float(sensitivity),\n",
        "        \"specificity\": float(specificity),\n",
        "        \"f1_score\": float(f1),\n",
        "        \"confusion_matrix\": cm.tolist(),\n",
        "        \"train_auc_mean\": train_auc_mean,\n",
        "        \"train_auc_std\": float(np.std(train_auc_folds)),\n",
        "        \"test_auc_mean\": test_auc_mean,\n",
        "        \"test_auc_std\": float(np.std(test_auc_folds)),\n",
        "        \"n_repeats\": N_REPEATS,\n",
        "        \"n_folds\": N_OUTER_SPLITS,\n",
        "        \"repeat_aucs\": [float(x) for x in repeat_aucs],\n",
        "        \"kuncheva_stability_index\": float(kuncheva_idx) if not np.isnan(kuncheva_idx) else None,\n",
        "        \"feature_stability\": {\n",
        "            \"n_iterations\": n_folds,\n",
        "            \"stability_threshold\": STABILITY_THRESHOLD,\n",
        "            \"n_highly_stable\": len(highly_stable),\n",
        "            \"highly_stable_features\": highly_stable,\n",
        "            \"n_moderately_stable\": len(moderately_stable),\n",
        "            \"feature_frequency\": {str(k): int(v) for k, v in feature_frequency.most_common(50)}\n",
        "        }\n",
        "    }\n",
        "\n",
        "    oof_predictions[model_name] = {\n",
        "        \"y_true\": labels_full.copy(),\n",
        "        \"y_proba\": oof_proba.copy(),\n",
        "        \"y_pred\": oof_pred.copy()\n",
        "    }\n",
        "\n",
        "# ==============================================================================\n",
        "# 6. Model Selection and Comparison\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 5: Model Selection Summary\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Select best model by OOF AUC\n",
        "best_name = max(results, key=lambda k: results[k][\"oof_auc\"])\n",
        "\n",
        "print(\"\\nAll Models (ranked by OOF AUC):\")\n",
        "for m_name in sorted(results, key=lambda k: results[k][\"oof_auc\"], reverse=True):\n",
        "    res = results[m_name]\n",
        "    ksi = res.get('kuncheva_stability_index', 'N/A')\n",
        "    ksi_str = f\"{ksi:.3f}\" if isinstance(ksi, float) else ksi\n",
        "    print(f\"  {m_name:30s}: AUC = {res['oof_auc']:.4f} ± {res['oof_auc_std']:.4f} \"\n",
        "          f\"(Kuncheva: {ksi_str})\")\n",
        "\n",
        "print(f\"\\n{'=' * 80}\")\n",
        "print(f\"BEST MODEL: {best_name}\")\n",
        "print(f\"{'=' * 80}\")\n",
        "print(f\"  Out-of-fold AUC:         {results[best_name]['oof_auc']:.4f} ± {results[best_name]['oof_auc_std']:.4f}\")\n",
        "print(f\"  95% Confidence Interval: [{results[best_name]['oof_auc_ci'][0]:.4f}, \"\n",
        "      f\"{results[best_name]['oof_auc_ci'][1]:.4f}]\")\n",
        "print(f\"  Accuracy:                {results[best_name]['oof_accuracy']:.4f}\")\n",
        "print(f\"  Balanced Accuracy:       {results[best_name]['oof_balanced_accuracy']:.4f}\")\n",
        "print(f\"  Sensitivity:             {results[best_name]['recall_sensitivity']:.4f}\")\n",
        "print(f\"  Specificity:             {results[best_name]['specificity']:.4f}\")\n",
        "print(f\"  Kuncheva Stability:      {results[best_name].get('kuncheva_stability_index', 'N/A')}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# ┌─────────────────────────────────────────────────────────────────────────────┐\n",
        "# │ CHANGE 7: Dual-Path Feature Selection (Lewis 2023, Parvandeh 2020)         │\n",
        "# │                                                                             │\n",
        "# │ Path A: Production Model - Full-data retrained for external validation     │\n",
        "# │ Path B: CV-Stable Features - For biomarker reporting (>70% of folds)       │\n",
        "# └─────────────────────────────────────────────────────────────────────────────┘\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 6: DUAL-PATH Feature Selection\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# PATH B: CV-Stable Features (for Biomarker Reporting)\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "print(\"\\n[PATH B] CV-STABLE FEATURES (Biomarker Identification)\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Get features stable across CV folds (>= STABILITY_THRESHOLD)\n",
        "feature_freq_best = results[best_name]['feature_stability']['feature_frequency']\n",
        "cv_stable_features = [\n",
        "    f for f, count in feature_freq_best.items()\n",
        "    if int(count) >= int(total_iterations * STABILITY_THRESHOLD)\n",
        "]\n",
        "\n",
        "print(f\"  Stability threshold: {STABILITY_THRESHOLD*100:.0f}% ({int(total_iterations * STABILITY_THRESHOLD)}/{total_iterations} folds)\")\n",
        "print(f\"  CV-stable features:  {len(cv_stable_features)}\")\n",
        "\n",
        "# Save CV-stable features for biomarker reporting\n",
        "cv_stable_df = pd.DataFrame([\n",
        "    {\n",
        "        'Probe_ID': str(f),\n",
        "        'Selection_Count': int(feature_freq_best.get(f, feature_freq_best.get(str(f), 0))),\n",
        "        'Selection_Rate': f\"{int(feature_freq_best.get(f, feature_freq_best.get(str(f), 0)))/total_iterations*100:.1f}%\"\n",
        "    }\n",
        "    for f in cv_stable_features\n",
        "]).sort_values('Selection_Count', ascending=False)\n",
        "\n",
        "cv_stable_path = os.path.join(RESULT_DIR, f\"cv_stable_biomarkers_{best_name}.csv\")\n",
        "cv_stable_df.to_csv(cv_stable_path, index=False)\n",
        "print(f\"  Saved: {cv_stable_path}\")\n",
        "\n",
        "print(f\"\\n  [RATIONALE] These {len(cv_stable_features)} features are:\")\n",
        "print(f\"    - Selected consistently across {STABILITY_THRESHOLD*100:.0f}%+ of CV iterations\")\n",
        "print(f\"    - Suitable for reporting as 'validated biomarkers' in manuscript\")\n",
        "print(f\"    - NOT used for external validation (Path A features used instead)\")\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# PATH A: Production Model (for External Validation)\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "print(\"\\n[PATH A] PRODUCTION MODEL (External Validation)\")\n",
        "print(\"-\" * 60)\n",
        "print(\"[INFO] Applying identical feature selection procedure to FULL dataset...\")\n",
        "\n",
        "# Perform feature selection on full dataset (same procedure as each CV fold)\n",
        "expr_full_df = pd.DataFrame(\n",
        "    expr_log_full.values,\n",
        "    index=sample_ids_full,\n",
        "    columns=expr_log_full.columns\n",
        ")\n",
        "\n",
        "final_features, final_feat_info = perform_fold_feature_selection(\n",
        "    expr_full_df,\n",
        "    labels_full,\n",
        "    fold_idx=\"FULL_DATA\",\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(f\"\\n  Full-data selected features: {len(final_features)}\")\n",
        "\n",
        "X_final = expr_full_df[final_features].values\n",
        "\n",
        "# Store for downstream analysis\n",
        "feature_cols = final_features\n",
        "X = X_final\n",
        "y = labels_full\n",
        "\n",
        "# Train final model with hyperparameter optimization\n",
        "best_cfg = model_configs[best_name]\n",
        "final_pipeline, final_param_grid = create_pipeline(best_cfg, len(final_features))\n",
        "\n",
        "final_model = GridSearchCV(\n",
        "    estimator=final_pipeline,\n",
        "    param_grid=final_param_grid,\n",
        "    scoring=\"roc_auc\",\n",
        "    cv=inner_cv,\n",
        "    n_jobs=-1,\n",
        "    refit=True,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "final_model.fit(X_final, labels_full)\n",
        "\n",
        "# Store best model and parameters\n",
        "best_full_model = final_model.best_estimator_\n",
        "best_full_params = final_model.best_params_\n",
        "\n",
        "results[best_name][\"best_estimator\"] = best_full_model\n",
        "results[best_name][\"best_params\"] = best_full_params\n",
        "\n",
        "print(f\"\\n  Final Model Configuration:\")\n",
        "print(f\"    Model type: {best_name}\")\n",
        "print(f\"    Features: {len(feature_cols)}\")\n",
        "print(f\"    Hyperparameters:\")\n",
        "for k, v in best_full_params.items():\n",
        "    print(f\"      {k}: {v}\")\n",
        "\n",
        "# Save production model features\n",
        "production_features_path = os.path.join(RESULT_DIR, f\"production_model_features_{best_name}.csv\")\n",
        "production_df = pd.DataFrame({'Probe_ID': feature_cols})\n",
        "production_df.to_csv(production_features_path, index=False)\n",
        "print(f\"\\n  Saved: {production_features_path}\")\n",
        "\n",
        "print(f\"\\n  [RATIONALE] These {len(feature_cols)} features are:\")\n",
        "print(f\"    - Selected using identical procedure applied to FULL n=99 dataset\")\n",
        "print(f\"    - Used for external validation (represents 'deployed' model)\")\n",
        "print(f\"    - Performance estimate comes from nested CV, NOT resubstitution\")\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# DUAL-PATH SUMMARY\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"DUAL-PATH FEATURE SELECTION SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Calculate overlap between paths\n",
        "path_a_set = set([str(f) for f in feature_cols])\n",
        "path_b_set = set([str(f) for f in cv_stable_features])\n",
        "overlap = path_a_set & path_b_set\n",
        "only_a = path_a_set - path_b_set\n",
        "only_b = path_b_set - path_a_set\n",
        "\n",
        "print(f\"\"\"\n",
        "  PATH A (Production Model):     {len(feature_cols)} features\n",
        "    - For: External validation, clinical deployment\n",
        "    - Source: Full-data feature selection (n=99)\n",
        "\n",
        "  PATH B (CV-Stable Biomarkers): {len(cv_stable_features)} features\n",
        "    - For: Biomarker reporting in manuscript\n",
        "    - Source: ≥{STABILITY_THRESHOLD*100:.0f}% selection across {total_iterations} CV iterations\n",
        "\n",
        "  OVERLAP ANALYSIS:\n",
        "    - In BOTH paths:   {len(overlap)} features\n",
        "    - Only in Path A:  {len(only_a)} features\n",
        "    - Only in Path B:  {len(only_b)} features\n",
        "\n",
        "  [INTERPRETATION]\n",
        "    Features in BOTH paths are the most robust biomarkers.\n",
        "    They are consistently selected AND appear in the production model.\n",
        "\"\"\")\n",
        "\n",
        "# Save overlap analysis\n",
        "overlap_df = pd.DataFrame({\n",
        "    'Probe_ID': list(overlap),\n",
        "    'In_Production_Model': True,\n",
        "    'In_CV_Stable': True\n",
        "})\n",
        "overlap_path = os.path.join(RESULT_DIR, f\"overlap_robust_biomarkers_{best_name}.csv\")\n",
        "overlap_df.to_csv(overlap_path, index=False)\n",
        "print(f\"  Overlap (most robust) saved: {overlap_path}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 7. Save Comprehensive Results\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 7: Save Results\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Save results to JSON (excluding non-serializable objects)\n",
        "json_results = {}\n",
        "for model_name, res in results.items():\n",
        "    json_results[model_name] = {k: v for k, v in res.items()\n",
        "                                 if k not in ['best_estimator']}\n",
        "\n",
        "json_path = os.path.join(RESULT_DIR, \"repeated_nested_cv_results_v3.json\")\n",
        "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(json_results, f, indent=2, ensure_ascii=False, default=str)\n",
        "print(f\"[INFO] Results saved: {json_path}\")\n",
        "\n",
        "# Save feature stability for each model\n",
        "for model_name in results:\n",
        "    stability_df = pd.DataFrame([\n",
        "        {\"Probe_ID\": k, \"Selection_Count\": v,\n",
        "         \"Selection_Rate\": f\"{v/total_iterations*100:.1f}%\"}\n",
        "        for k, v in results[model_name]['feature_stability']['feature_frequency'].items()\n",
        "    ]).sort_values(\"Selection_Count\", ascending=False)\n",
        "\n",
        "    stability_path = os.path.join(RESULT_DIR, f\"feature_stability_{model_name}.csv\")\n",
        "    stability_df.to_csv(stability_path, index=False)\n",
        "\n",
        "print(f\"[INFO] Feature stability tables saved for all models\")\n",
        "\n",
        "# ==============================================================================\n",
        "# FINAL SUMMARY\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"✓ NESTED CV (v3.0) COMPLETE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"\"\"\n",
        "[SUMMARY]\n",
        "  Cross-Validation:    {N_OUTER_SPLITS}-fold × {N_REPEATS} repeats = {total_iterations} iterations\n",
        "  Feature Selection:   |log2FC| > {LOG2FC_THRESHOLD}, FDR q < {FDR_THRESHOLD}\n",
        "  Consensus:           2-of-3 methods (unchanged)\n",
        "  Stability Threshold: {STABILITY_THRESHOLD*100:.0f}% (lowered from 80%)\n",
        "  Best Model:          {best_name}\n",
        "\n",
        "[PERFORMANCE]\n",
        "  OOF AUC:             {results[best_name]['oof_auc']:.4f} ± {results[best_name]['oof_auc_std']:.4f}\n",
        "  95% CI:              [{results[best_name]['oof_auc_ci'][0]:.4f}, {results[best_name]['oof_auc_ci'][1]:.4f}]\n",
        "  Balanced Accuracy:   {results[best_name]['oof_balanced_accuracy']:.4f}\n",
        "  Sensitivity:         {results[best_name]['recall_sensitivity']:.4f}\n",
        "  Specificity:         {results[best_name]['specificity']:.4f}\n",
        "  Kuncheva Index:      {results[best_name].get('kuncheva_stability_index', 'N/A')}\n",
        "\n",
        "[DUAL-PATH FEATURES]\n",
        "  Path A (Production): {len(feature_cols)} features (for external validation)\n",
        "  Path B (Biomarkers): {len(cv_stable_features)} features (for manuscript)\n",
        "  Overlap (Robust):    {len(overlap)} features\n",
        "\n",
        "[KEY CHANGES IN v3.0]\n",
        "  ✓ SMOTE removed → class_weight='balanced' used instead\n",
        "  ✓ Stability threshold: 80% → {STABILITY_THRESHOLD*100:.0f}%\n",
        "  ✓ Kuncheva stability index calculated\n",
        "  ✓ Dual-path feature selection implemented\n",
        "  ✓ Overlap analysis for robust biomarkers\n",
        "\n",
        "[READY FOR]\n",
        "  - Cell 2-2: Balanced Subsampling comparison (sensitivity analysis)\n",
        "  - Cell 3: SHAP analysis\n",
        "  - Cell 4: External validation (use Path A features)\n",
        "  - Manuscript: Report Path B features as biomarkers\n",
        "\"\"\")\n",
        "\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXXF4Wc_VToE"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Cell 2-2: Balanced Subsampling Feature Selection (Ma et al. Strategy)\n",
        "# =============================================================================\n",
        "\"\"\"\n",
        "Purpose:\n",
        "    Implement Ma et al. (2021) balanced subsampling strategy for robust\n",
        "    feature selection in severely imbalanced datasets.\n",
        "\n",
        "Reference:\n",
        "    Ma J, Wang P, et al. (2021) \"Bioinformatic analysis reveals an exosomal\n",
        "    miRNA-mRNA network in colorectal cancer.\" BMC Med Genomics 14(1):60.\n",
        "    DOI: 10.1186/s12920-021-00905-2\n",
        "\n",
        "Key Differences from Cell 2 (SMOTE-based):\n",
        "    1. No synthetic data generation (100% real samples)\n",
        "    2. Balanced subsets: n_minority samples from each class per iteration\n",
        "    3. 1000 iterations for robust frequency estimation\n",
        "    4. Double threshold: frequency (>50%) AND consensus (≥2/3 methods)\n",
        "\n",
        "VERSION 2.0 MODIFICATIONS:\n",
        "    - Added FDR correction (Benjamini-Hochberg) within each iteration\n",
        "    - Fixed Probe ID type consistency (string normalization)\n",
        "    - Clarified consensus definitions (within-method vs cross-method)\n",
        "    - Fixed Jaccard similarity calculation\n",
        "\n",
        "Prerequisites:\n",
        "    - Cell 0-2 must be executed first\n",
        "    - Required variables: expr_log_full, labels_full, sample_ids_full, mapping_df\n",
        "\n",
        "Output:\n",
        "    - balanced_stable_features: Features selected by balanced subsampling\n",
        "    - comparison_results: Comparison with Cell 2 SMOTE-based results\n",
        "    - consensus_features: Intersection of both methods (high confidence)\n",
        "\n",
        "Author: Jungho Sohn\n",
        "Date: 2025-12-23\n",
        "Version: 2.0 (FDR correction added)\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "from collections import defaultdict, Counter\n",
        "from scipy.stats import mannwhitneyu\n",
        "\n",
        "from sklearn.linear_model import LassoCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Cell 2-2: BALANCED SUBSAMPLING FEATURE SELECTION (v2.0)\")\n",
        "print(\"         (Ma et al. 2021 Strategy - No Synthetic Data)\")\n",
        "print(\"         [WITH FDR CORRECTION]\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 0: Verify Data Availability (must be first!)\n",
        "# =============================================================================\n",
        "\n",
        "# Check required variables exist using globals()\n",
        "required_vars = ['expr_log_full', 'labels_full', 'sample_ids_full', 'results', 'best_name', 'SEED']\n",
        "missing_vars = [v for v in required_vars if v not in globals()]\n",
        "\n",
        "if missing_vars:\n",
        "    raise RuntimeError(f\"Missing required variables: {missing_vars}\\n\"\n",
        "                       f\"Please run Cell 0-2 first!\")\n",
        "\n",
        "print(f\"[INFO] ✓ All required variables verified from Cell 0-2\")\n",
        "\n",
        "# =============================================================================\n",
        "# CONFIGURATION\n",
        "# =============================================================================\n",
        "\n",
        "# Ensure RESULT_DIR is available (from Cell 2)\n",
        "if 'RESULT_DIR' not in globals():\n",
        "    RESULT_DIR = base_save_path if 'base_save_path' in globals() else './geoexosome_results'\n",
        "    os.makedirs(RESULT_DIR, exist_ok=True)\n",
        "\n",
        "# Subsampling parameters (Ma et al. strategy)\n",
        "N_ITERATIONS = 1000          # Number of balanced subsampling iterations\n",
        "FREQ_THRESHOLD = 0.5         # Minimum selection frequency (>50%)\n",
        "CONSENSUS_THRESHOLD = 2      # Minimum methods agreeing (≥2 of 3)\n",
        "\n",
        "# Feature selection thresholds (same as Cell 2 for fair comparison)\n",
        "BS_LOG2FC_THRESHOLD = LOG2FC_THRESHOLD if 'LOG2FC_THRESHOLD' in globals() else 1.0\n",
        "BS_FDR_THRESHOLD = FDR_THRESHOLD if 'FDR_THRESHOLD' in globals() else 0.05\n",
        "\n",
        "# Get N_OUTER_SPLITS and N_REPEATS from Cell 2 (for comparison threshold calculation)\n",
        "N_OUTER_SPLITS_REF = N_OUTER_SPLITS if 'N_OUTER_SPLITS' in globals() else 5\n",
        "N_REPEATS_REF = N_REPEATS if 'N_REPEATS' in globals() else 10\n",
        "\n",
        "print(f\"\"\"\n",
        "CONFIGURATION:\n",
        "  Iterations:              {N_ITERATIONS}\n",
        "  Frequency threshold:     >{FREQ_THRESHOLD*100:.0f}%\n",
        "  Consensus threshold:     ≥{CONSENSUS_THRESHOLD}/3 methods\n",
        "\n",
        "  Biological filter:\n",
        "    |log2FC| threshold:    > {BS_LOG2FC_THRESHOLD}\n",
        "    FDR q-value:           < {BS_FDR_THRESHOLD}\n",
        "    FDR correction:        Benjamini-Hochberg (per iteration)  ← NEW\n",
        "\n",
        "  Key difference from Cell 2:\n",
        "    - NO SMOTE (synthetic oversampling)\n",
        "    - Each iteration uses {sum(labels_full==0)} HC + {sum(labels_full==0)} CRC (balanced)\n",
        "    - Total real samples per iteration: {sum(labels_full==0) * 2}\n",
        "\"\"\")\n",
        "\n",
        "# =============================================================================\n",
        "# UTILITY FUNCTIONS\n",
        "# =============================================================================\n",
        "\n",
        "def normalize_probe_id(probe_id):\n",
        "    \"\"\"\n",
        "    Convert probe ID to consistent string format.\n",
        "\n",
        "    This ensures compatibility between Cell 2 and Cell 2-2 results,\n",
        "    regardless of whether probe IDs are stored as integers or strings.\n",
        "    \"\"\"\n",
        "    if isinstance(probe_id, (int, np.integer)):\n",
        "        return str(probe_id)\n",
        "    return str(probe_id).strip()\n",
        "\n",
        "\n",
        "def benjamini_hochberg_correction(pvalues):\n",
        "    \"\"\"\n",
        "    Apply Benjamini-Hochberg FDR correction to p-values.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    pvalues : array-like\n",
        "        Raw p-values from statistical tests\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    qvalues : np.ndarray\n",
        "        FDR-adjusted q-values (same length as input)\n",
        "\n",
        "    Reference\n",
        "    ---------\n",
        "    Benjamini Y, Hochberg Y (1995). Controlling the false discovery rate:\n",
        "    a practical and powerful approach to multiple testing.\n",
        "    J R Stat Soc B 57:289-300.\n",
        "    \"\"\"\n",
        "    pvalues = np.asarray(pvalues)\n",
        "    n_tests = len(pvalues)\n",
        "\n",
        "    if n_tests == 0:\n",
        "        return np.array([])\n",
        "\n",
        "    # Get sort order\n",
        "    sorted_idx = np.argsort(pvalues)\n",
        "\n",
        "    # Calculate adjusted p-values\n",
        "    qvalues = np.zeros(n_tests)\n",
        "    for i, idx in enumerate(sorted_idx):\n",
        "        rank = i + 1\n",
        "        qvalues[idx] = min(1.0, pvalues[idx] * n_tests / rank)\n",
        "\n",
        "    # Ensure monotonicity (cumulative minimum from the end)\n",
        "    # This guarantees q-values are non-decreasing when sorted by p-value\n",
        "    for i in range(n_tests - 2, -1, -1):\n",
        "        if qvalues[sorted_idx[i]] > qvalues[sorted_idx[i + 1]]:\n",
        "            qvalues[sorted_idx[i]] = qvalues[sorted_idx[i + 1]]\n",
        "\n",
        "    return qvalues\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 1: Data Summary and Class Distribution\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 1: Data Summary\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"  - Expression matrix: {expr_log_full.shape}\")\n",
        "print(f\"  - Labels: {len(labels_full)} samples ({sum(labels_full==0)} HC, {sum(labels_full==1)} CRC)\")\n",
        "print(f\"  - Class ratio: 1:{sum(labels_full==1)/sum(labels_full==0):.1f}\")\n",
        "\n",
        "# Extract class indices\n",
        "hc_indices = np.where(labels_full == 0)[0]\n",
        "crc_indices = np.where(labels_full == 1)[0]\n",
        "\n",
        "n_hc = len(hc_indices)\n",
        "n_crc = len(crc_indices)\n",
        "\n",
        "print(f\"\\n[INFO] Balanced subsampling configuration:\")\n",
        "print(f\"  - Each iteration: {n_hc} HC + {n_hc} CRC = {2*n_hc} samples\")\n",
        "print(f\"  - CRC samples used per iteration: {n_hc}/{n_crc} ({100*n_hc/n_crc:.1f}%)\")\n",
        "print(f\"  - Total iterations: {N_ITERATIONS}\")\n",
        "print(f\"  - Expected CRC coverage: Each sample appears ~{N_ITERATIONS*n_hc/n_crc:.0f} times\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 2: Balanced Subsampling Feature Selection (WITH FDR CORRECTION)\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 2: Execute Balanced Subsampling ({} iterations)\".format(N_ITERATIONS))\n",
        "print(\"        [WITH Benjamini-Hochberg FDR Correction per iteration]\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Initialize frequency counters for each method\n",
        "freq_biological = defaultdict(int)\n",
        "freq_lasso = defaultdict(int)\n",
        "freq_svm_rfe = defaultdict(int)\n",
        "freq_rf = defaultdict(int)\n",
        "\n",
        "# Track iteration statistics\n",
        "iteration_stats = []\n",
        "valid_iterations = 0\n",
        "\n",
        "# Convert expression data to numpy for faster indexing\n",
        "expr_values = expr_log_full.values\n",
        "probe_names = expr_log_full.columns.tolist()\n",
        "\n",
        "np.random.seed(SEED)\n",
        "\n",
        "print(f\"\\n[INFO] Starting {N_ITERATIONS} balanced subsampling iterations...\")\n",
        "print(f\"[INFO] Progress will be shown every 100 iterations\\n\")\n",
        "\n",
        "for iteration in range(N_ITERATIONS):\n",
        "\n",
        "    # Progress indicator\n",
        "    if (iteration + 1) % 100 == 0:\n",
        "        avg_bio = np.mean([s['n_bio'] for s in iteration_stats if s.get('valid', False)]) if iteration_stats else 0\n",
        "        print(f\"  Iteration {iteration + 1}/{N_ITERATIONS} \"\n",
        "              f\"(valid: {valid_iterations}, bio features avg: {avg_bio:.1f})\")\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # STEP 2a: Balanced Random Subsampling (Ma et al. strategy)\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Sample n_hc CRC samples to match HC count (NO replacement within iteration)\n",
        "    crc_subset_idx = np.random.choice(crc_indices, size=n_hc, replace=False)\n",
        "\n",
        "    # Combine with all HC samples\n",
        "    subset_idx = np.concatenate([hc_indices, crc_subset_idx])\n",
        "\n",
        "    # Extract balanced subset\n",
        "    X_sub = expr_values[subset_idx, :]\n",
        "    y_sub = labels_full[subset_idx]\n",
        "\n",
        "    # Verify balance\n",
        "    assert sum(y_sub == 0) == sum(y_sub == 1) == n_hc, \"Imbalanced subset!\"\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # STEP 2b: Biological Filtering WITH FDR CORRECTION (MODIFIED v2.0)\n",
        "    # -------------------------------------------------------------------------\n",
        "\n",
        "    # First pass: collect all statistics for FDR correction\n",
        "    all_log2fc = []\n",
        "    all_pvals = []\n",
        "\n",
        "    for probe_idx in range(len(probe_names)):\n",
        "        hc_vals = X_sub[y_sub == 0, probe_idx]\n",
        "        crc_vals = X_sub[y_sub == 1, probe_idx]\n",
        "\n",
        "        # Log2 fold change (data is already log2 transformed)\n",
        "        mean_hc = np.mean(hc_vals)\n",
        "        mean_crc = np.mean(crc_vals)\n",
        "        log2fc = mean_crc - mean_hc\n",
        "\n",
        "        # Mann-Whitney U test (non-parametric)\n",
        "        try:\n",
        "            _, pval = mannwhitneyu(hc_vals, crc_vals, alternative='two-sided')\n",
        "        except:\n",
        "            pval = 1.0\n",
        "\n",
        "        all_log2fc.append(log2fc)\n",
        "        all_pvals.append(pval)\n",
        "\n",
        "    # ─────────────────────────────────────────────────────────────────────────\n",
        "    # FDR CORRECTION (Benjamini-Hochberg) - CRITICAL ADDITION\n",
        "    # ─────────────────────────────────────────────────────────────────────────\n",
        "    all_qvals = benjamini_hochberg_correction(all_pvals)\n",
        "\n",
        "    # Apply thresholds with FDR-corrected q-values\n",
        "    bio_selected_idx = []\n",
        "    for probe_idx, probe_name in enumerate(probe_names):\n",
        "        log2fc = all_log2fc[probe_idx]\n",
        "        qval = all_qvals[probe_idx]  # FDR-adjusted q-value\n",
        "\n",
        "        # Criteria: |log2FC| > threshold AND FDR q-value < threshold\n",
        "        if abs(log2fc) > BS_LOG2FC_THRESHOLD and qval < BS_FDR_THRESHOLD:\n",
        "            bio_selected_idx.append(probe_idx)\n",
        "            freq_biological[probe_name] += 1\n",
        "\n",
        "    # Skip iteration if too few features pass filter\n",
        "    if len(bio_selected_idx) < 10:\n",
        "        iteration_stats.append({\n",
        "            'iteration': iteration + 1,\n",
        "            'n_bio': len(bio_selected_idx),\n",
        "            'valid': False\n",
        "        })\n",
        "        continue\n",
        "\n",
        "    valid_iterations += 1\n",
        "\n",
        "    # Extract filtered features\n",
        "    X_bio = X_sub[:, bio_selected_idx]\n",
        "    bio_probe_names = [probe_names[i] for i in bio_selected_idx]\n",
        "\n",
        "    # Standardize for ML methods\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X_bio)\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # STEP 2c: Multi-Method Feature Selection\n",
        "    # -------------------------------------------------------------------------\n",
        "\n",
        "    # Method 1: LASSO\n",
        "    try:\n",
        "        lasso = LassoCV(cv=3, random_state=SEED, max_iter=5000, n_jobs=-1)\n",
        "        lasso.fit(X_scaled, y_sub)\n",
        "        lasso_selected = [bio_probe_names[i] for i, c in enumerate(lasso.coef_) if abs(c) > 1e-6]\n",
        "\n",
        "        for feat in lasso_selected:\n",
        "            freq_lasso[feat] += 1\n",
        "    except:\n",
        "        lasso_selected = []\n",
        "\n",
        "    # Method 2: SVM-RFE (with adjusted n_features_to_select)\n",
        "    try:\n",
        "        # More stable feature count selection\n",
        "        n_select = min(50, max(20, int(0.5 * len(bio_selected_idx))))\n",
        "        svc = SVC(kernel='linear', class_weight='balanced', random_state=SEED)\n",
        "        rfe = RFE(svc, n_features_to_select=n_select, step=0.2)\n",
        "        rfe.fit(X_scaled, y_sub)\n",
        "        svm_selected = [bio_probe_names[i] for i in np.where(rfe.support_)[0]]\n",
        "\n",
        "        for feat in svm_selected:\n",
        "            freq_svm_rfe[feat] += 1\n",
        "    except:\n",
        "        svm_selected = []\n",
        "\n",
        "    # Method 3: Random Forest\n",
        "    try:\n",
        "        rf = RandomForestClassifier(\n",
        "            n_estimators=100,\n",
        "            random_state=SEED,\n",
        "            n_jobs=-1,\n",
        "            class_weight='balanced'\n",
        "        )\n",
        "        rf.fit(X_bio, y_sub)  # RF doesn't need scaling\n",
        "        importances = rf.feature_importances_\n",
        "        threshold = np.percentile(importances, 75)  # Top 25%\n",
        "        rf_selected = [bio_probe_names[i] for i, imp in enumerate(importances) if imp >= threshold]\n",
        "\n",
        "        for feat in rf_selected:\n",
        "            freq_rf[feat] += 1\n",
        "    except:\n",
        "        rf_selected = []\n",
        "\n",
        "    # Record iteration statistics\n",
        "    iteration_stats.append({\n",
        "        'iteration': iteration + 1,\n",
        "        'n_bio': len(bio_selected_idx),\n",
        "        'n_lasso': len(lasso_selected),\n",
        "        'n_svm': len(svm_selected),\n",
        "        'n_rf': len(rf_selected),\n",
        "        'valid': True\n",
        "    })\n",
        "\n",
        "print(f\"\\n[INFO] ✓ Balanced subsampling complete\")\n",
        "print(f\"  Total iterations: {N_ITERATIONS}\")\n",
        "print(f\"  Valid iterations: {valid_iterations} ({100*valid_iterations/N_ITERATIONS:.1f}%)\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 3: Apply Double Threshold (Frequency + Consensus)\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 3: Apply Double Threshold Selection\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Calculate minimum count for frequency threshold\n",
        "min_count = int(valid_iterations * FREQ_THRESHOLD)\n",
        "print(f\"\\n[INFO] Frequency threshold: >{FREQ_THRESHOLD*100:.0f}% = >{min_count}/{valid_iterations} iterations\")\n",
        "\n",
        "# Combine all features\n",
        "all_features = set(freq_lasso.keys()) | set(freq_svm_rfe.keys()) | set(freq_rf.keys())\n",
        "print(f\"[INFO] Total unique features selected across all iterations: {len(all_features)}\")\n",
        "\n",
        "# Calculate selection statistics for each feature\n",
        "feature_stats = []\n",
        "\n",
        "for feat in all_features:\n",
        "    n_bio = freq_biological.get(feat, 0)\n",
        "    n_lasso = freq_lasso.get(feat, 0)\n",
        "    n_svm = freq_svm_rfe.get(feat, 0)\n",
        "    n_rf = freq_rf.get(feat, 0)\n",
        "\n",
        "    # Count methods where feature passed frequency threshold\n",
        "    methods_passed = sum([\n",
        "        n_lasso >= min_count,\n",
        "        n_svm >= min_count,\n",
        "        n_rf >= min_count\n",
        "    ])\n",
        "\n",
        "    # Overall frequency (max across methods)\n",
        "    max_freq = max(n_lasso, n_svm, n_rf)\n",
        "    overall_freq = max_freq / valid_iterations if valid_iterations > 0 else 0\n",
        "\n",
        "    feature_stats.append({\n",
        "        'Probe_ID': feat,\n",
        "        'freq_bio': n_bio / valid_iterations if valid_iterations > 0 else 0,\n",
        "        'freq_lasso': n_lasso / valid_iterations if valid_iterations > 0 else 0,\n",
        "        'freq_svm': n_svm / valid_iterations if valid_iterations > 0 else 0,\n",
        "        'freq_rf': n_rf / valid_iterations if valid_iterations > 0 else 0,\n",
        "        'n_lasso': n_lasso,\n",
        "        'n_svm': n_svm,\n",
        "        'n_rf': n_rf,\n",
        "        'methods_passed': methods_passed,\n",
        "        'overall_freq': overall_freq,\n",
        "        'max_count': max_freq\n",
        "    })\n",
        "\n",
        "df_feature_stats = pd.DataFrame(feature_stats)\n",
        "\n",
        "# Apply double threshold: frequency AND consensus\n",
        "df_stable = df_feature_stats[\n",
        "    (df_feature_stats['overall_freq'] >= FREQ_THRESHOLD) &\n",
        "    (df_feature_stats['methods_passed'] >= CONSENSUS_THRESHOLD)\n",
        "].sort_values('overall_freq', ascending=False).copy()\n",
        "\n",
        "# Also get features passing only frequency threshold (for comparison)\n",
        "df_freq_only = df_feature_stats[\n",
        "    df_feature_stats['overall_freq'] >= FREQ_THRESHOLD\n",
        "].sort_values('overall_freq', ascending=False).copy()\n",
        "\n",
        "print(f\"\\n[SELECTION RESULTS]\")\n",
        "print(f\"  Features passing frequency threshold (>{FREQ_THRESHOLD*100:.0f}%): {len(df_freq_only)}\")\n",
        "print(f\"  Features passing consensus (≥{CONSENSUS_THRESHOLD}/3 methods): \"\n",
        "      f\"{len(df_feature_stats[df_feature_stats['methods_passed'] >= CONSENSUS_THRESHOLD])}\")\n",
        "print(f\"  Features passing BOTH (final stable): {len(df_stable)}\")\n",
        "\n",
        "# Normalize probe IDs to strings for consistency\n",
        "balanced_stable_features = [normalize_probe_id(p) for p in df_stable['Probe_ID'].tolist()]\n",
        "\n",
        "print(f\"\\n[INFO] ✓ Balanced Subsampling selected {len(balanced_stable_features)} stable features\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 4: Compare with Cell 2 Results (SMOTE-based)\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 4: Compare with Cell 2 (SMOTE-based) Results\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Get Cell 2 stable features\n",
        "cell2_feature_freq = results[best_name][\"feature_stability\"][\"feature_frequency\"]\n",
        "\n",
        "# Calculate threshold for Cell 2 stability (80% of total iterations)\n",
        "stability_threshold_80 = int(0.8 * N_OUTER_SPLITS_REF * N_REPEATS_REF)\n",
        "stability_threshold_50 = int(0.5 * N_OUTER_SPLITS_REF * N_REPEATS_REF)\n",
        "\n",
        "cell2_stable_features = []\n",
        "for k, v in cell2_feature_freq.items():\n",
        "    probe_str = normalize_probe_id(k)\n",
        "    if v >= stability_threshold_80:\n",
        "        cell2_stable_features.append(probe_str)\n",
        "\n",
        "# Fallback to 50% threshold if too few features\n",
        "if len(cell2_stable_features) < 5:\n",
        "    print(f\"[WARNING] Only {len(cell2_stable_features)} features at 80% threshold\")\n",
        "    print(f\"[WARNING] Using 50% threshold instead ({stability_threshold_50} iterations)\")\n",
        "    cell2_stable_features = []\n",
        "    for k, v in cell2_feature_freq.items():\n",
        "        probe_str = normalize_probe_id(k)\n",
        "        if v >= stability_threshold_50:\n",
        "            cell2_stable_features.append(probe_str)\n",
        "\n",
        "print(f\"\\n[COMPARISON]\")\n",
        "print(f\"  Cell 2 (SMOTE-based) stable features: {len(cell2_stable_features)}\")\n",
        "print(f\"  Cell 2-2 (Balanced Subsampling) stable features: {len(balanced_stable_features)}\")\n",
        "\n",
        "# Calculate overlap with normalized IDs\n",
        "cell2_set = set(cell2_stable_features)\n",
        "bs_set = set(balanced_stable_features)\n",
        "\n",
        "overlap = cell2_set & bs_set\n",
        "only_cell2 = cell2_set - bs_set\n",
        "only_bs = bs_set - cell2_set\n",
        "\n",
        "print(f\"\\n  Overlap (both methods): {len(overlap)} features\")\n",
        "print(f\"  Only in Cell 2 (SMOTE): {len(only_cell2)} features\")\n",
        "print(f\"  Only in Cell 2-2 (Balanced): {len(only_bs)} features\")\n",
        "\n",
        "# Safe Jaccard calculation (FIXED)\n",
        "jaccard = None\n",
        "if len(cell2_set) > 0 and len(bs_set) > 0:\n",
        "    union_size = len(cell2_set | bs_set)\n",
        "    if union_size > 0:\n",
        "        jaccard = len(overlap) / union_size\n",
        "        print(f\"  Jaccard similarity: {jaccard:.3f}\")\n",
        "else:\n",
        "    print(f\"  Jaccard similarity: N/A (empty set)\")\n",
        "\n",
        "# High-confidence consensus features (selected by BOTH methods)\n",
        "consensus_features = list(overlap)\n",
        "print(f\"\\n[HIGH-CONFIDENCE CONSENSUS FEATURES]\")\n",
        "print(f\"  Total: {len(consensus_features)} features\")\n",
        "print(f\"  These features are robust to both SMOTE and balanced subsampling\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 5: Display Top Stable Features with miRNA Names\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 5: Top Stable Features from Balanced Subsampling\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Get miRNA mapping function from Cell 1 or create one\n",
        "if 'probe_to_mirna_dict' not in globals():\n",
        "    if 'mapping_df' in globals() and mapping_df is not None:\n",
        "        mirna_col = 'miRNA' if 'miRNA' in mapping_df.columns else 'SPOT_ID'\n",
        "        probe_to_mirna_dict = dict(zip(\n",
        "            mapping_df[\"Probe_ID\"].apply(normalize_probe_id),\n",
        "            mapping_df[mirna_col]\n",
        "        ))\n",
        "    else:\n",
        "        probe_to_mirna_dict = {}\n",
        "\n",
        "def get_mirna_name_safe(probe_id):\n",
        "    \"\"\"Safely retrieve miRNA name from probe ID.\"\"\"\n",
        "    probe_str = normalize_probe_id(probe_id)\n",
        "    mirna = probe_to_mirna_dict.get(probe_str, None)\n",
        "\n",
        "    if mirna is None or pd.isna(mirna) or str(mirna).strip() == \"\":\n",
        "        return f\"Unmapped_{probe_str}\"\n",
        "    return str(mirna).strip()\n",
        "\n",
        "# Add miRNA names to stable features\n",
        "df_stable = df_stable.copy()\n",
        "df_stable['Probe_ID_str'] = df_stable['Probe_ID'].apply(normalize_probe_id)\n",
        "df_stable['miRNA_Name'] = df_stable['Probe_ID_str'].apply(get_mirna_name_safe)\n",
        "df_stable['In_Consensus'] = df_stable['Probe_ID_str'].isin(consensus_features)\n",
        "\n",
        "# Display top features\n",
        "print(f\"\\n{'Rank':<6} {'Probe ID':<12} {'miRNA Name':<25} {'Freq':<8} {'Methods':<10} {'Consensus'}\")\n",
        "print(\"-\" * 85)\n",
        "\n",
        "for rank, (_, row) in enumerate(df_stable.head(20).iterrows(), 1):\n",
        "    consensus_mark = \"★\" if row['In_Consensus'] else \"\"\n",
        "    print(f\"{rank:<6} {row['Probe_ID']:<12} {row['miRNA_Name']:<25} \"\n",
        "          f\"{row['overall_freq']:.1%}   {row['methods_passed']}/3       {consensus_mark}\")\n",
        "\n",
        "# Highlight consensus features\n",
        "print(f\"\\n[CONSENSUS FEATURES (Both SMOTE & Balanced Subsampling)]\")\n",
        "print(\"-\" * 85)\n",
        "\n",
        "consensus_df = df_stable[df_stable['In_Consensus']].copy()\n",
        "if len(consensus_df) > 0:\n",
        "    for _, row in consensus_df.iterrows():\n",
        "        print(f\"  ★ {row['Probe_ID']:<12} {row['miRNA_Name']:<25} freq={row['overall_freq']:.1%}\")\n",
        "else:\n",
        "    print(\"  No overlapping features found between methods.\")\n",
        "    print(\"  This suggests method-dependent feature selection.\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 6: Save Results\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 6: Save Balanced Subsampling Results\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Save full feature statistics\n",
        "stats_path = os.path.join(RESULT_DIR, \"balanced_subsampling_feature_stats.csv\")\n",
        "df_feature_stats.to_csv(stats_path, index=False)\n",
        "print(f\"[INFO] All feature statistics saved: {stats_path}\")\n",
        "\n",
        "# Save stable features\n",
        "stable_path = os.path.join(RESULT_DIR, \"balanced_subsampling_stable_features.csv\")\n",
        "df_stable.to_csv(stable_path, index=False)\n",
        "print(f\"[INFO] Stable features saved: {stable_path}\")\n",
        "\n",
        "# Save comparison results (FIXED Jaccard handling)\n",
        "comparison_results = {\n",
        "    'methodology': {\n",
        "        'cell2': 'SMOTE-based oversampling with nested CV',\n",
        "        'cell2_2': 'Balanced subsampling (Ma et al. 2021)',\n",
        "        'fdr_correction': 'Benjamini-Hochberg per iteration',\n",
        "        'primary_analysis': 'Cell 2-2 (Balanced Subsampling)',\n",
        "        'sensitivity_analysis': 'Cell 2 (SMOTE)'\n",
        "    },\n",
        "    'cell2_smote': {\n",
        "        'n_stable': len(cell2_stable_features),\n",
        "        'features': cell2_stable_features,\n",
        "        'stability_threshold': f'{stability_threshold_80 if len(cell2_stable_features) >= 5 else stability_threshold_50} iterations'\n",
        "    },\n",
        "    'cell2_2_balanced': {\n",
        "        'n_stable': len(balanced_stable_features),\n",
        "        'features': balanced_stable_features,\n",
        "        'n_iterations': N_ITERATIONS,\n",
        "        'valid_iterations': valid_iterations,\n",
        "        'freq_threshold': FREQ_THRESHOLD,\n",
        "        'consensus_threshold': CONSENSUS_THRESHOLD,\n",
        "        'fdr_threshold': BS_FDR_THRESHOLD,\n",
        "        'log2fc_threshold': BS_LOG2FC_THRESHOLD\n",
        "    },\n",
        "    'consensus': {\n",
        "        'n_features': len(consensus_features),\n",
        "        'features': consensus_features,\n",
        "        'jaccard_similarity': float(jaccard) if jaccard is not None else None,\n",
        "        'interpretation': 'Features robust to both synthetic and real-sample approaches'\n",
        "    }\n",
        "}\n",
        "\n",
        "comparison_path = os.path.join(RESULT_DIR, \"feature_selection_comparison.json\")\n",
        "with open(comparison_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(comparison_results, f, indent=2, ensure_ascii=False)\n",
        "print(f\"[INFO] Comparison results saved: {comparison_path}\")\n",
        "\n",
        "# Save iteration statistics\n",
        "iter_stats_df = pd.DataFrame(iteration_stats)\n",
        "iter_stats_path = os.path.join(RESULT_DIR, \"balanced_subsampling_iteration_stats.csv\")\n",
        "iter_stats_df.to_csv(iter_stats_path, index=False)\n",
        "print(f\"[INFO] Iteration statistics saved: {iter_stats_path}\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 7: Prepare Variables for Cell 3 (CLEAR DEFINITIONS)\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 7: Prepare Variables for Cell 3\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# CLEAR CONSENSUS DEFINITIONS\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "print(\"\"\"\n",
        "╔═══════════════════════════════════════════════════════════════════════════════╗\n",
        "║                        CONSENSUS DEFINITIONS                                   ║\n",
        "╠═══════════════════════════════════════════════════════════════════════════════╣\n",
        "║                                                                               ║\n",
        "║  1. WITHIN-METHOD CONSENSUS (Cell 2-2 internal):                              ║\n",
        "║     → Freq > 50% AND ≥2/3 methods (LASSO, SVM-RFE, RF)                        ║\n",
        "║     → Result: balanced_stable_features ({:>3} features)                       ║\n",
        "║                                                                               ║\n",
        "║  2. CROSS-METHOD CONSENSUS (Cell 2 vs Cell 2-2):                              ║\n",
        "║     → Selected by BOTH SMOTE (Cell 2) AND Balanced Subsampling (Cell 2-2)     ║\n",
        "║     → Result: consensus_features ({:>3} features)                             ║\n",
        "║                                                                               ║\n",
        "╠═══════════════════════════════════════════════════════════════════════════════╣\n",
        "║  RECOMMENDED FOR PUBLICATION:                                                 ║\n",
        "║                                                                               ║\n",
        "║  • PRIMARY ANALYSIS:     Cell 2-2 (Balanced Subsampling)                      ║\n",
        "║                          - No synthetic data, Ma et al. validated             ║\n",
        "║                          - Use: balanced_stable_features                      ║\n",
        "║                                                                               ║\n",
        "║  • SENSITIVITY ANALYSIS: Cell 2 (SMOTE)                                       ║\n",
        "║                          - Report in Supplementary Materials                  ║\n",
        "║                          - Shows robustness to methodological choices         ║\n",
        "║                                                                               ║\n",
        "║  • EXPERIMENTAL VALIDATION: Cross-method consensus                            ║\n",
        "║                          - Highest confidence for organoid experiments        ║\n",
        "║                          - Use: consensus_features                            ║\n",
        "╚═══════════════════════════════════════════════════════════════════════════════╝\n",
        "\"\"\".format(len(balanced_stable_features), len(consensus_features)))\n",
        "\n",
        "# Define which features to use downstream\n",
        "# PRIMARY: Balanced subsampling (for main analysis)\n",
        "primary_features = balanced_stable_features.copy()\n",
        "\n",
        "# VALIDATION: Cross-method consensus (for organoid experiments)\n",
        "validation_features = consensus_features.copy() if len(consensus_features) >= 3 else balanced_stable_features.copy()\n",
        "\n",
        "# Legacy variable for Cell 3 compatibility\n",
        "bs_prioritized_features = primary_features\n",
        "\n",
        "print(f\"[INFO] Variables created for Cell 3:\")\n",
        "print(f\"  • primary_features:       {len(primary_features)} features (main analysis)\")\n",
        "print(f\"  • validation_features:    {len(validation_features)} features (organoid validation)\")\n",
        "print(f\"  • bs_prioritized_features: {len(bs_prioritized_features)} features (legacy compatibility)\")\n",
        "print(f\"  • consensus_features:     {len(consensus_features)} features (cross-method)\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# FINAL SUMMARY\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"✓ BALANCED SUBSAMPLING COMPLETE (v2.0 with FDR Correction)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"\"\"\n",
        "[SUMMARY]\n",
        "  Method: Ma et al. (2021) Balanced Subsampling\n",
        "  Iterations: {N_ITERATIONS} ({valid_iterations} valid)\n",
        "  Sample size per iteration: {2 * n_hc} ({n_hc} HC + {n_hc} CRC)\n",
        "  FDR correction: Benjamini-Hochberg (per iteration)  ← NEW\n",
        "\n",
        "[RESULTS]\n",
        "  Balanced Subsampling stable features: {len(balanced_stable_features)}\n",
        "  Cell 2 (SMOTE) stable features: {len(cell2_stable_features)}\n",
        "  Cross-method consensus: {len(consensus_features)}\n",
        "  Jaccard similarity: {f'{jaccard:.3f}' if jaccard is not None else 'N/A'}\n",
        "\n",
        "[INTERPRETATION]\n",
        "\"\"\")\n",
        "\n",
        "if len(consensus_features) >= 5:\n",
        "    print(f\"  ✓ GOOD: {len(consensus_features)} features are robust to both methods\")\n",
        "    print(f\"    → These are HIGH-CONFIDENCE biomarker candidates\")\n",
        "    print(f\"    → Recommended for organoid validation\")\n",
        "elif len(consensus_features) >= 1:\n",
        "    print(f\"  ○ MODERATE: Only {len(consensus_features)} features overlap\")\n",
        "    print(f\"    → Feature selection is somewhat method-dependent\")\n",
        "    print(f\"    → Consider using balanced subsampling results for novelty\")\n",
        "else:\n",
        "    print(f\"  △ DIVERGENT: No overlapping features between methods\")\n",
        "    print(f\"    → SMOTE may be introducing artifacts\")\n",
        "    print(f\"    → Recommend using balanced subsampling results\")\n",
        "    print(f\"    → Be transparent about this in manuscript\")\n",
        "\n",
        "print(f\"\"\"\n",
        "[FILES GENERATED]\n",
        "  - balanced_subsampling_feature_stats.csv\n",
        "  - balanced_subsampling_stable_features.csv\n",
        "  - balanced_subsampling_iteration_stats.csv\n",
        "  - feature_selection_comparison.json\n",
        "\n",
        "[MANUSCRIPT NOTES]\n",
        "  Methods section should include:\n",
        "  - \"FDR correction (Benjamini-Hochberg) was applied within each\n",
        "    balanced subsampling iteration to control for multiple testing\n",
        "    across {len(probe_names):,} probes.\"\n",
        "  - \"Features were considered stable if selected in >{FREQ_THRESHOLD*100:.0f}% of\n",
        "    {N_ITERATIONS} iterations by at least {CONSENSUS_THRESHOLD} of 3 methods.\"\n",
        "\n",
        "[NEXT STEPS]\n",
        "  1. Compare primary_features with Ma et al. (2021) hub miRNAs\n",
        "  2. Proceed to Cell 3 for miRNA → mRNA target prediction\n",
        "  3. Use validation_features for organoid validation priority\n",
        "\"\"\")\n",
        "\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YYTI5JEChSc"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Cell 3: SHAP Analysis and Biomarker Discovery (v3.0 Compatible)\n",
        "# =============================================================================\n",
        "\"\"\"\n",
        "VERSION 3.0 MODIFICATIONS:\n",
        "\n",
        "1. Stability Threshold: 80% → 70% (matches Cell 2 v3.0)\n",
        "2. Adaptive threshold detection improved\n",
        "3. Dual-path awareness (uses CV-stable features from Cell 2)\n",
        "4. Better handling of repeated CV structure\n",
        "\n",
        "Author: Jungho Sohn\n",
        "Date: 2025-12-28\n",
        "Version: 3.0\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Cell 3: SHAP ANALYSIS & BIOMARKER DISCOVERY (v3.0)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 0: Verify Required Variables\n",
        "# =============================================================================\n",
        "print(\"\\n[STEP 0] Verify required variables from Cell 2...\")\n",
        "\n",
        "required_vars = ['results', 'best_name', 'oof_predictions', 'feature_cols',\n",
        "                 'df_expression', 'mapping_df', 'base_save_path', 'SEED']\n",
        "missing = [v for v in required_vars if v not in dir()]\n",
        "\n",
        "if missing:\n",
        "    raise NameError(f\"Missing variables: {missing}\\nPlease run Cell 0-2 first!\")\n",
        "\n",
        "print(f\"[OK] All required variables verified\")\n",
        "print(f\"[INFO] Best model: {best_name}\")\n",
        "print(f\"[INFO] Number of final features: {len(feature_cols)}\")\n",
        "\n",
        "# =============================================================================\n",
        "# ┌─────────────────────────────────────────────────────────────────────────────┐\n",
        "# │ CHANGE 1: Detect CV Structure and Use Consistent Thresholds               │\n",
        "# │ Now matches Cell 2 v3.0 STABILITY_THRESHOLD = 0.70                          │\n",
        "# └─────────────────────────────────────────────────────────────────────────────┘\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 1: DETECT CV STRUCTURE AND ADAPT THRESHOLDS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Get feature frequency from results\n",
        "feature_freq_raw = results[best_name]['feature_stability']['feature_frequency']\n",
        "\n",
        "# Convert string keys to int if necessary\n",
        "if isinstance(list(feature_freq_raw.keys())[0], str):\n",
        "    feature_freq = {int(k): v for k, v in feature_freq_raw.items()}\n",
        "    print(f\"[INFO] Converted {len(feature_freq)} feature keys from str to int\")\n",
        "else:\n",
        "    feature_freq = feature_freq_raw.copy()\n",
        "\n",
        "# Detect CV structure by checking n_iterations or max frequency\n",
        "n_iterations = results[best_name]['feature_stability'].get('n_iterations', None)\n",
        "stability_threshold = results[best_name]['feature_stability'].get('stability_threshold', 0.70)\n",
        "\n",
        "if n_iterations is None:\n",
        "    # Fallback: infer from max frequency\n",
        "    max_freq = max(feature_freq.values()) if feature_freq else 0\n",
        "    if max_freq <= 5:\n",
        "        n_iterations = 5\n",
        "        print(f\"[INFO] Detected 5-fold CV structure (max_freq={max_freq})\")\n",
        "    else:\n",
        "        n_iterations = 50  # Assume repeated CV\n",
        "        print(f\"[INFO] Detected Repeated CV structure (max_freq={max_freq}, assuming 50 iterations)\")\n",
        "else:\n",
        "    print(f\"[INFO] CV iterations from results: {n_iterations}\")\n",
        "\n",
        "# ┌─────────────────────────────────────────────────────────────────────────────┐\n",
        "# │ CHANGE 2: Set adaptive thresholds based on CV structure                    │\n",
        "# │ 70% is now the primary threshold (was 80%)                                  │\n",
        "# └─────────────────────────────────────────────────────────────────────────────┘\n",
        "THRESHOLD_CORE = n_iterations                     # 100% selection\n",
        "THRESHOLD_HIGH = int(n_iterations * stability_threshold)  # ≥70% selection (was 80%)\n",
        "THRESHOLD_MODERATE = int(n_iterations * 0.5)      # ≥50% selection\n",
        "\n",
        "print(f\"\\n[INFO] Adaptive Stability Thresholds (based on {n_iterations} iterations):\")\n",
        "print(f\"  Core stable:        {THRESHOLD_CORE}/{n_iterations} (100%)\")\n",
        "print(f\"  Highly stable:      ≥{THRESHOLD_HIGH}/{n_iterations} (≥{stability_threshold*100:.0f}%)\")\n",
        "print(f\"  Moderately stable:  ≥{THRESHOLD_MODERATE}/{n_iterations} (≥50%)\")\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 2: Extract Stable miRNA Biomarkers from ML Model\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 2: EXTRACT STABLE miRNA BIOMARKERS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Categorize by stability with ADAPTIVE thresholds\n",
        "core_stable = [pid for pid, freq in feature_freq.items() if freq >= THRESHOLD_CORE]\n",
        "highly_stable = [pid for pid, freq in feature_freq.items()\n",
        "                 if THRESHOLD_HIGH <= freq < THRESHOLD_CORE]\n",
        "moderately_stable = [pid for pid, freq in feature_freq.items()\n",
        "                     if THRESHOLD_MODERATE <= freq < THRESHOLD_HIGH]\n",
        "\n",
        "print(f\"\\n[INFO] Feature Stability Summary ({best_name}):\")\n",
        "print(f\"  Core stable (100%, {THRESHOLD_CORE}/{n_iterations}):           {len(core_stable)} features\")\n",
        "print(f\"  Highly stable (≥{stability_threshold*100:.0f}%, ≥{THRESHOLD_HIGH}/{n_iterations}):   {len(highly_stable)} features\")\n",
        "print(f\"  Moderately stable (≥50%, ≥{THRESHOLD_MODERATE}/{n_iterations}): {len(moderately_stable)} features\")\n",
        "\n",
        "# Show actual top features if categories are empty\n",
        "if len(core_stable) == 0 and len(highly_stable) == 0:\n",
        "    print(f\"\\n[WARNING] No features meet ≥{stability_threshold*100:.0f}% threshold!\")\n",
        "    print(f\"[INFO] Top 10 features by selection frequency:\")\n",
        "    sorted_features = sorted(feature_freq.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "    for pid, freq in sorted_features:\n",
        "        pct = freq / n_iterations * 100\n",
        "        print(f\"  Probe {pid}: {freq}/{n_iterations} ({pct:.1f}%)\")\n",
        "\n",
        "    # Use top features that meet minimum threshold\n",
        "    min_threshold_features = [pid for pid, freq in feature_freq.items()\n",
        "                              if freq >= THRESHOLD_MODERATE]\n",
        "\n",
        "    if len(min_threshold_features) > 0:\n",
        "        print(f\"\\n[INFO] Using {len(min_threshold_features)} features with ≥50% selection rate\")\n",
        "        highly_stable = min_threshold_features[:20]  # Top 20 at most\n",
        "    else:\n",
        "        # Fallback: use top N features by frequency\n",
        "        top_n = min(20, len(sorted_features))\n",
        "        highly_stable = [pid for pid, freq in sorted_features[:top_n]]\n",
        "        print(f\"\\n[INFO] Fallback: Using top {top_n} features by frequency\")\n",
        "\n",
        "# Combine core + highly stable for comprehensive analysis\n",
        "prioritized_features = list(set(core_stable + highly_stable))\n",
        "print(f\"\\n[INFO] Total prioritized features for target prediction: {len(prioritized_features)}\")\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 3: Check for CV-Stable Features from Cell 2 v3.0\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 3: CHECK DUAL-PATH FEATURE LISTS (Cell 2 v3.0)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Check if CV-stable biomarkers file exists from Cell 2 v3.0\n",
        "cv_stable_path = os.path.join(base_save_path, f\"cv_stable_biomarkers_{best_name}.csv\")\n",
        "production_path = os.path.join(base_save_path, f\"production_model_features_{best_name}.csv\")\n",
        "overlap_path = os.path.join(base_save_path, f\"overlap_robust_biomarkers_{best_name}.csv\")\n",
        "\n",
        "if os.path.exists(cv_stable_path):\n",
        "    cv_stable_df = pd.read_csv(cv_stable_path)\n",
        "    print(f\"[OK] Found CV-stable biomarkers from Cell 2 v3.0: {len(cv_stable_df)} features\")\n",
        "    print(f\"     File: {cv_stable_path}\")\n",
        "\n",
        "    # Use these as the primary biomarker list\n",
        "    cv_stable_probes = cv_stable_df['Probe_ID'].tolist()\n",
        "    prioritized_features = [int(p) if str(p).isdigit() else p for p in cv_stable_probes]\n",
        "    print(f\"[INFO] Using Cell 2 v3.0 CV-stable features as primary biomarker list\")\n",
        "else:\n",
        "    print(f\"[INFO] CV-stable biomarkers file not found (Cell 2 v3.0 format)\")\n",
        "    print(f\"[INFO] Using features from stability analysis above\")\n",
        "\n",
        "if os.path.exists(overlap_path):\n",
        "    overlap_df = pd.read_csv(overlap_path)\n",
        "    print(f\"[OK] Found overlap (most robust) biomarkers: {len(overlap_df)} features\")\n",
        "else:\n",
        "    print(f\"[INFO] Overlap biomarkers file not found\")\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 4: Verification - Check if probe IDs exist in data\n",
        "# =============================================================================\n",
        "print(f\"\\n[VERIFICATION] Checking probe ID existence:\")\n",
        "\n",
        "if len(prioritized_features) == 0:\n",
        "    print(\"[ERROR] No prioritized features available!\")\n",
        "    print(\"[INFO] Checking feature_freq content:\")\n",
        "    print(f\"  Total features in freq dict: {len(feature_freq)}\")\n",
        "    print(f\"  Max frequency: {max(feature_freq.values()) if feature_freq else 0}\")\n",
        "    raise RuntimeError(\"No features to analyze. Check Cell 2 results.\")\n",
        "\n",
        "sample_probe = prioritized_features[0]\n",
        "print(f\"  Sample probe: {sample_probe} (type: {type(sample_probe)})\")\n",
        "\n",
        "# Check in df_expression columns\n",
        "df_cols = df_expression.columns.tolist()\n",
        "# Handle potential type mismatch\n",
        "if isinstance(df_cols[0], str) and isinstance(sample_probe, int):\n",
        "    sample_probe_check = str(sample_probe)\n",
        "elif isinstance(df_cols[0], int) and isinstance(sample_probe, str):\n",
        "    sample_probe_check = int(sample_probe)\n",
        "else:\n",
        "    sample_probe_check = sample_probe\n",
        "\n",
        "in_expression = sample_probe_check in df_cols or sample_probe in df_cols\n",
        "print(f\"  In df_expression.columns: {in_expression}\")\n",
        "\n",
        "# Check in mapping_df\n",
        "if mapping_df is not None:\n",
        "    mapping_probes = mapping_df['Probe_ID'].tolist()\n",
        "    in_mapping = sample_probe in mapping_probes or int(sample_probe) in mapping_probes\n",
        "    print(f\"  In mapping_df Probe_ID: {in_mapping}\")\n",
        "else:\n",
        "    print(f\"  mapping_df not available\")\n",
        "\n",
        "print(\"[INFO] ✓ Probe ID verification complete\")\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 5: Map Probe IDs to miRNA Names\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 5: PROBE ID → miRNA NAME MAPPING\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "if mapping_df is None:\n",
        "    print(\"[WARNING] mapping_df not available! Using probe IDs as identifiers.\")\n",
        "    probe_to_mirna_dict = {}\n",
        "else:\n",
        "    print(f\"\\n[INFO] mapping_df structure:\")\n",
        "    print(f\"  Shape: {mapping_df.shape}\")\n",
        "    print(f\"  Columns: {mapping_df.columns.tolist()}\")\n",
        "\n",
        "    # Determine miRNA column name\n",
        "    mirna_col_candidates = ['miRNA', 'miRNA_ID', 'miRNA_name', 'NAME']\n",
        "    mirna_col = None\n",
        "    for col in mirna_col_candidates:\n",
        "        if col in mapping_df.columns:\n",
        "            mirna_col = col\n",
        "            break\n",
        "\n",
        "    if mirna_col is None:\n",
        "        mirna_col = mapping_df.columns[1]  # Fallback to second column\n",
        "        print(f\"[WARNING] Using fallback miRNA column: {mirna_col}\")\n",
        "    else:\n",
        "        print(f\"[INFO] Using miRNA column: {mirna_col}\")\n",
        "\n",
        "    # Create probe-to-miRNA lookup dictionary\n",
        "    probe_to_mirna_dict = dict(zip(mapping_df[\"Probe_ID\"], mapping_df[mirna_col]))\n",
        "    print(f\"\\n[INFO] Probe-to-miRNA dictionary created:\")\n",
        "    print(f\"  Total probes: {len(probe_to_mirna_dict)}\")\n",
        "\n",
        "\n",
        "def get_mirna_name(probe_id):\n",
        "    \"\"\"Safely retrieve miRNA name from probe ID.\"\"\"\n",
        "    # Try both int and string versions\n",
        "    mirna = probe_to_mirna_dict.get(probe_id, None)\n",
        "    if mirna is None:\n",
        "        mirna = probe_to_mirna_dict.get(int(probe_id) if isinstance(probe_id, str) else str(probe_id), None)\n",
        "\n",
        "    if mirna is None or pd.isna(mirna) or str(mirna).strip() == \"\":\n",
        "        return f\"Unmapped_Probe_{probe_id}\"\n",
        "\n",
        "    return str(mirna).strip()\n",
        "\n",
        "\n",
        "# Create prioritized miRNA biomarker list\n",
        "mirna_biomarker_data = []\n",
        "\n",
        "for probe_id in prioritized_features:\n",
        "    mirna_name = get_mirna_name(probe_id)\n",
        "    freq = feature_freq.get(probe_id, feature_freq.get(int(probe_id), 0))\n",
        "    pct = freq / n_iterations * 100\n",
        "\n",
        "    if freq >= THRESHOLD_CORE:\n",
        "        stability_cat = f\"Core ({freq}/{n_iterations}, {pct:.0f}%)\"\n",
        "        priority = 1\n",
        "    elif freq >= THRESHOLD_HIGH:\n",
        "        stability_cat = f\"Highly Stable ({freq}/{n_iterations}, {pct:.0f}%)\"\n",
        "        priority = 2\n",
        "    else:\n",
        "        stability_cat = f\"Stable ({freq}/{n_iterations}, {pct:.0f}%)\"\n",
        "        priority = 3\n",
        "\n",
        "    mirna_biomarker_data.append({\n",
        "        'Probe_ID': probe_id,\n",
        "        'miRNA_Name': mirna_name,\n",
        "        'Selection_Frequency': freq,\n",
        "        'Selection_Percentage': pct,\n",
        "        'Stability_Category': stability_cat,\n",
        "        'Priority_Rank': priority\n",
        "    })\n",
        "\n",
        "df_mirna_biomarkers = pd.DataFrame(mirna_biomarker_data)\n",
        "df_mirna_biomarkers = df_mirna_biomarkers.sort_values(\n",
        "    ['Priority_Rank', 'Selection_Frequency'],\n",
        "    ascending=[True, False]\n",
        ")\n",
        "\n",
        "# Display results\n",
        "print(\"\\n[INFO] Prioritized miRNA Biomarkers:\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"{'Probe ID':<12} {'miRNA Name':<30} {'Freq':<8} {'%':<8} {'Category'}\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for _, row in df_mirna_biomarkers.head(20).iterrows():\n",
        "    print(f\"{row['Probe_ID']:<12} {row['miRNA_Name']:<30} \"\n",
        "          f\"{row['Selection_Frequency']:<8} {row['Selection_Percentage']:<8.1f} \"\n",
        "          f\"{row['Stability_Category']}\")\n",
        "\n",
        "if len(df_mirna_biomarkers) > 20:\n",
        "    print(f\"... and {len(df_mirna_biomarkers) - 20} more features\")\n",
        "\n",
        "# Save biomarker list\n",
        "biomarker_path = os.path.join(base_save_path, 'mirna_biomarkers_prioritized.csv')\n",
        "df_mirna_biomarkers.to_csv(biomarker_path, index=False)\n",
        "print(f\"\\n[INFO] Biomarker list saved: {biomarker_path}\")\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 6: Summary Statistics\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 6: BIOMARKER DISCOVERY SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "n_mapped = sum(1 for _, row in df_mirna_biomarkers.iterrows()\n",
        "               if not row['miRNA_Name'].startswith('Unmapped'))\n",
        "n_unmapped = len(df_mirna_biomarkers) - n_mapped\n",
        "\n",
        "# Get Kuncheva index from Cell 2 if available\n",
        "kuncheva = results[best_name].get('kuncheva_stability_index', 'N/A')\n",
        "kuncheva_str = f\"{kuncheva:.4f}\" if isinstance(kuncheva, float) else str(kuncheva)\n",
        "\n",
        "print(f\"\"\"\n",
        "[SUMMARY]\n",
        "  CV Structure:           {n_iterations} iterations (5-fold × {n_iterations//5} repeats)\n",
        "  Stability Threshold:    {stability_threshold*100:.0f}% (per Cell 2 v3.0)\n",
        "  Kuncheva Index:         {kuncheva_str}\n",
        "\n",
        "  Total prioritized:      {len(prioritized_features)} features\n",
        "  Successfully mapped:    {n_mapped} miRNAs\n",
        "  Unmapped probes:        {n_unmapped}\n",
        "\n",
        "  Stability Distribution:\n",
        "    Core (100%):              {len(core_stable)}\n",
        "    Highly stable (≥{stability_threshold*100:.0f}%):   {len(highly_stable)}\n",
        "    Moderately (≥50%):        {len(moderately_stable)}\n",
        "\n",
        "[READY FOR]\n",
        "  - Target mRNA prediction (TargetScan, miRDB)\n",
        "  - Pathway enrichment analysis\n",
        "  - Organoid RNA-seq validation matching\n",
        "\"\"\")\n",
        "\n",
        "# Store for downstream analysis\n",
        "prioritized_mirnas = df_mirna_biomarkers['miRNA_Name'].tolist()\n",
        "prioritized_probes = df_mirna_biomarkers['Probe_ID'].tolist()\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"✓ BIOMARKER EXTRACTION COMPLETE (v3.0)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 6: Summary Statistics for Methods Section\n",
        "# ==============================================================================\n",
        "\"\"\"\n",
        "Generate statistics needed for manuscript Methods and Results sections.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"STEP 6: Summary for Manuscript\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# HOTFIX: Variable name alignment\n",
        "# Original code used 'df_importance' which was never defined\n",
        "# Solution: Use df_mirna_biomarkers and map column names appropriately\n",
        "# -----------------------------------------------------------------------------\n",
        "df_importance = df_mirna_biomarkers.copy()\n",
        "\n",
        "# Ensure 'Rank' column exists (mapped from 'SHAP_Rank')\n",
        "if 'SHAP_Rank' in df_importance.columns:\n",
        "    df_importance['Rank'] = df_importance['SHAP_Rank'].fillna(999).astype(int)\n",
        "else:\n",
        "    df_importance['Rank'] = 999  # Default if SHAP analysis failed\n",
        "\n",
        "# Count features by stability in top 20\n",
        "top20 = df_importance.head(20)\n",
        "n_core_in_top20 = sum(top20[\"Selection_Frequency\"] == 5)\n",
        "n_high_in_top20 = sum(top20[\"Selection_Frequency\"] == 4)\n",
        "\n",
        "print(\"\\n[MANUSCRIPT STATISTICS]\")\n",
        "print(f\"  Total features in final model: {len(feature_cols)}\")\n",
        "print(f\"  Core stable features (5/5):    {len(core_stable)}\")\n",
        "print(f\"  Highly stable features (4/5):  {len(highly_stable)}\")\n",
        "print(f\"  Core stable in top 20 SHAP:    {n_core_in_top20}\")\n",
        "print(f\"  Highly stable in top 20 SHAP:  {n_high_in_top20}\")\n",
        "\n",
        "# List core stable features for biological interpretation\n",
        "print(\"\\n[CORE BIOMARKER CANDIDATES - For Literature Review]\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"{'miRNA Name':<30} {'Probe ID':<12} {'SHAP Rank':<12} {'Status'}\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "if len(core_stable) > 0:\n",
        "    for probe_id in core_stable:\n",
        "        mirna_name = get_mirna_name(probe_id) if mapping_df is not None else str(probe_id)\n",
        "\n",
        "        # Check if this probe is in the final model\n",
        "        matching_rows = df_importance[df_importance[\"Probe_ID\"] == probe_id]\n",
        "\n",
        "        if len(matching_rows) > 0:\n",
        "            # Feature is in final model\n",
        "            shap_rank = matching_rows[\"Rank\"].values[0]\n",
        "            status = \"✓ In model\"\n",
        "            print(f\"{mirna_name:<30} {probe_id:<12} #{shap_rank:<11} {status}\")\n",
        "        else:\n",
        "            # Feature NOT in final model\n",
        "            status = \"⚠ Not selected\"\n",
        "            print(f\"{mirna_name:<30} {probe_id:<12} {'N/A':<12} {status}\")\n",
        "\n",
        "    # Summary statistics\n",
        "    core_in_final = sum(1 for pid in core_stable\n",
        "                        if len(df_importance[df_importance[\"Probe_ID\"] == pid]) > 0)\n",
        "\n",
        "    print(\"\\n\" + \"-\" * 80)\n",
        "    print(f\"[SUMMARY]\")\n",
        "    print(f\"  Core stable in final model:    {core_in_final}/{len(core_stable)}\")\n",
        "    print(f\"  Core stable NOT in final:      {len(core_stable) - core_in_final}/{len(core_stable)}\")\n",
        "\n",
        "    if core_in_final < len(core_stable):\n",
        "        print(f\"\\n[INTERPRETATION]\")\n",
        "        print(f\"  Some CV-stable features were not selected in the final model.\")\n",
        "        print(f\"  This occurs when:\")\n",
        "        print(f\"    - Fold-specific patterns don't generalize to full dataset\")\n",
        "        print(f\"    - Feature selection is stochastic (different splits)\")\n",
        "        print(f\"    - Final model uses slightly different feature set\")\n",
        "        print(f\"  Recommendation: Focus on features present in BOTH CV and final model\")\n",
        "\n",
        "else:\n",
        "    print(\"  No features selected in all 5 folds.\")\n",
        "    print(\"  Consider using highly stable (4/5) features for interpretation.\")\n",
        "\n",
        "# Optional: List highly stable features that ARE in final model\n",
        "print(\"\\n[HIGHLY STABLE FEATURES IN FINAL MODEL (4/5 folds)]\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "high_in_final = []\n",
        "for probe_id in highly_stable:\n",
        "    if len(df_importance[df_importance[\"Probe_ID\"] == probe_id]) > 0:\n",
        "        high_in_final.append(probe_id)\n",
        "\n",
        "if len(high_in_final) > 0:\n",
        "    print(f\"Found {len(high_in_final)} highly stable features in final model\")\n",
        "    print(\"\\nTop 10 by SHAP importance:\")\n",
        "    print(f\"{'miRNA Name':<30} {'Probe ID':<12} {'SHAP Rank'}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # Sort by SHAP rank\n",
        "    high_with_rank = []\n",
        "    for probe_id in high_in_final:\n",
        "        mirna_name = get_mirna_name(probe_id) if mapping_df is not None else str(probe_id)\n",
        "        shap_rank = df_importance[df_importance[\"Probe_ID\"] == probe_id][\"Rank\"].values[0]\n",
        "        high_with_rank.append((mirna_name, probe_id, shap_rank))\n",
        "\n",
        "    high_with_rank.sort(key=lambda x: x[2])  # Sort by SHAP rank\n",
        "\n",
        "    for mirna_name, probe_id, shap_rank in high_with_rank[:10]:\n",
        "        print(f\"{mirna_name:<30} {probe_id:<12} #{shap_rank}\")\n",
        "else:\n",
        "    print(\"  No highly stable features in final model\")\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 7: Generate Summary Report for Professor\n",
        "# ==============================================================================\n",
        "\"\"\"\n",
        "Create comprehensive summary document ready for organoid validation planning.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 7: GENERATE SUMMARY REPORT\")\n",
        "print(\"=\" * 80)\n",
        "best_model_name = best_name\n",
        "report_path = os.path.join(RESULT_DIR, \"VALIDATION_SUMMARY_REPORT.txt\")\n",
        "\n",
        "with open(report_path, 'w', encoding='utf-8') as f:\n",
        "    f.write(\"=\" * 80 + \"\\n\")\n",
        "    f.write(\"miRNA BIOMARKER DISCOVERY → ORGANOID VALIDATION SUMMARY\\n\")\n",
        "    f.write(\"=\" * 80 + \"\\n\\n\")\n",
        "\n",
        "    f.write(f\"Analysis Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "    f.write(f\"Dataset: GSE39833 (n=99, HC=11, CRC=88)\\n\")\n",
        "    f.write(f\"Best Model: {best_model_name}\\n\")\n",
        "    f.write(f\"OOF AUC: {results[best_model_name]['oof_auc']:.4f}\\n\")\n",
        "    f.write(f\"95% CI: [{results[best_model_name]['oof_auc_ci'][0]:.4f}, \"\n",
        "            f\"{results[best_model_name]['oof_auc_ci'][1]:.4f}]\\n\\n\")\n",
        "\n",
        "    f.write(\"-\" * 80 + \"\\n\")\n",
        "    f.write(\"SECTION 1: STABLE miRNA BIOMARKERS\\n\")\n",
        "    f.write(\"-\" * 80 + \"\\n\\n\")\n",
        "\n",
        "    f.write(f\"Core stable (5/5 folds):       {len(core_stable)} miRNAs\\n\")\n",
        "    f.write(f\"Highly stable (4/5 folds):     {len(highly_stable)} miRNAs\\n\")\n",
        "    f.write(f\"Total prioritized:             {len(prioritized_features)} miRNAs\\n\\n\")\n",
        "\n",
        "    f.write(\"Top 10 miRNAs for Experimental Validation:\\n\")\n",
        "    f.write(\"-\" * 60 + \"\\n\")\n",
        "    for i, (_, row) in enumerate(df_mirna_biomarkers.head(10).iterrows(), 1):\n",
        "        f.write(f\"{i:2d}. {row['miRNA_Name']:<25} \"\n",
        "                f\"(Stability: {row['Stability_Category']}, \"\n",
        "                f\"SHAP Rank: #{row.get('SHAP_Rank', 'N/A')})\\n\")\n",
        "\n",
        "    f.write(\"\\n\" + \"-\" * 80 + \"\\n\")\n",
        "    f.write(\"SECTION 2: PREDICTED mRNA TARGETS\\n\")\n",
        "    f.write(\"-\" * 80 + \"\\n\\n\")\n",
        "\n",
        "    if TARGETS_AVAILABLE:\n",
        "        f.write(f\"Total miRNA-mRNA interactions: {len(df_mirna_targets)}\\n\")\n",
        "        f.write(f\"Unique target genes:           {df_mirna_targets['Target_Gene'].nunique()}\\n\")\n",
        "        f.write(f\"High-priority candidates:      {len(df_filtered_mrnas)}\\n\")\n",
        "        f.write(f\"Known CRC genes:               {df_filtered_mrnas['Is_Known_CRC_Gene'].sum()}\\n\\n\")\n",
        "\n",
        "        f.write(\"Top 20 mRNA Candidates for Organoid Matching:\\n\")\n",
        "        f.write(\"-\" * 80 + \"\\n\")\n",
        "        f.write(f\"{'Rank':<6} {'Gene':<12} {'#miRNAs':<10} {'CRC Gene':<10} {'Score'}\\n\")\n",
        "        f.write(\"-\" * 80 + \"\\n\")\n",
        "\n",
        "        for _, row in df_filtered_mrnas.head(20).iterrows():\n",
        "            known = \"Yes\" if row['Is_Known_CRC_Gene'] else \"No\"\n",
        "            f.write(f\"{row['Priority_Rank']:<6} {row.name:<12} {row['miRNA_Count']:<10} \"\n",
        "                    f\"{known:<10} {row['Priority_Score']:.3f}\\n\")\n",
        "    else:\n",
        "        f.write(\"Target prediction not completed.\\n\")\n",
        "        f.write(\"Complete Step 4 (manual database search) to generate mRNA candidates.\\n\")\n",
        "\n",
        "    f.write(\"\\n\" + \"-\" * 80 + \"\\n\")\n",
        "    f.write(\"SECTION 3: NEXT STEPS FOR ORGANOID VALIDATION\\n\")\n",
        "    f.write(\"-\" * 80 + \"\\n\\n\")\n",
        "\n",
        "    f.write(\"1. Experimental Design:\\n\")\n",
        "    f.write(\"   - Compare predicted mRNA expression in organoid RNA-seq data\\n\")\n",
        "    f.write(\"   - Focus on top 20 high-priority candidates (priority score ≥0.5)\\n\")\n",
        "    f.write(\"   - Validate known CRC genes first (established biological plausibility)\\n\\n\")\n",
        "\n",
        "    f.write(\"2. Statistical Analysis Plan:\\n\")\n",
        "    f.write(\"   - Differential expression: CRC organoid vs normal tissue\\n\")\n",
        "    f.write(\"   - Correlation: miRNA abundance ↔ target mRNA expression\\n\")\n",
        "    f.write(\"   - Pathway enrichment: Confirm CRC pathway activation\\n\\n\")\n",
        "\n",
        "    f.write(\"3. Expected Outcomes:\\n\")\n",
        "    f.write(\"   - Validation rate: 50-70% of predicted targets (realistic expectation)\\n\")\n",
        "    f.write(\"   - High priority = higher validation probability\\n\")\n",
        "    f.write(\"   - Multi-miRNA targets = stronger signal\\n\\n\")\n",
        "\n",
        "    f.write(\"4. Publication Strategy:\\n\")\n",
        "    f.write(\"   - Figure 1: ML model performance (ROC, confusion matrix)\\n\")\n",
        "    f.write(\"   - Figure 2: SHAP feature importance\\n\")\n",
        "    f.write(\"   - Figure 3: miRNA-mRNA network\\n\")\n",
        "    f.write(\"   - Figure 4: Organoid validation results (your data!)\\n\")\n",
        "    f.write(\"   - Figure 5: Pathway enrichment heatmap\\n\\n\")\n",
        "\n",
        "    f.write(\"-\" * 80 + \"\\n\")\n",
        "    f.write(\"OUTPUT FILES FOR COLLABORATION\\n\")\n",
        "    f.write(\"-\" * 80 + \"\\n\\n\")\n",
        "\n",
        "    f.write(\"1. stable_mirnas_for_validation.csv\\n\")\n",
        "    f.write(\"   → Core miRNA biomarker list (share with wet lab team)\\n\\n\")\n",
        "\n",
        "    f.write(\"2. mirna_target_mrnas_predicted.csv\\n\")\n",
        "    f.write(\"   → All predicted miRNA-mRNA interactions\\n\\n\")\n",
        "\n",
        "    f.write(\"3. crc_relevant_mrnas_filtered.csv\\n\")\n",
        "    f.write(\"   → HIGH PRIORITY: mRNA candidates for organoid matching\\n\")\n",
        "    f.write(\"   → Use this file for RNA-seq comparison!\\n\\n\")\n",
        "\n",
        "    f.write(\"4. mirna_mrna_network.csv\\n\")\n",
        "    f.write(\"   → Network edge list for pathway visualization\\n\\n\")\n",
        "\n",
        "    f.write(\"5. kegg_pathway_enrichment.csv (if generated)\\n\")\n",
        "    f.write(\"   → Enriched pathways for biological interpretation\\n\\n\")\n",
        "\n",
        "    f.write(\"=\" * 80 + \"\\n\")\n",
        "    f.write(\"END OF REPORT\\n\")\n",
        "    f.write(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "print(f\"[INFO] ✓ Summary report saved: {report_path}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# FINAL COMPLETION MESSAGE\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"✓ CELL 3 COMPLETE: BIOMARKER DISCOVERY → TARGET PREDICTION PIPELINE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"\"\"\n",
        "[SUMMARY]\n",
        "\n",
        "miRNA Biomarkers Identified:  {len(prioritized_features)}\n",
        "  - Core stable (5/5 folds):   {len(core_stable)}\n",
        "  - Highly stable (4/5 folds): {len(highly_stable)}\n",
        "\n",
        "mRNA Target Prediction:        {'COMPLETED' if TARGETS_AVAILABLE else 'PENDING'}\n",
        "  - Total interactions:        {len(df_mirna_targets) if TARGETS_AVAILABLE else 'N/A'}\n",
        "  - High-priority candidates:  {len(df_filtered_mrnas) if TARGETS_AVAILABLE else 'N/A'}\n",
        "\n",
        "[CRITICAL FILES FOR PROFESSOR]\n",
        "\n",
        "Ready for Organoid Validation:\n",
        "  ✓ {biomarker_path}\n",
        "  {'✓' if TARGETS_AVAILABLE else '⚠'} {os.path.join(base_save_path, 'crc_relevant_mrnas_filtered.csv')}\n",
        "  ✓ {report_path}\n",
        "\n",
        "{'' if TARGETS_AVAILABLE else '''\n",
        "[ACTION REQUIRED]\n",
        "Complete Step 4 (miRNA target prediction) by:\n",
        "  1. Manually searching each miRNA in TargetScan/miRDB\n",
        "  2. Saving results as: ''' + manual_targets_path + '''\n",
        "  3. Re-running this cell to generate filtered mRNA candidates\n",
        "'''}\n",
        "\n",
        "[NEXT STEPS]\n",
        "\n",
        "1. Review summary report: {os.path.basename(report_path)}\n",
        "2. Share mRNA candidate list with organoid team\n",
        "3. Plan RNA-seq comparison analysis\n",
        "4. Prepare for manuscript writing (you have the data!)\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "print(\"=\" * 80 + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6QB5NjFaezW"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Cell 4: EXTERNAL VALIDATION ON INDEPENDENT DATASETS\n",
        "# =============================================================================\n",
        "\"\"\"\n",
        "Purpose:\n",
        "    Validate the trained model on completely independent cohorts to assess\n",
        "    true generalization performance. This is the MOST CRITICAL step for\n",
        "    publication-ready results.\n",
        "\n",
        "Datasets:\n",
        "    - GSE39814: Independent serum exosome miRNA cohort (Patient samples)\n",
        "    - GSE39832: Cell line data (HT-29) - NOT suitable for patient validation\n",
        "\n",
        "Strategy:\n",
        "    1. Load external datasets from GEO\n",
        "    2. Match probe IDs with training features\n",
        "    3. Apply identical preprocessing (log2 transformation)\n",
        "    4. Predict using the trained model (NO retraining!)\n",
        "    5. Evaluate performance metrics\n",
        "\n",
        "Version: 2.1 - Fixed variable naming conflicts\n",
        "Author: Jungho Sohn\n",
        "Date: 2025-12-22\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import GEOparse\n",
        "import json\n",
        "import warnings\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score,\n",
        "    accuracy_score,\n",
        "    balanced_accuracy_score,\n",
        "    precision_recall_fscore_support,\n",
        "    confusion_matrix,\n",
        "    roc_curve\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"CELL 4: EXTERNAL VALIDATION ON INDEPENDENT DATASETS\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\"\"\n",
        "[CRITICAL] This cell validates the model trained on GSE39833 using\n",
        "           completely independent cohorts.\n",
        "\n",
        "           NO learning or parameter tuning occurs on external data.\n",
        "           This provides an unbiased estimate of true generalization.\n",
        "\"\"\")\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 0: Verify Required Variables from Previous Cells\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 0: Verify Required Variables\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "required_vars = {\n",
        "    'best_full_model': 'Trained classifier pipeline',\n",
        "    'feature_cols': 'List of selected probe IDs',\n",
        "    'best_name': 'Name of best model',\n",
        "    'SEED': 'Random seed for reproducibility',\n",
        "    'base_save_path': 'Output directory path',\n",
        "    'results': 'Nested CV results dictionary',\n",
        "    'oof_predictions': 'Out-of-fold predictions dictionary'\n",
        "}\n",
        "\n",
        "missing_vars = []\n",
        "for var_name, description in required_vars.items():\n",
        "    if var_name not in dir():\n",
        "        missing_vars.append(f\"  - {var_name}: {description}\")\n",
        "    else:\n",
        "        print(f\"[OK] {var_name} found\")\n",
        "\n",
        "if missing_vars:\n",
        "    raise NameError(\n",
        "        f\"[ERROR] Missing required variables from previous cells:\\n\"\n",
        "        + \"\\n\".join(missing_vars) +\n",
        "        \"\\n\\nPlease run Cell 2 (Nested CV) first!\"\n",
        "    )\n",
        "\n",
        "# ┌─────────────────────────────────────────────────────────────────────────────┐\n",
        "# │ CRITICAL FIX: Save Cell 2 results to avoid variable name conflicts         │\n",
        "# └─────────────────────────────────────────────────────────────────────────────┘\n",
        "nested_cv_results = results.copy()  # Preserve Cell 2's results dictionary\n",
        "\n",
        "print(f\"\\n[INFO] Model to validate: {best_name}\")\n",
        "print(f\"[INFO] Number of features: {len(feature_cols)}\")\n",
        "\n",
        "# Get training performance from nested CV\n",
        "train_auc = nested_cv_results[best_name]['oof_auc']\n",
        "train_bal_acc = nested_cv_results[best_name]['oof_balanced_accuracy']\n",
        "train_sensitivity = nested_cv_results[best_name]['recall_sensitivity']\n",
        "train_specificity = nested_cv_results[best_name]['specificity']\n",
        "\n",
        "print(f\"\\n[INFO] Training Performance (from Nested CV):\")\n",
        "print(f\"  - OOF AUC:          {train_auc:.4f}\")\n",
        "print(f\"  - Balanced Accuracy: {train_bal_acc:.4f}\")\n",
        "print(f\"  - Sensitivity:       {train_sensitivity:.4f}\")\n",
        "print(f\"  - Specificity:       {train_specificity:.4f}\")\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 1: Define External Validation Datasets\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 1: Define External Validation Datasets\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "EXTERNAL_DATASETS = {\n",
        "    'GSE39814': {\n",
        "        'description': 'Serum exosome miRNA - Independent patient cohort',\n",
        "        'expected_platform': 'GPL16016',\n",
        "        'is_patient_data': True,  # Suitable for validation\n",
        "        'notes': 'Patient serum samples'\n",
        "    },\n",
        "    'GSE39832': {\n",
        "        'description': 'Serum exosome miRNA - Cell line data',\n",
        "        'expected_platform': 'GPL16016',\n",
        "        'is_patient_data': False,  # NOT suitable for patient validation\n",
        "        'notes': 'HT-29 colorectal cancer cell line (NOT patient samples)'\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"[INFO] External datasets:\")\n",
        "for gse_id, info in EXTERNAL_DATASETS.items():\n",
        "    status = \"✓ Patient data\" if info['is_patient_data'] else \"⚠️ Cell line (not for patient validation)\"\n",
        "    print(f\"  - {gse_id}: {info['description']}\")\n",
        "    print(f\"    Status: {status}\")\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 2: Define Helper Functions\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 2: Define Helper Functions\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "\n",
        "def bootstrap_auc_ci(y_true, y_proba, n_bootstrap=1000, alpha=0.05, random_state=42):\n",
        "    \"\"\"\n",
        "    Compute bootstrap confidence interval for ROC-AUC.\n",
        "    \"\"\"\n",
        "    rng = np.random.RandomState(random_state)\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_proba = np.asarray(y_proba)\n",
        "    n = len(y_true)\n",
        "\n",
        "    aucs = []\n",
        "    for _ in range(n_bootstrap):\n",
        "        indices = rng.choice(n, n, replace=True)\n",
        "\n",
        "        # Skip if bootstrap sample doesn't contain both classes\n",
        "        if len(np.unique(y_true[indices])) < 2:\n",
        "            continue\n",
        "\n",
        "        aucs.append(roc_auc_score(y_true[indices], y_proba[indices]))\n",
        "\n",
        "    if len(aucs) == 0:\n",
        "        return np.nan, np.nan\n",
        "\n",
        "    lower = np.percentile(aucs, 100 * (alpha / 2))\n",
        "    upper = np.percentile(aucs, 100 * (1 - alpha / 2))\n",
        "\n",
        "    return float(lower), float(upper)\n",
        "\n",
        "\n",
        "def load_geo_dataset(gse_id, verbose=True):\n",
        "    \"\"\"\n",
        "    Load GEO dataset and extract expression matrix with sample metadata.\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(f\"\\n[INFO] Loading {gse_id} from GEO...\")\n",
        "\n",
        "    # Download and parse GEO dataset\n",
        "    gse = GEOparse.get_GEO(geo=gse_id, destdir='./geo_cache', silent=True)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"[INFO] Successfully loaded {gse_id}\")\n",
        "        print(f\"  - Title: {gse.metadata.get('title', ['Unknown'])[0][:60]}...\")\n",
        "        print(f\"  - Samples: {len(gse.gsms)}\")\n",
        "\n",
        "    # Extract sample IDs\n",
        "    sample_ids = list(gse.gsms.keys())\n",
        "\n",
        "    # Extract expression data\n",
        "    expression_data = []\n",
        "    sample_metadata = []\n",
        "\n",
        "    for gsm_id in sample_ids:\n",
        "        gsm = gse.gsms[gsm_id]\n",
        "\n",
        "        # Get expression values\n",
        "        if 'VALUE' in gsm.table.columns:\n",
        "            values = gsm.table['VALUE'].values\n",
        "        else:\n",
        "            value_cols = [c for c in gsm.table.columns if 'value' in c.lower()]\n",
        "            if value_cols:\n",
        "                values = gsm.table[value_cols[0]].values\n",
        "            else:\n",
        "                raise KeyError(f\"No expression value column found in {gsm_id}\")\n",
        "\n",
        "        expression_data.append(values)\n",
        "\n",
        "        # Extract metadata for label assignment\n",
        "        characteristics = gsm.metadata.get('characteristics_ch1', [])\n",
        "        title = gsm.metadata.get('title', [''])[0]\n",
        "        source = gsm.metadata.get('source_name_ch1', [''])[0]\n",
        "\n",
        "        sample_metadata.append({\n",
        "            'sample_id': gsm_id,\n",
        "            'title': title,\n",
        "            'source': source,\n",
        "            'characteristics': '; '.join(characteristics) if characteristics else ''\n",
        "        })\n",
        "\n",
        "    # Get probe IDs from first sample\n",
        "    probe_ids = gse.gsms[sample_ids[0]].table['ID_REF'].tolist()\n",
        "\n",
        "    # Create expression DataFrame\n",
        "    expr_df = pd.DataFrame(\n",
        "        expression_data,\n",
        "        index=sample_ids,\n",
        "        columns=probe_ids\n",
        "    )\n",
        "\n",
        "    # Create metadata DataFrame\n",
        "    sample_info = pd.DataFrame(sample_metadata)\n",
        "    sample_info.set_index('sample_id', inplace=True)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"  - Expression matrix shape: {expr_df.shape}\")\n",
        "        print(f\"  - Probe count: {len(probe_ids)}\")\n",
        "\n",
        "    return expr_df, sample_info, gse\n",
        "\n",
        "\n",
        "def assign_labels_external(sample_info, gse_id, verbose=True):\n",
        "    \"\"\"\n",
        "    Assign binary labels (0=healthy, 1=cancer) based on sample metadata.\n",
        "    \"\"\"\n",
        "    labels = []\n",
        "    label_log = []\n",
        "\n",
        "    # Define keywords for classification\n",
        "    healthy_keywords = [\n",
        "        'healthy', 'normal', 'control', 'hc', 'nc',\n",
        "        'non-cancer', 'non-tumor', 'benign', 'volunteer'\n",
        "    ]\n",
        "    cancer_keywords = [\n",
        "        'cancer', 'tumor', 'crc', 'colorectal', 'carcinoma',\n",
        "        'malignant', 'adenocarcinoma', 'patient', 'case'\n",
        "    ]\n",
        "\n",
        "    # Cell line keywords (for GSE39832)\n",
        "    cell_line_keywords = ['ht-29', 'ht29', 'caco', 'sw480', 'hct', 'cell line']\n",
        "\n",
        "    for sample_id in sample_info.index:\n",
        "        row = sample_info.loc[sample_id]\n",
        "\n",
        "        # Combine all text fields for keyword search\n",
        "        text_to_search = ' '.join([\n",
        "            str(row.get('title', '')),\n",
        "            str(row.get('source', '')),\n",
        "            str(row.get('characteristics', ''))\n",
        "        ]).lower()\n",
        "\n",
        "        # Check if this is cell line data\n",
        "        is_cell_line = any(kw in text_to_search for kw in cell_line_keywords)\n",
        "\n",
        "        # Check for healthy/cancer indicators\n",
        "        is_healthy = any(kw in text_to_search for kw in healthy_keywords)\n",
        "        is_cancer = any(kw in text_to_search for kw in cancer_keywords)\n",
        "\n",
        "        # Assign label with priority logic\n",
        "        if is_cell_line:\n",
        "            label = 1  # Cell lines are cancer-derived\n",
        "            reason = \"Cell line sample (cancer-derived)\"\n",
        "        elif is_healthy and not is_cancer:\n",
        "            label = 0\n",
        "            reason = \"Matched healthy keywords\"\n",
        "        elif is_cancer and not is_healthy:\n",
        "            label = 1\n",
        "            reason = \"Matched cancer keywords\"\n",
        "        elif is_healthy and is_cancer:\n",
        "            if 'healthy' in text_to_search or 'normal control' in text_to_search:\n",
        "                label = 0\n",
        "                reason = \"Matched 'healthy' specifically (ambiguous case)\"\n",
        "            else:\n",
        "                label = 1\n",
        "                reason = \"Matched cancer keywords (ambiguous case)\"\n",
        "        else:\n",
        "            label = 1  # Default to cancer if unclear\n",
        "            reason = \"No clear keywords - defaulting to cancer\"\n",
        "\n",
        "        labels.append(label)\n",
        "        label_log.append({\n",
        "            'sample_id': sample_id,\n",
        "            'title': row.get('title', ''),\n",
        "            'source': row.get('source', ''),\n",
        "            'label': label,\n",
        "            'reason': reason,\n",
        "            'is_cell_line': is_cell_line\n",
        "        })\n",
        "\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    if verbose:\n",
        "        n_healthy = (labels == 0).sum()\n",
        "        n_cancer = (labels == 1).sum()\n",
        "        n_cell_line = sum(1 for log in label_log if log.get('is_cell_line', False))\n",
        "\n",
        "        print(f\"\\n[INFO] Label assignment for {gse_id}:\")\n",
        "        print(f\"  - Healthy controls: {n_healthy}\")\n",
        "        print(f\"  - Cancer patients: {n_cancer}\")\n",
        "\n",
        "        if n_cell_line > 0:\n",
        "            print(f\"  ⚠️  WARNING: {n_cell_line} samples are cell line data!\")\n",
        "            print(f\"     Cell line data is NOT suitable for patient-level validation.\")\n",
        "\n",
        "        if n_healthy == 0 or n_cancer == 0:\n",
        "            print(f\"  ⚠️  WARNING: Only one class detected!\")\n",
        "            print(f\"  Sample titles for review:\")\n",
        "            for log_entry in label_log[:5]:\n",
        "                print(f\"    - {log_entry['title'][:50]}... → Label: {log_entry['label']}\")\n",
        "\n",
        "    return labels, label_log\n",
        "\n",
        "\n",
        "def validate_on_external_dataset(\n",
        "    model,\n",
        "    feature_list,\n",
        "    external_expr,\n",
        "    external_labels,\n",
        "    dataset_name,\n",
        "    seed=42,\n",
        "    verbose=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Validate trained model on external dataset.\n",
        "\n",
        "    Returns validation_result dict (renamed from 'results' to avoid conflicts)\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(f\"\\n{'─' * 60}\")\n",
        "        print(f\"Validating on {dataset_name}\")\n",
        "        print(f\"{'─' * 60}\")\n",
        "\n",
        "    # Step 1: Match features (probe IDs)\n",
        "    available_features = set(external_expr.columns)\n",
        "    required_features = set(feature_list)\n",
        "\n",
        "    matched_features = list(required_features & available_features)\n",
        "    missing_features = list(required_features - available_features)\n",
        "\n",
        "    match_rate = len(matched_features) / len(required_features) * 100\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"[INFO] Feature matching:\")\n",
        "        print(f\"  - Required features: {len(required_features)}\")\n",
        "        print(f\"  - Matched features: {len(matched_features)} ({match_rate:.1f}%)\")\n",
        "        print(f\"  - Missing features: {len(missing_features)}\")\n",
        "\n",
        "    if match_rate < 50:\n",
        "        print(f\"[ERROR] Less than 50% feature match. Validation may be unreliable.\")\n",
        "\n",
        "    # Step 2: Prepare feature matrix in SAME ORDER as training\n",
        "    X_external = np.zeros((len(external_expr), len(feature_list)))\n",
        "\n",
        "    for i, feat in enumerate(feature_list):\n",
        "        if feat in external_expr.columns:\n",
        "            X_external[:, i] = external_expr[feat].values\n",
        "        else:\n",
        "            X_external[:, i] = 0.0\n",
        "\n",
        "    # Step 3: Apply log2 transformation if needed\n",
        "    if external_expr.values.max() > 20:\n",
        "        if verbose:\n",
        "            print(f\"[INFO] Applying log2(x + 1) transformation\")\n",
        "        X_external = np.log2(X_external + 1.0)\n",
        "    else:\n",
        "        if verbose:\n",
        "            print(f\"[INFO] Data appears log2-transformed. Using as-is.\")\n",
        "\n",
        "    # Handle any infinite or NaN values\n",
        "    X_external = np.nan_to_num(X_external, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "    # Step 4: Predict using trained model\n",
        "    try:\n",
        "        y_proba = model.predict_proba(X_external)[:, 1]\n",
        "        y_pred = (y_proba >= 0.5).astype(int)\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Prediction failed: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "    # Step 5: Calculate performance metrics\n",
        "    unique_labels = np.unique(external_labels)\n",
        "    if len(unique_labels) < 2:\n",
        "        print(f\"[WARNING] Only one class present. AUC cannot be computed.\")\n",
        "        auc = np.nan\n",
        "        ci_lower, ci_upper = np.nan, np.nan\n",
        "    else:\n",
        "        auc = roc_auc_score(external_labels, y_proba)\n",
        "        ci_lower, ci_upper = bootstrap_auc_ci(\n",
        "            external_labels, y_proba, n_bootstrap=1000, random_state=seed\n",
        "        )\n",
        "\n",
        "    acc = accuracy_score(external_labels, y_pred)\n",
        "    bal_acc = balanced_accuracy_score(external_labels, y_pred)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        external_labels, y_pred, average='binary', zero_division=0\n",
        "    )\n",
        "\n",
        "    cm = confusion_matrix(external_labels, y_pred)\n",
        "\n",
        "    # Calculate specificity and sensitivity\n",
        "    if cm.shape == (2, 2):\n",
        "        tn, fp, fn, tp = cm.ravel()\n",
        "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
        "        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "    else:\n",
        "        specificity = np.nan\n",
        "        sensitivity = recall\n",
        "\n",
        "    # Compile results\n",
        "    validation_result = {\n",
        "        'dataset': dataset_name,\n",
        "        'n_samples': len(external_labels),\n",
        "        'n_healthy': int((external_labels == 0).sum()),\n",
        "        'n_cancer': int((external_labels == 1).sum()),\n",
        "        'feature_match_rate': float(match_rate),\n",
        "        'n_matched_features': len(matched_features),\n",
        "        'n_missing_features': len(missing_features),\n",
        "        'auc': float(auc) if not np.isnan(auc) else None,\n",
        "        'auc_ci_lower': float(ci_lower) if not np.isnan(ci_lower) else None,\n",
        "        'auc_ci_upper': float(ci_upper) if not np.isnan(ci_upper) else None,\n",
        "        'accuracy': float(acc),\n",
        "        'balanced_accuracy': float(bal_acc),\n",
        "        'sensitivity': float(sensitivity) if not np.isnan(sensitivity) else None,\n",
        "        'specificity': float(specificity) if not np.isnan(specificity) else None,\n",
        "        'precision': float(precision),\n",
        "        'f1_score': float(f1),\n",
        "        'confusion_matrix': cm.tolist(),\n",
        "        'y_true': external_labels.tolist(),\n",
        "        'y_proba': y_proba.tolist(),\n",
        "        'y_pred': y_pred.tolist()\n",
        "    }\n",
        "\n",
        "    # Print results\n",
        "    if verbose:\n",
        "        print(f\"\\n[RESULTS] {dataset_name}\")\n",
        "        print(f\"  Samples: {validation_result['n_samples']} \"\n",
        "              f\"({validation_result['n_healthy']} HC, {validation_result['n_cancer']} CRC)\")\n",
        "        print(f\"  Feature match: {validation_result['feature_match_rate']:.1f}%\")\n",
        "        print(f\"  ────────────────────────────────────\")\n",
        "        if validation_result['auc'] is not None:\n",
        "            print(f\"  AUC:              {validation_result['auc']:.4f} \"\n",
        "                  f\"(95% CI: [{validation_result['auc_ci_lower']:.4f}, \"\n",
        "                  f\"{validation_result['auc_ci_upper']:.4f}])\")\n",
        "        else:\n",
        "            print(f\"  AUC:              N/A (single class)\")\n",
        "        print(f\"  Accuracy:         {validation_result['accuracy']:.4f}\")\n",
        "        print(f\"  Balanced Acc:     {validation_result['balanced_accuracy']:.4f}\")\n",
        "        if validation_result['sensitivity'] is not None:\n",
        "            print(f\"  Sensitivity:      {validation_result['sensitivity']:.4f}\")\n",
        "        if validation_result['specificity'] is not None:\n",
        "            print(f\"  Specificity:      {validation_result['specificity']:.4f}\")\n",
        "        print(f\"  F1-score:         {validation_result['f1_score']:.4f}\")\n",
        "        print(f\"  Confusion Matrix:\")\n",
        "        print(f\"    {cm}\")\n",
        "\n",
        "    return validation_result\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 3: Load and Validate External Datasets\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 3: Load and Validate External Datasets\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Create cache directory for GEO downloads\n",
        "os.makedirs('./geo_cache', exist_ok=True)\n",
        "\n",
        "external_results = {}\n",
        "all_label_logs = {}\n",
        "\n",
        "for gse_id, info in EXTERNAL_DATASETS.items():\n",
        "    print(f\"\\n{'═' * 80}\")\n",
        "    print(f\"Processing {gse_id}: {info['description']}\")\n",
        "    if not info['is_patient_data']:\n",
        "        print(f\"⚠️  NOTE: This is cell line data - results for reference only\")\n",
        "    print(f\"{'═' * 80}\")\n",
        "\n",
        "    try:\n",
        "        # Load dataset\n",
        "        expr_df, sample_info, gse = load_geo_dataset(gse_id, verbose=True)\n",
        "\n",
        "        # Assign labels\n",
        "        labels, label_log = assign_labels_external(sample_info, gse_id, verbose=True)\n",
        "        all_label_logs[gse_id] = label_log\n",
        "\n",
        "        # Save label log for transparency\n",
        "        label_log_df = pd.DataFrame(label_log)\n",
        "        label_log_path = os.path.join(base_save_path, f\"external_label_log_{gse_id}.csv\")\n",
        "        label_log_df.to_csv(label_log_path, index=False)\n",
        "        print(f\"[INFO] Label log saved: {label_log_path}\")\n",
        "\n",
        "        # ┌─────────────────────────────────────────────────────────────────────┐\n",
        "        # │ FIX: Use different variable name to avoid overwriting Cell 2 results│\n",
        "        # └─────────────────────────────────────────────────────────────────────┘\n",
        "        validation_result = validate_on_external_dataset(\n",
        "            model=best_full_model,\n",
        "            feature_list=feature_cols,\n",
        "            external_expr=expr_df,\n",
        "            external_labels=labels,\n",
        "            dataset_name=gse_id,\n",
        "            seed=SEED,\n",
        "            verbose=True\n",
        "        )\n",
        "\n",
        "        if validation_result is not None:\n",
        "            validation_result['is_patient_data'] = info['is_patient_data']\n",
        "            external_results[gse_id] = validation_result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Failed to process {gse_id}: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        continue\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 4: Summary and Comparison\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 4: External Validation Summary\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "if len(external_results) == 0:\n",
        "    print(\"[ERROR] No external datasets were successfully validated.\")\n",
        "else:\n",
        "    # Create summary table\n",
        "    print(\"\\n┌\" + \"─\" * 78 + \"┐\")\n",
        "    print(\"│\" + \" EXTERNAL VALIDATION RESULTS \".center(78) + \"│\")\n",
        "    print(\"├\" + \"─\" * 78 + \"┤\")\n",
        "    print(f\"│ {'Dataset':<12} │ {'Type':<8} │ {'N':<5} │ {'AUC':<8} │ {'95% CI':<15} │ {'Bal.Acc':<7} │ {'Sens':<5} │ {'Spec':<5} │\")\n",
        "    print(\"├\" + \"─\" * 78 + \"┤\")\n",
        "\n",
        "    for gse_id, res in external_results.items():\n",
        "        auc_str = f\"{res['auc']:.4f}\" if res['auc'] else \"N/A\"\n",
        "        ci_str = f\"[{res['auc_ci_lower']:.3f},{res['auc_ci_upper']:.3f}]\" if res['auc_ci_lower'] else \"N/A\"\n",
        "        sens_str = f\"{res['sensitivity']:.2f}\" if res['sensitivity'] else \"N/A\"\n",
        "        spec_str = f\"{res['specificity']:.2f}\" if res['specificity'] else \"N/A\"\n",
        "        data_type = \"Patient\" if res.get('is_patient_data', True) else \"Cell\"\n",
        "\n",
        "        print(f\"│ {gse_id:<12} │ {data_type:<8} │ {res['n_samples']:<5} │ {auc_str:<8} │ {ci_str:<15} │ {res['balanced_accuracy']:.4f}  │ {sens_str:<5} │ {spec_str:<5} │\")\n",
        "\n",
        "    print(\"└\" + \"─\" * 78 + \"┘\")\n",
        "\n",
        "    # Compare with training performance\n",
        "    print(\"\\n\" + \"─\" * 80)\n",
        "    print(\"COMPARISON: Training vs External Validation\")\n",
        "    print(\"─\" * 80)\n",
        "\n",
        "    # Use preserved nested CV results\n",
        "    print(f\"\\n{'Metric':<20} │ {'Training (CV)':<15} │ \", end=\"\")\n",
        "    for gse_id in external_results.keys():\n",
        "        print(f\"{gse_id:<15} │ \", end=\"\")\n",
        "    print()\n",
        "    print(\"─\" * (22 + 18 + 18 * len(external_results)))\n",
        "\n",
        "    print(f\"{'AUC':<20} │ {train_auc:.4f}          │ \", end=\"\")\n",
        "    for gse_id, res in external_results.items():\n",
        "        auc_str = f\"{res['auc']:.4f}\" if res['auc'] else \"N/A\"\n",
        "        print(f\"{auc_str:<15} │ \", end=\"\")\n",
        "    print()\n",
        "\n",
        "    print(f\"{'Balanced Accuracy':<20} │ {train_bal_acc:.4f}          │ \", end=\"\")\n",
        "    for gse_id, res in external_results.items():\n",
        "        print(f\"{res['balanced_accuracy']:.4f}          │ \", end=\"\")\n",
        "    print()\n",
        "\n",
        "    # Performance drop analysis (only for patient data)\n",
        "    print(\"\\n\" + \"─\" * 80)\n",
        "    print(\"PERFORMANCE DROP ANALYSIS (Patient Data Only)\")\n",
        "    print(\"─\" * 80)\n",
        "\n",
        "    patient_datasets = {k: v for k, v in external_results.items()\n",
        "                        if v.get('is_patient_data', True)}\n",
        "\n",
        "    for gse_id, res in patient_datasets.items():\n",
        "        if res['auc'] is not None:\n",
        "            auc_drop = train_auc - res['auc']\n",
        "            bal_acc_drop = train_bal_acc - res['balanced_accuracy']\n",
        "\n",
        "            print(f\"\\n{gse_id}:\")\n",
        "            print(f\"  AUC drop:          {auc_drop:+.4f} ({train_auc:.4f} → {res['auc']:.4f})\")\n",
        "            print(f\"  Balanced Acc drop: {bal_acc_drop:+.4f} ({train_bal_acc:.4f} → {res['balanced_accuracy']:.4f})\")\n",
        "\n",
        "            # Interpretation\n",
        "            if auc_drop < 0.05:\n",
        "                print(f\"  ✓ Excellent generalization (AUC drop < 5%)\")\n",
        "            elif auc_drop < 0.10:\n",
        "                print(f\"  ○ Good generalization (AUC drop < 10%)\")\n",
        "            elif auc_drop < 0.20:\n",
        "                print(f\"  △ Moderate generalization (AUC drop < 20%)\")\n",
        "            else:\n",
        "                print(f\"  ✗ Poor generalization (AUC drop ≥ 20%) - Overfitting likely\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 5: Generate Visualization\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 5: Generate ROC Curves for External Validation\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "if len(external_results) > 0:\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "    # Plot training ROC (from OOF predictions)\n",
        "    train_y_true = oof_predictions[best_name]['y_true']\n",
        "    train_y_proba = oof_predictions[best_name]['y_proba']\n",
        "    fpr_train, tpr_train, _ = roc_curve(train_y_true, train_y_proba)\n",
        "\n",
        "    ax.plot(\n",
        "        fpr_train, tpr_train,\n",
        "        linewidth=2.5,\n",
        "        label=f'Training (GSE39833) - AUC = {train_auc:.3f}',\n",
        "        color='#2C3E50'\n",
        "    )\n",
        "\n",
        "    # Plot external validation ROCs\n",
        "    colors = {'GSE39814': '#E74C3C', 'GSE39832': '#95A5A6'}  # Gray for cell line\n",
        "    linestyles = {'GSE39814': '--', 'GSE39832': ':'}\n",
        "\n",
        "    for gse_id, res in external_results.items():\n",
        "        if res['auc'] is not None:\n",
        "            y_true = np.array(res['y_true'])\n",
        "            y_proba = np.array(res['y_proba'])\n",
        "            fpr, tpr, _ = roc_curve(y_true, y_proba)\n",
        "\n",
        "            label_suffix = \"\" if res.get('is_patient_data', True) else \" (Cell line)\"\n",
        "\n",
        "            ax.plot(\n",
        "                fpr, tpr,\n",
        "                linewidth=2,\n",
        "                linestyle=linestyles.get(gse_id, '--'),\n",
        "                label=f'{gse_id}{label_suffix} - AUC = {res[\"auc\"]:.3f}',\n",
        "                color=colors.get(gse_id, '#3498DB')\n",
        "            )\n",
        "\n",
        "    # Diagonal reference line\n",
        "    ax.plot([0, 1], [0, 1], 'k--', alpha=0.3, label='Random (AUC = 0.5)')\n",
        "\n",
        "    ax.set_xlim([0.0, 1.0])\n",
        "    ax.set_ylim([0.0, 1.05])\n",
        "    ax.set_xlabel('False Positive Rate (1 - Specificity)', fontsize=12)\n",
        "    ax.set_ylabel('True Positive Rate (Sensitivity)', fontsize=12)\n",
        "    ax.set_title('ROC Curves: Training vs External Validation', fontsize=14, fontweight='bold')\n",
        "    ax.legend(loc='lower right', fontsize=10)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Save figure\n",
        "    roc_path = os.path.join(base_save_path, 'external_validation_roc.png')\n",
        "    plt.savefig(roc_path, dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(f\"[INFO] ROC curve saved: {roc_path}\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 6: Save Comprehensive Results\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 6: Save External Validation Results\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Prepare JSON-serializable results\n",
        "json_output = {\n",
        "    'training_dataset': 'GSE39833',\n",
        "    'model_name': best_name,\n",
        "    'n_features': len(feature_cols),\n",
        "    'training_performance': {\n",
        "        'oof_auc': float(train_auc),\n",
        "        'oof_balanced_accuracy': float(train_bal_acc),\n",
        "        'oof_sensitivity': float(train_sensitivity),\n",
        "        'oof_specificity': float(train_specificity)\n",
        "    },\n",
        "    'external_validation': {}\n",
        "}\n",
        "\n",
        "for gse_id, res in external_results.items():\n",
        "    # Remove large arrays from JSON\n",
        "    json_res = {k: v for k, v in res.items() if k not in ['y_true', 'y_proba', 'y_pred']}\n",
        "    json_output['external_validation'][gse_id] = json_res\n",
        "\n",
        "# Save JSON\n",
        "json_path = os.path.join(base_save_path, 'external_validation_results.json')\n",
        "with open(json_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(json_output, f, indent=2, ensure_ascii=False)\n",
        "print(f\"[INFO] Results saved: {json_path}\")\n",
        "\n",
        "# Save detailed CSV for each external dataset\n",
        "for gse_id, res in external_results.items():\n",
        "    detail_df = pd.DataFrame({\n",
        "        'y_true': res['y_true'],\n",
        "        'y_proba': res['y_proba'],\n",
        "        'y_pred': res['y_pred']\n",
        "    })\n",
        "    detail_path = os.path.join(base_save_path, f'external_predictions_{gse_id}.csv')\n",
        "    detail_df.to_csv(detail_path, index=False)\n",
        "    print(f\"[INFO] Predictions saved: {detail_path}\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# FINAL SUMMARY\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"✓ EXTERNAL VALIDATION COMPLETE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"\"\"\n",
        "[SUMMARY]\n",
        "  Model validated: {best_name}\n",
        "  Training dataset: GSE39833 (n=99)\n",
        "  External datasets: {len(external_results)}\n",
        "\"\"\")\n",
        "\n",
        "# Final interpretation (patient data only)\n",
        "patient_results = {k: v for k, v in external_results.items()\n",
        "                   if v.get('is_patient_data', True) and v['auc'] is not None}\n",
        "\n",
        "if len(patient_results) > 0:\n",
        "    avg_patient_auc = np.mean([res['auc'] for res in patient_results.values()])\n",
        "\n",
        "    print(f\"[PATIENT DATA VALIDATION]\")\n",
        "    print(f\"  Training AUC:         {train_auc:.4f}\")\n",
        "    print(f\"  Avg Patient Ext AUC:  {avg_patient_auc:.4f}\")\n",
        "    print(f\"  Performance drop:     {train_auc - avg_patient_auc:.4f}\")\n",
        "\n",
        "    if avg_patient_auc >= 0.85:\n",
        "        print(f\"\\n  ✓ EXCELLENT: Model shows strong generalization.\")\n",
        "        print(f\"    → Ready for publication with strong external validation.\")\n",
        "    elif avg_patient_auc >= 0.75:\n",
        "        print(f\"\\n  ○ GOOD: Model generalizes reasonably well.\")\n",
        "        print(f\"    → Publishable with appropriate limitations discussed.\")\n",
        "    elif avg_patient_auc >= 0.65:\n",
        "        print(f\"\\n  △ MODERATE: Some overfitting observed.\")\n",
        "        print(f\"    → Consider feature reduction or regularization.\")\n",
        "    else:\n",
        "        print(f\"\\n  ✗ POOR: Significant overfitting detected.\")\n",
        "        print(f\"    → Model revision strongly recommended.\")\n",
        "\n",
        "# Note about cell line data\n",
        "cell_line_results = {k: v for k, v in external_results.items()\n",
        "                     if not v.get('is_patient_data', True)}\n",
        "if len(cell_line_results) > 0:\n",
        "    print(f\"\\n[CELL LINE DATA NOTE]\")\n",
        "    print(f\"  GSE39832 contains HT-29 colorectal cancer cell line data.\")\n",
        "    print(f\"  This is NOT suitable for patient-level clinical validation.\")\n",
        "    print(f\"  Results are provided for reference only.\")\n",
        "\n",
        "print(f\"\"\"\n",
        "[NEXT STEPS]\n",
        "  1. Review label assignment logs for accuracy\n",
        "  2. If performance drop > 0.15, consider:\n",
        "     - Reducing feature count (use CV-stable features only)\n",
        "     - Increasing regularization\n",
        "     - Using simpler model\n",
        "  3. Update manuscript with external validation results\n",
        "\n",
        "[FILES GENERATED]\n",
        "  - external_validation_results.json\n",
        "  - external_validation_roc.png\n",
        "  - external_label_log_GSE39814.csv\n",
        "  - external_label_log_GSE39832.csv\n",
        "  - external_predictions_GSE39814.csv\n",
        "  - external_predictions_GSE39832.csv\n",
        "\"\"\")\n",
        "\n",
        "print(\"=\" * 80)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPyBcoT9Glir7h+tBwkroA0",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}